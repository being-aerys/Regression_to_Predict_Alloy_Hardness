{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Given the scarcity of data in this task, we will use k-Fold cross-validation for training with k set to a large value to make a good use of the available data of a significantly small size."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Following are the hyperparameters that can be tuned for a neural network\n",
    "\n",
    "1. learning rate\n",
    "2. no of hidden units\n",
    "3. no of epochs to train\n",
    "4. dropout probability\n",
    "5. loss function (NOT TUNED HERE)\n",
    "6. mini batch size\n",
    "7. weights initialization (NOT TUNED HERE)\n",
    "8. l1/ l2 regularizers\n",
    "9. activation function to use at the nodes (NOT TUNED HERE)\n",
    "10. no of layers (NOT TUNED HERE)\n",
    "11. learning rate decay (NOT TUNED HERE)\n",
    "12. optimizer (NOT TUNED HERE)\n",
    "13. momentum (only if sgd or rmsprop optimizer used, not with adam, adagrad)\n",
    "14. momentum_dampening (only if sgd or rmsprop optimizer used, not with adam, adagrad) (NOT TUNED HERE)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There are several methods to tune hyperparameters of a neural network. Two of them are grid search and randomized search that select points in the parameter space and evaluate these points (each point is essestially a unique configuration of hyperparameters in the hyperparameter space) and return the best hyperparameter combination based on the performace on the validation data.\n",
    "\n",
    "#### How is grid search/ randomized search done with k-fold cross validation?\n",
    "1. Select n points in the hyperparameter space. For each point, do:\n",
    "    a. Each point corresponds to a hyperparameter configuration in the hyperparameter space.\n",
    "    b. Train and evaluate this model k times as follows.\n",
    "        i. Randomly divide the training data into k partitions. Call them partition 1, partition 2,.........., partition k.\n",
    "        ii. Repeat for each partition j starting from j = 1 to j = k\n",
    "            . Train the model with this hyperparameter configuration on the partitions except partition j and test on                     partition j.\n",
    "            . Store the performance of the model.\n",
    "        iii. Calculate the average performance of the model with this hyperparameter configuration over the k folds.\n",
    "2. Declare the model that was trained using the hyperparameters corresponding to the point that achieved the best              validation result as the best model and declare this choice of hyperparameters as the best hyperparameters.\n",
    "3. Predict the labels of the testing data using this model."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Import Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "import copy\n",
    "import sys\n",
    "import os\n",
    "from math import sqrt\n",
    "\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "import pandas as pd\n",
    "\n",
    "import seaborn as sns\n",
    "\n",
    "import sklearn\n",
    "from sklearn.externals import joblib\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from skorch import NeuralNetRegressor\n",
    "from sklearn.model_selection import RandomizedSearchCV\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "\n",
    "from sklearn.metrics import mean_squared_error as MSE\n",
    "from sklearn.metrics import r2_score\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The python version used is 3.6.8 (tags/v3.6.8:3c6b436a57, Dec 24 2018, 00:16:47) [MSC v.1916 64 bit (AMD64)]\n",
      "The torch version used is 1.4.0\n",
      "The sklearn version used is 0.20.3\n"
     ]
    }
   ],
   "source": [
    "print(\"The python version used is {}\".format(sys.version))\n",
    "print(\"The torch version used is {}\".format(torch.__version__))\n",
    "print(\"The sklearn version used is {}\".format(sklearn.__version__))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Import Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>% of Cr</th>\n",
       "      <th>% of Hf</th>\n",
       "      <th>% of Mo</th>\n",
       "      <th>% of Nb</th>\n",
       "      <th>% of Ta</th>\n",
       "      <th>% of Ti</th>\n",
       "      <th>% of V</th>\n",
       "      <th>% of Zr</th>\n",
       "      <th>% of Ni</th>\n",
       "      <th>% of Al</th>\n",
       "      <th>% of Mn</th>\n",
       "      <th>%Cu</th>\n",
       "      <th>%C</th>\n",
       "      <th>Entropy</th>\n",
       "      <th>Hardness</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.470</td>\n",
       "      <td>0.44</td>\n",
       "      <td>0.97491</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.992366</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.138686</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.602527</td>\n",
       "      <td>0.429768</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.7500</td>\n",
       "      <td>0.200</td>\n",
       "      <td>0.25</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.4000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.589731</td>\n",
       "      <td>0.539457</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.5815</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.4026</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.396171</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.733380</td>\n",
       "      <td>0.559966</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.5000</td>\n",
       "      <td>0.800</td>\n",
       "      <td>0.25</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.4000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.541151</td>\n",
       "      <td>0.351534</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.468</td>\n",
       "      <td>0.33</td>\n",
       "      <td>0.97491</td>\n",
       "      <td>0.0860</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.138686</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.741308</td>\n",
       "      <td>0.441290</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   % of Cr  % of Hf  % of Mo  % of Nb  % of Ta  % of Ti  % of V   % of Zr  \\\n",
       "0      0.0      0.0   0.0000    0.470     0.44  0.97491  0.0000  0.992366   \n",
       "1      0.0      0.0   0.7500    0.200     0.25  0.00000  0.4000  0.000000   \n",
       "2      0.0      0.0   0.5815    0.000     0.00  0.00000  0.4026  0.000000   \n",
       "3      0.0      0.0   0.5000    0.800     0.25  0.00000  0.4000  0.000000   \n",
       "4      0.0      0.0   0.0000    0.468     0.33  0.97491  0.0860  1.000000   \n",
       "\n",
       "    % of Ni   % of Al  % of Mn  %Cu   %C   Entropy  Hardness  \n",
       "0  0.000000  0.138686      0.0  0.0  0.0  0.602527  0.429768  \n",
       "1  0.000000  0.000000      0.0  0.0  0.0  0.589731  0.539457  \n",
       "2  0.396171  0.000000      0.0  0.0  0.0  0.733380  0.559966  \n",
       "3  0.000000  0.000000      0.0  0.0  0.0  0.541151  0.351534  \n",
       "4  0.000000  0.138686      0.0  0.0  0.0  0.741308  0.441290  "
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "training_data = pd.read_csv(\"normalized_training_features_and_targets.csv\", sep = \",\")\n",
    "training_data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The number of features is 14\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>% of Cr</th>\n",
       "      <th>% of Hf</th>\n",
       "      <th>% of Mo</th>\n",
       "      <th>% of Nb</th>\n",
       "      <th>% of Ta</th>\n",
       "      <th>% of Ti</th>\n",
       "      <th>% of V</th>\n",
       "      <th>% of Zr</th>\n",
       "      <th>% of Ni</th>\n",
       "      <th>% of Al</th>\n",
       "      <th>% of Mn</th>\n",
       "      <th>%Cu</th>\n",
       "      <th>%C</th>\n",
       "      <th>Entropy</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.470</td>\n",
       "      <td>0.44</td>\n",
       "      <td>0.97491</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.992366</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.138686</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.602527</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.7500</td>\n",
       "      <td>0.200</td>\n",
       "      <td>0.25</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.4000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.589731</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.5815</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.4026</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.396171</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.733380</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.5000</td>\n",
       "      <td>0.800</td>\n",
       "      <td>0.25</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.4000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.541151</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.468</td>\n",
       "      <td>0.33</td>\n",
       "      <td>0.97491</td>\n",
       "      <td>0.0860</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.138686</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.741308</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   % of Cr  % of Hf  % of Mo  % of Nb  % of Ta  % of Ti  % of V   % of Zr  \\\n",
       "0      0.0      0.0   0.0000    0.470     0.44  0.97491  0.0000  0.992366   \n",
       "1      0.0      0.0   0.7500    0.200     0.25  0.00000  0.4000  0.000000   \n",
       "2      0.0      0.0   0.5815    0.000     0.00  0.00000  0.4026  0.000000   \n",
       "3      0.0      0.0   0.5000    0.800     0.25  0.00000  0.4000  0.000000   \n",
       "4      0.0      0.0   0.0000    0.468     0.33  0.97491  0.0860  1.000000   \n",
       "\n",
       "    % of Ni   % of Al  % of Mn  %Cu   %C   Entropy  \n",
       "0  0.000000  0.138686      0.0  0.0  0.0  0.602527  \n",
       "1  0.000000  0.000000      0.0  0.0  0.0  0.589731  \n",
       "2  0.396171  0.000000      0.0  0.0  0.0  0.733380  \n",
       "3  0.000000  0.000000      0.0  0.0  0.0  0.541151  \n",
       "4  0.000000  0.138686      0.0  0.0  0.0  0.741308  "
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "num_features = len( training_data.iloc[0,:] ) - 1\n",
    "print(\"The number of features is {}\".format(num_features))\n",
    "X_training = training_data.iloc[:,0:num_features]\n",
    "\n",
    "X_training.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Hardness</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.429768</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.539457</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.559966</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.351534</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.441290</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   Hardness\n",
       "0  0.429768\n",
       "1  0.539457\n",
       "2  0.559966\n",
       "3  0.351534\n",
       "4  0.441290"
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Y_training = pd.DataFrame(training_data[\"Hardness\"])\n",
    "Y_training.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Initialize Seeds for Reproducibility"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.random.seed(0)\n",
    "torch.manual_seed(0)\n",
    "torch.cuda.manual_seed(0)\n",
    "\n",
    "device_to_use = \"cuda\" if torch.cuda.device_count() > 0 else \"cpu\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## torch Regression Model definition"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Regression_Module(nn.Module):\n",
    "    def __init__(self, num_units = 10, dropout = 0.5, activation = F.leaky_relu, input_dim = num_features, output_dim = 1):\n",
    "           \n",
    "        \n",
    "        super(Regression_Module, self).__init__()\n",
    "        self.activation = activation\n",
    "        \n",
    "        \n",
    "        self.L1 = nn.Linear(input_dim, num_units)\n",
    "        self.L2 = nn.Linear(num_units, num_units)\n",
    "        \n",
    "        #self.batchnorm_1 = nn.BatchNorm1d(num_units, 1e-12, affine=True, track_running_stats=True)\n",
    "        self.dropout_1 = nn.Dropout(p=dropout)\n",
    "        \n",
    "        self.L3 = nn.Linear(num_units, num_units)\n",
    "        self.L4 = nn.Linear(num_units, num_units)\n",
    "        \n",
    "        #self.batchnorm_2 = nn.BatchNorm1d(num_units, 1e-12, affine=True, track_running_stats=True)\n",
    "        self.dropout_2 = nn.Dropout(p=dropout)\n",
    "        \n",
    "        self.L5 = nn.Linear(num_units, num_units)\n",
    "        self.L6 = nn.Linear(num_units, output_dim)\n",
    "\n",
    "\n",
    "\n",
    "    def forward(self, input):\n",
    "            \n",
    "        input = self.activation(self.L1(input.float()))\n",
    "        input = self.L2(input.float())\n",
    "\n",
    "        # input = input.unsqueeze(0) #https://discuss.pytorch.org/t/batchnorm1d-valueerror-expected-2d-or-3d-input-got-1d-input/42081\n",
    "        # input = self.batchnorm_1(input)\n",
    "\n",
    "        input = self.activation(input.float())\n",
    "        input = self.dropout_1(input.float())\n",
    "\n",
    "        input = self.activation(self.L3(input.float()))\n",
    "        input = self.L4(input.float())\n",
    "\n",
    "        # input = input.unsqueeze(0)\n",
    "        # input = self.batchnorm_2(input)\n",
    "\n",
    "        input = self.activation(input.float())\n",
    "        input = self.dropout_2(input.float())\n",
    "\n",
    "        input = self.activation(self.L5(input.float()))\n",
    "\n",
    "        input = self.L6(input.float())\n",
    "\n",
    "\n",
    "        #VVI: need to return in double format instead of a float\n",
    "        return input.double()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## skorch Regression Model definition\n",
    "Takes torch regressor as an argument."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "skorch_regressor = NeuralNetRegressor(module = Regression_Module,  #pass a torch module class\n",
    "                                      device = device_to_use,\n",
    "                                      iterator_train__shuffle = True,\n",
    "                                      \n",
    "                                     ) \n",
    "                                      "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Generate hyperparameters for hyperparameter tuning using sklearn's Randomized Search\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "#lr = np.random.uniform(low = 0.000001,high = 0.05, size = 20).tolist()\n",
    "lr = [0.00005, 0.0001, 0.0003, 0.0005, 0.001, 0.003, 0.005]\n",
    "\n",
    "weight_decay_for_regularization = [1e-5, 5e-5, 1e-4, 5e-5, 1e-3, 5e-3, 1e-2, 5e-2, 1e-1]      #weight decay equals L2 regularization for SGD\n",
    "\n",
    "momentum_vals = [0.5, 0.75, 0.99]\n",
    "\n",
    "momentum_dampening = [0.]\n",
    "\n",
    "nesterov = [True, False]\n",
    "\n",
    "no_of_nodes_per_layer = [num_features, num_features * int(1.25), num_features * int(1.5), num_features * int(1.75), num_features *2]\n",
    "\n",
    "#max_epochs = [epoch_num for epoch_num in range(25, 400, 25)]\n",
    "max_epochs = [50, 75, 100, 125, 150, 175, 200, 250, 300]\n",
    "\n",
    "dropout_probability_per_node = [0., 0.3, 0.5]\n",
    "\n",
    "#We will use onle mse as the loss function for now. so skip tuning the loss function.\n",
    "\n",
    "minibatch_size = [8, 16, 32, 64] #should always be less than the size of the trianing set\n",
    "\n",
    "#optimizers = [torch.optim.SGD, torch.optim.RMSprop, torch.optim.Adagrad]\n",
    "optimizers = [torch.optim.SGD] #Using only SGD"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Create the grid for selection of random points on the hyperparameter space.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [],
   "source": [
    "common_param_grid = {\n",
    "    \n",
    "    \"lr\" : lr,\n",
    "    \"max_epochs\" : max_epochs,\n",
    "    \n",
    "    \n",
    "    \n",
    "    #list all the formal arguments of the torch module that u pass to skorch regressor here beginning with module__\n",
    "    \"module__num_units\" : no_of_nodes_per_layer,\n",
    "    \"module__dropout\" : dropout_probability_per_node,\n",
    "    #not passing the activation here though\n",
    "    \n",
    "    \n",
    "    \n",
    "    \"optimizer\" : optimizers,\n",
    "    \"optimizer__weight_decay\": weight_decay_for_regularization,\n",
    "    \"optimizer__momentum\" : momentum_vals,\n",
    "    \"optimizer__dampening\" : momentum_dampening,\n",
    "    \"optimizer__nesterov\" : nesterov, \n",
    "    \n",
    "    \n",
    "    \n",
    "    \"iterator_train__batch_size\": minibatch_size,\n",
    "    #\"callbacks__scheduler__epoch\": [10, 50, 100], #learning rate scheduler    \n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Create a Randomized Search Instance and pass the estimator, thehyperparameters' grid, the no of random samples to take from the grid, and the value of k for k-Fold cross-validation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [],
   "source": [
    "sgd_randomized_search = RandomizedSearchCV(estimator = skorch_regressor,\n",
    "                                          scoring = \"neg_mean_squared_error\",\n",
    "                                          param_distributions = common_param_grid,\n",
    "                                          n_iter = 150,  #no of random samples to take from the grid                                                    \n",
    "                                          cv = 10,\n",
    "                                          )\n",
    "\n",
    "#skorch_regressor.get_params().keys()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Train the models."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  epoch    train_loss    valid_loss     dur\n",
      "-------  ------------  ------------  ------\n",
      "      1        \u001b[36m0.6099\u001b[0m        \u001b[32m0.5296\u001b[0m  0.0070\n",
      "      2        \u001b[36m0.6088\u001b[0m        \u001b[32m0.5259\u001b[0m  0.0079\n",
      "      3        \u001b[36m0.6058\u001b[0m        \u001b[32m0.5215\u001b[0m  0.0090\n",
      "      4        \u001b[36m0.5984\u001b[0m        \u001b[32m0.5169\u001b[0m  0.0080\n",
      "      5        0.6032        \u001b[32m0.5122\u001b[0m  0.0090\n",
      "      6        0.6010        \u001b[32m0.5073\u001b[0m  0.0080\n",
      "      7        \u001b[36m0.5885\u001b[0m        \u001b[32m0.5026\u001b[0m  0.0091\n",
      "      8        \u001b[36m0.5852\u001b[0m        \u001b[32m0.4978\u001b[0m  0.0080\n",
      "      9        0.5852        \u001b[32m0.4930\u001b[0m  0.0090\n",
      "     10        \u001b[36m0.5707\u001b[0m        \u001b[32m0.4884\u001b[0m  0.0080\n",
      "     11        \u001b[36m0.5652\u001b[0m        \u001b[32m0.4837\u001b[0m  0.0079\n",
      "     12        \u001b[36m0.5649\u001b[0m        \u001b[32m0.4792\u001b[0m  0.0080\n",
      "     13        \u001b[36m0.5526\u001b[0m        \u001b[32m0.4746\u001b[0m  0.0080\n",
      "     14        0.5552        \u001b[32m0.4701\u001b[0m  0.0090\n",
      "     15        \u001b[36m0.5495\u001b[0m        \u001b[32m0.4657\u001b[0m  0.0100\n",
      "     16        \u001b[36m0.5392\u001b[0m        \u001b[32m0.4612\u001b[0m  0.0090\n",
      "     17        0.5449        \u001b[32m0.4569\u001b[0m  0.0090\n",
      "     18        \u001b[36m0.5309\u001b[0m        \u001b[32m0.4525\u001b[0m  0.0080\n",
      "     19        \u001b[36m0.5261\u001b[0m        \u001b[32m0.4483\u001b[0m  0.0090\n",
      "     20        \u001b[36m0.5248\u001b[0m        \u001b[32m0.4440\u001b[0m  0.0100\n",
      "     21        \u001b[36m0.5149\u001b[0m        \u001b[32m0.4398\u001b[0m  0.0100\n",
      "     22        0.5170        \u001b[32m0.4357\u001b[0m  0.0090\n",
      "     23        \u001b[36m0.5045\u001b[0m        \u001b[32m0.4316\u001b[0m  0.0090\n",
      "     24        0.5072        \u001b[32m0.4275\u001b[0m  0.0090\n",
      "     25        0.5116        \u001b[32m0.4234\u001b[0m  0.0090\n",
      "     26        \u001b[36m0.4956\u001b[0m        \u001b[32m0.4194\u001b[0m  0.0080\n",
      "     27        \u001b[36m0.4931\u001b[0m        \u001b[32m0.4155\u001b[0m  0.0090\n",
      "     28        \u001b[36m0.4930\u001b[0m        \u001b[32m0.4115\u001b[0m  0.0080\n",
      "     29        0.4961        \u001b[32m0.4076\u001b[0m  0.0090\n",
      "     30        \u001b[36m0.4854\u001b[0m        \u001b[32m0.4037\u001b[0m  0.0090\n",
      "     31        \u001b[36m0.4760\u001b[0m        \u001b[32m0.3998\u001b[0m  0.0100\n",
      "     32        \u001b[36m0.4726\u001b[0m        \u001b[32m0.3961\u001b[0m  0.0090\n",
      "     33        \u001b[36m0.4679\u001b[0m        \u001b[32m0.3923\u001b[0m  0.0100\n",
      "     34        \u001b[36m0.4626\u001b[0m        \u001b[32m0.3886\u001b[0m  0.0110\n",
      "     35        \u001b[36m0.4620\u001b[0m        \u001b[32m0.3849\u001b[0m  0.0100\n",
      "     36        \u001b[36m0.4537\u001b[0m        \u001b[32m0.3813\u001b[0m  0.0100\n",
      "     37        0.4585        \u001b[32m0.3777\u001b[0m  0.0090\n",
      "     38        \u001b[36m0.4526\u001b[0m        \u001b[32m0.3741\u001b[0m  0.0090\n",
      "     39        \u001b[36m0.4523\u001b[0m        \u001b[32m0.3706\u001b[0m  0.0090\n",
      "     40        \u001b[36m0.4405\u001b[0m        \u001b[32m0.3670\u001b[0m  0.0090\n",
      "     41        \u001b[36m0.4387\u001b[0m        \u001b[32m0.3636\u001b[0m  0.0080\n",
      "     42        \u001b[36m0.4324\u001b[0m        \u001b[32m0.3602\u001b[0m  0.0090\n",
      "     43        0.4343        \u001b[32m0.3568\u001b[0m  0.0090\n",
      "     44        \u001b[36m0.4213\u001b[0m        \u001b[32m0.3534\u001b[0m  0.0090\n",
      "     45        0.4264        \u001b[32m0.3501\u001b[0m  0.0100\n",
      "     46        0.4273        \u001b[32m0.3468\u001b[0m  0.0110\n",
      "     47        \u001b[36m0.4158\u001b[0m        \u001b[32m0.3435\u001b[0m  0.0080\n",
      "     48        \u001b[36m0.4120\u001b[0m        \u001b[32m0.3403\u001b[0m  0.0090\n",
      "     49        \u001b[36m0.4079\u001b[0m        \u001b[32m0.3371\u001b[0m  0.0090\n",
      "     50        \u001b[36m0.4031\u001b[0m        \u001b[32m0.3339\u001b[0m  0.0090\n",
      "     51        0.4068        \u001b[32m0.3308\u001b[0m  0.0110\n",
      "     52        \u001b[36m0.4015\u001b[0m        \u001b[32m0.3277\u001b[0m  0.0110\n",
      "     53        \u001b[36m0.3980\u001b[0m        \u001b[32m0.3246\u001b[0m  0.0110\n",
      "     54        \u001b[36m0.3941\u001b[0m        \u001b[32m0.3215\u001b[0m  0.0090\n",
      "     55        \u001b[36m0.3887\u001b[0m        \u001b[32m0.3185\u001b[0m  0.0079\n",
      "     56        0.3923        \u001b[32m0.3155\u001b[0m  0.0100\n",
      "     57        \u001b[36m0.3844\u001b[0m        \u001b[32m0.3125\u001b[0m  0.0089\n",
      "     58        \u001b[36m0.3808\u001b[0m        \u001b[32m0.3095\u001b[0m  0.0090\n",
      "     59        \u001b[36m0.3768\u001b[0m        \u001b[32m0.3066\u001b[0m  0.0120\n",
      "     60        \u001b[36m0.3767\u001b[0m        \u001b[32m0.3037\u001b[0m  0.0090\n",
      "     61        \u001b[36m0.3684\u001b[0m        \u001b[32m0.3009\u001b[0m  0.0080\n",
      "     62        \u001b[36m0.3665\u001b[0m        \u001b[32m0.2981\u001b[0m  0.0100\n",
      "     63        \u001b[36m0.3640\u001b[0m        \u001b[32m0.2953\u001b[0m  0.0100\n",
      "     64        \u001b[36m0.3571\u001b[0m        \u001b[32m0.2925\u001b[0m  0.0089\n",
      "     65        0.3605        \u001b[32m0.2898\u001b[0m  0.0090\n",
      "     66        \u001b[36m0.3491\u001b[0m        \u001b[32m0.2871\u001b[0m  0.0160\n",
      "     67        0.3533        \u001b[32m0.2844\u001b[0m  0.0080\n",
      "     68        \u001b[36m0.3453\u001b[0m        \u001b[32m0.2817\u001b[0m  0.0090\n",
      "     69        0.3482        \u001b[32m0.2791\u001b[0m  0.0100\n",
      "     70        \u001b[36m0.3419\u001b[0m        \u001b[32m0.2765\u001b[0m  0.0110\n",
      "     71        \u001b[36m0.3383\u001b[0m        \u001b[32m0.2740\u001b[0m  0.0110\n",
      "     72        \u001b[36m0.3310\u001b[0m        \u001b[32m0.2714\u001b[0m  0.0090\n",
      "     73        0.3352        \u001b[32m0.2689\u001b[0m  0.0110\n",
      "     74        \u001b[36m0.3299\u001b[0m        \u001b[32m0.2664\u001b[0m  0.0100\n",
      "     75        \u001b[36m0.3233\u001b[0m        \u001b[32m0.2639\u001b[0m  0.0100\n",
      "     76        0.3282        \u001b[32m0.2615\u001b[0m  0.0130\n",
      "     77        \u001b[36m0.3232\u001b[0m        \u001b[32m0.2591\u001b[0m  0.0130\n",
      "     78        \u001b[36m0.3190\u001b[0m        \u001b[32m0.2567\u001b[0m  0.0110\n",
      "     79        \u001b[36m0.3140\u001b[0m        \u001b[32m0.2543\u001b[0m  0.0100\n",
      "     80        0.3166        \u001b[32m0.2520\u001b[0m  0.0110\n",
      "     81        \u001b[36m0.3087\u001b[0m        \u001b[32m0.2497\u001b[0m  0.0120\n",
      "     82        \u001b[36m0.3059\u001b[0m        \u001b[32m0.2474\u001b[0m  0.0100\n",
      "     83        \u001b[36m0.3009\u001b[0m        \u001b[32m0.2451\u001b[0m  0.0100\n",
      "     84        0.3055        \u001b[32m0.2428\u001b[0m  0.0110\n",
      "     85        0.3024        \u001b[32m0.2406\u001b[0m  0.0100\n",
      "     86        \u001b[36m0.2976\u001b[0m        \u001b[32m0.2384\u001b[0m  0.0090\n",
      "     87        0.2977        \u001b[32m0.2362\u001b[0m  0.0130\n",
      "     88        \u001b[36m0.2957\u001b[0m        \u001b[32m0.2340\u001b[0m  0.0100\n",
      "     89        \u001b[36m0.2873\u001b[0m        \u001b[32m0.2319\u001b[0m  0.0120\n",
      "     90        0.2907        \u001b[32m0.2297\u001b[0m  0.0090\n",
      "     91        \u001b[36m0.2862\u001b[0m        \u001b[32m0.2276\u001b[0m  0.0080\n",
      "     92        0.2862        \u001b[32m0.2255\u001b[0m  0.0090\n",
      "     93        \u001b[36m0.2844\u001b[0m        \u001b[32m0.2234\u001b[0m  0.0080\n",
      "     94        \u001b[36m0.2778\u001b[0m        \u001b[32m0.2214\u001b[0m  0.0080\n",
      "     95        0.2825        \u001b[32m0.2193\u001b[0m  0.0080\n",
      "     96        \u001b[36m0.2750\u001b[0m        \u001b[32m0.2173\u001b[0m  0.0100\n",
      "     97        \u001b[36m0.2713\u001b[0m        \u001b[32m0.2153\u001b[0m  0.0090\n",
      "     98        \u001b[36m0.2700\u001b[0m        \u001b[32m0.2134\u001b[0m  0.0090\n",
      "     99        0.2753        \u001b[32m0.2114\u001b[0m  0.0090\n",
      "    100        \u001b[36m0.2663\u001b[0m        \u001b[32m0.2095\u001b[0m  0.0090\n",
      "    101        \u001b[36m0.2611\u001b[0m        \u001b[32m0.2076\u001b[0m  0.0100\n",
      "    102        0.2616        \u001b[32m0.2057\u001b[0m  0.0090\n",
      "    103        \u001b[36m0.2601\u001b[0m        \u001b[32m0.2038\u001b[0m  0.0090\n",
      "    104        0.2614        \u001b[32m0.2020\u001b[0m  0.0080\n",
      "    105        \u001b[36m0.2561\u001b[0m        \u001b[32m0.2002\u001b[0m  0.0080\n",
      "    106        0.2568        \u001b[32m0.1983\u001b[0m  0.0090\n",
      "    107        \u001b[36m0.2531\u001b[0m        \u001b[32m0.1965\u001b[0m  0.0080\n",
      "    108        \u001b[36m0.2463\u001b[0m        \u001b[32m0.1948\u001b[0m  0.0080\n",
      "    109        \u001b[36m0.2460\u001b[0m        \u001b[32m0.1930\u001b[0m  0.0080\n",
      "    110        \u001b[36m0.2429\u001b[0m        \u001b[32m0.1913\u001b[0m  0.0080\n",
      "    111        \u001b[36m0.2409\u001b[0m        \u001b[32m0.1896\u001b[0m  0.0090\n",
      "    112        0.2422        \u001b[32m0.1879\u001b[0m  0.0090\n",
      "    113        0.2443        \u001b[32m0.1862\u001b[0m  0.0100\n",
      "    114        \u001b[36m0.2359\u001b[0m        \u001b[32m0.1845\u001b[0m  0.0080\n",
      "    115        \u001b[36m0.2347\u001b[0m        \u001b[32m0.1829\u001b[0m  0.0090\n",
      "    116        0.2369        \u001b[32m0.1812\u001b[0m  0.0090\n",
      "    117        \u001b[36m0.2305\u001b[0m        \u001b[32m0.1796\u001b[0m  0.0100\n",
      "    118        0.2336        \u001b[32m0.1780\u001b[0m  0.0080\n",
      "    119        \u001b[36m0.2273\u001b[0m        \u001b[32m0.1764\u001b[0m  0.0090\n",
      "    120        0.2296        \u001b[32m0.1748\u001b[0m  0.0090\n",
      "    121        \u001b[36m0.2202\u001b[0m        \u001b[32m0.1733\u001b[0m  0.0090\n",
      "    122        0.2223        \u001b[32m0.1717\u001b[0m  0.0090\n",
      "    123        0.2222        \u001b[32m0.1702\u001b[0m  0.0080\n",
      "    124        0.2211        \u001b[32m0.1687\u001b[0m  0.0110\n",
      "    125        0.2212        \u001b[32m0.1672\u001b[0m  0.0080\n",
      "    126        \u001b[36m0.2158\u001b[0m        \u001b[32m0.1657\u001b[0m  0.0080\n",
      "    127        \u001b[36m0.2150\u001b[0m        \u001b[32m0.1643\u001b[0m  0.0090\n",
      "    128        \u001b[36m0.2124\u001b[0m        \u001b[32m0.1628\u001b[0m  0.0120\n",
      "    129        \u001b[36m0.2102\u001b[0m        \u001b[32m0.1614\u001b[0m  0.0090\n",
      "    130        \u001b[36m0.2093\u001b[0m        \u001b[32m0.1600\u001b[0m  0.0080\n",
      "    131        \u001b[36m0.2088\u001b[0m        \u001b[32m0.1586\u001b[0m  0.0080\n",
      "    132        \u001b[36m0.2068\u001b[0m        \u001b[32m0.1572\u001b[0m  0.0080\n",
      "    133        \u001b[36m0.2039\u001b[0m        \u001b[32m0.1558\u001b[0m  0.0080\n",
      "    134        \u001b[36m0.2033\u001b[0m        \u001b[32m0.1544\u001b[0m  0.0080\n",
      "    135        \u001b[36m0.2012\u001b[0m        \u001b[32m0.1531\u001b[0m  0.0100\n",
      "    136        0.2013        \u001b[32m0.1518\u001b[0m  0.0080\n",
      "    137        \u001b[36m0.1962\u001b[0m        \u001b[32m0.1504\u001b[0m  0.0100\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "    138        \u001b[36m0.1957\u001b[0m        \u001b[32m0.1491\u001b[0m  0.0080\n",
      "    139        \u001b[36m0.1931\u001b[0m        \u001b[32m0.1479\u001b[0m  0.0090\n",
      "    140        0.1960        \u001b[32m0.1466\u001b[0m  0.0090\n",
      "    141        \u001b[36m0.1915\u001b[0m        \u001b[32m0.1453\u001b[0m  0.0110\n",
      "    142        0.1932        \u001b[32m0.1441\u001b[0m  0.0080\n",
      "    143        0.1919        \u001b[32m0.1428\u001b[0m  0.0090\n",
      "    144        \u001b[36m0.1860\u001b[0m        \u001b[32m0.1416\u001b[0m  0.0090\n",
      "    145        0.1865        \u001b[32m0.1404\u001b[0m  0.0090\n",
      "    146        \u001b[36m0.1854\u001b[0m        \u001b[32m0.1392\u001b[0m  0.0080\n",
      "    147        \u001b[36m0.1820\u001b[0m        \u001b[32m0.1380\u001b[0m  0.0090\n",
      "    148        \u001b[36m0.1815\u001b[0m        \u001b[32m0.1368\u001b[0m  0.0080\n",
      "    149        0.1836        \u001b[32m0.1357\u001b[0m  0.0090\n",
      "    150        \u001b[36m0.1805\u001b[0m        \u001b[32m0.1345\u001b[0m  0.0080\n",
      "  epoch    train_loss    valid_loss     dur\n",
      "-------  ------------  ------------  ------\n",
      "      1        \u001b[36m0.2053\u001b[0m        \u001b[32m0.2857\u001b[0m  0.0060\n",
      "      2        \u001b[36m0.2015\u001b[0m        \u001b[32m0.2841\u001b[0m  0.0110\n",
      "      3        \u001b[36m0.1987\u001b[0m        \u001b[32m0.2822\u001b[0m  0.0080\n",
      "      4        0.2019        \u001b[32m0.2803\u001b[0m  0.0080\n",
      "      5        \u001b[36m0.1957\u001b[0m        \u001b[32m0.2783\u001b[0m  0.0110\n",
      "      6        \u001b[36m0.1916\u001b[0m        \u001b[32m0.2762\u001b[0m  0.0100\n",
      "      7        \u001b[36m0.1910\u001b[0m        \u001b[32m0.2742\u001b[0m  0.0090\n",
      "      8        \u001b[36m0.1905\u001b[0m        \u001b[32m0.2722\u001b[0m  0.0080\n",
      "      9        \u001b[36m0.1905\u001b[0m        \u001b[32m0.2702\u001b[0m  0.0090\n",
      "     10        \u001b[36m0.1864\u001b[0m        \u001b[32m0.2682\u001b[0m  0.0080\n",
      "     11        \u001b[36m0.1848\u001b[0m        \u001b[32m0.2663\u001b[0m  0.0080\n",
      "     12        \u001b[36m0.1847\u001b[0m        \u001b[32m0.2644\u001b[0m  0.0090\n",
      "     13        0.1860        \u001b[32m0.2624\u001b[0m  0.0090\n",
      "     14        \u001b[36m0.1776\u001b[0m        \u001b[32m0.2605\u001b[0m  0.0080\n",
      "     15        0.1793        \u001b[32m0.2586\u001b[0m  0.0120\n",
      "     16        0.1798        \u001b[32m0.2568\u001b[0m  0.0100\n",
      "     17        0.1778        \u001b[32m0.2550\u001b[0m  0.0110\n",
      "     18        \u001b[36m0.1762\u001b[0m        \u001b[32m0.2531\u001b[0m  0.0090\n",
      "     19        \u001b[36m0.1747\u001b[0m        \u001b[32m0.2513\u001b[0m  0.0100\n",
      "     20        \u001b[36m0.1682\u001b[0m        \u001b[32m0.2496\u001b[0m  0.0110\n",
      "     21        0.1705        \u001b[32m0.2478\u001b[0m  0.0090\n",
      "     22        0.1721        \u001b[32m0.2461\u001b[0m  0.0090\n",
      "     23        0.1697        \u001b[32m0.2444\u001b[0m  0.0100\n",
      "     24        0.1694        \u001b[32m0.2426\u001b[0m  0.0090\n",
      "     25        \u001b[36m0.1646\u001b[0m        \u001b[32m0.2410\u001b[0m  0.0080\n",
      "     26        \u001b[36m0.1636\u001b[0m        \u001b[32m0.2393\u001b[0m  0.0080\n",
      "     27        0.1674        \u001b[32m0.2376\u001b[0m  0.0107\n",
      "     28        \u001b[36m0.1604\u001b[0m        \u001b[32m0.2360\u001b[0m  0.0090\n",
      "     29        \u001b[36m0.1596\u001b[0m        \u001b[32m0.2344\u001b[0m  0.0080\n",
      "     30        0.1598        \u001b[32m0.2328\u001b[0m  0.0090\n",
      "     31        0.1597        \u001b[32m0.2312\u001b[0m  0.0090\n",
      "     32        \u001b[36m0.1562\u001b[0m        \u001b[32m0.2296\u001b[0m  0.0080\n",
      "     33        \u001b[36m0.1541\u001b[0m        \u001b[32m0.2281\u001b[0m  0.0080\n",
      "     34        0.1556        \u001b[32m0.2265\u001b[0m  0.0080\n",
      "     35        \u001b[36m0.1539\u001b[0m        \u001b[32m0.2250\u001b[0m  0.0090\n",
      "     36        \u001b[36m0.1512\u001b[0m        \u001b[32m0.2235\u001b[0m  0.0090\n",
      "     37        0.1522        \u001b[32m0.2220\u001b[0m  0.0080\n",
      "     38        \u001b[36m0.1472\u001b[0m        \u001b[32m0.2205\u001b[0m  0.0100\n",
      "     39        0.1495        \u001b[32m0.2191\u001b[0m  0.0080\n",
      "     40        \u001b[36m0.1472\u001b[0m        \u001b[32m0.2176\u001b[0m  0.0090\n",
      "     41        0.1477        \u001b[32m0.2162\u001b[0m  0.0090\n",
      "     42        \u001b[36m0.1451\u001b[0m        \u001b[32m0.2148\u001b[0m  0.0080\n",
      "     43        0.1455        \u001b[32m0.2134\u001b[0m  0.0076\n",
      "     44        \u001b[36m0.1440\u001b[0m        \u001b[32m0.2120\u001b[0m  0.0090\n",
      "     45        \u001b[36m0.1428\u001b[0m        \u001b[32m0.2106\u001b[0m  0.0080\n",
      "     46        \u001b[36m0.1398\u001b[0m        \u001b[32m0.2092\u001b[0m  0.0080\n",
      "     47        \u001b[36m0.1389\u001b[0m        \u001b[32m0.2079\u001b[0m  0.0090\n",
      "     48        0.1403        \u001b[32m0.2066\u001b[0m  0.0100\n",
      "     49        \u001b[36m0.1360\u001b[0m        \u001b[32m0.2052\u001b[0m  0.0100\n",
      "     50        0.1385        \u001b[32m0.2039\u001b[0m  0.0080\n",
      "     51        0.1373        \u001b[32m0.2026\u001b[0m  0.0100\n",
      "     52        0.1370        \u001b[32m0.2013\u001b[0m  0.0090\n",
      "     53        \u001b[36m0.1329\u001b[0m        \u001b[32m0.2000\u001b[0m  0.0090\n",
      "     54        \u001b[36m0.1327\u001b[0m        \u001b[32m0.1988\u001b[0m  0.0090\n",
      "     55        0.1356        \u001b[32m0.1975\u001b[0m  0.0080\n",
      "     56        \u001b[36m0.1285\u001b[0m        \u001b[32m0.1962\u001b[0m  0.0090\n",
      "     57        \u001b[36m0.1261\u001b[0m        \u001b[32m0.1950\u001b[0m  0.0080\n",
      "     58        \u001b[36m0.1251\u001b[0m        \u001b[32m0.1938\u001b[0m  0.0120\n",
      "     59        0.1280        \u001b[32m0.1926\u001b[0m  0.0080\n",
      "     60        \u001b[36m0.1249\u001b[0m        \u001b[32m0.1914\u001b[0m  0.0080\n",
      "     61        0.1266        \u001b[32m0.1903\u001b[0m  0.0090\n",
      "     62        0.1300        \u001b[32m0.1891\u001b[0m  0.0090\n",
      "     63        0.1302        \u001b[32m0.1879\u001b[0m  0.0090\n",
      "     64        0.1273        \u001b[32m0.1868\u001b[0m  0.0080\n",
      "     65        \u001b[36m0.1247\u001b[0m        \u001b[32m0.1856\u001b[0m  0.0080\n",
      "     66        \u001b[36m0.1229\u001b[0m        \u001b[32m0.1845\u001b[0m  0.0090\n",
      "     67        0.1264        \u001b[32m0.1834\u001b[0m  0.0080\n",
      "     68        \u001b[36m0.1183\u001b[0m        \u001b[32m0.1823\u001b[0m  0.0100\n",
      "     69        \u001b[36m0.1180\u001b[0m        \u001b[32m0.1812\u001b[0m  0.0100\n",
      "     70        0.1222        \u001b[32m0.1802\u001b[0m  0.0080\n",
      "     71        0.1186        \u001b[32m0.1791\u001b[0m  0.0090\n",
      "     72        \u001b[36m0.1162\u001b[0m        \u001b[32m0.1781\u001b[0m  0.0110\n",
      "     73        0.1197        \u001b[32m0.1770\u001b[0m  0.0080\n",
      "     74        0.1174        \u001b[32m0.1760\u001b[0m  0.0080\n",
      "     75        0.1164        \u001b[32m0.1750\u001b[0m  0.0080\n",
      "     76        \u001b[36m0.1148\u001b[0m        \u001b[32m0.1739\u001b[0m  0.0090\n",
      "     77        0.1152        \u001b[32m0.1729\u001b[0m  0.0080\n",
      "     78        0.1173        \u001b[32m0.1719\u001b[0m  0.0090\n",
      "     79        \u001b[36m0.1134\u001b[0m        \u001b[32m0.1709\u001b[0m  0.0100\n",
      "     80        \u001b[36m0.1104\u001b[0m        \u001b[32m0.1700\u001b[0m  0.0090\n",
      "     81        0.1124        \u001b[32m0.1690\u001b[0m  0.0080\n",
      "     82        \u001b[36m0.1094\u001b[0m        \u001b[32m0.1681\u001b[0m  0.0090\n",
      "     83        0.1125        \u001b[32m0.1671\u001b[0m  0.0080\n",
      "     84        \u001b[36m0.1091\u001b[0m        \u001b[32m0.1662\u001b[0m  0.0090\n",
      "     85        0.1110        \u001b[32m0.1652\u001b[0m  0.0090\n",
      "     86        \u001b[36m0.1088\u001b[0m        \u001b[32m0.1643\u001b[0m  0.0080\n",
      "     87        \u001b[36m0.1074\u001b[0m        \u001b[32m0.1634\u001b[0m  0.0080\n",
      "     88        0.1076        \u001b[32m0.1625\u001b[0m  0.0090\n",
      "     89        \u001b[36m0.1062\u001b[0m        \u001b[32m0.1616\u001b[0m  0.0090\n",
      "     90        0.1083        \u001b[32m0.1607\u001b[0m  0.0080\n",
      "     91        \u001b[36m0.1028\u001b[0m        \u001b[32m0.1598\u001b[0m  0.0080\n",
      "     92        0.1053        \u001b[32m0.1590\u001b[0m  0.0090\n",
      "     93        0.1063        \u001b[32m0.1581\u001b[0m  0.0090\n",
      "     94        0.1038        \u001b[32m0.1573\u001b[0m  0.0080\n",
      "     95        0.1053        \u001b[32m0.1564\u001b[0m  0.0090\n",
      "     96        \u001b[36m0.1025\u001b[0m        \u001b[32m0.1556\u001b[0m  0.0080\n",
      "     97        0.1026        \u001b[32m0.1548\u001b[0m  0.0090\n",
      "     98        \u001b[36m0.1014\u001b[0m        \u001b[32m0.1540\u001b[0m  0.0100\n",
      "     99        \u001b[36m0.1000\u001b[0m        \u001b[32m0.1531\u001b[0m  0.0090\n",
      "    100        \u001b[36m0.0991\u001b[0m        \u001b[32m0.1523\u001b[0m  0.0080\n",
      "    101        \u001b[36m0.0974\u001b[0m        \u001b[32m0.1516\u001b[0m  0.0090\n",
      "    102        0.1021        \u001b[32m0.1508\u001b[0m  0.0080\n",
      "    103        0.0996        \u001b[32m0.1500\u001b[0m  0.0090\n",
      "    104        \u001b[36m0.0921\u001b[0m        \u001b[32m0.1492\u001b[0m  0.0090\n",
      "    105        0.1000        \u001b[32m0.1485\u001b[0m  0.0080\n",
      "    106        0.1008        \u001b[32m0.1477\u001b[0m  0.0090\n",
      "    107        0.0961        \u001b[32m0.1470\u001b[0m  0.0090\n",
      "    108        0.0963        \u001b[32m0.1462\u001b[0m  0.0080\n",
      "    109        0.0927        \u001b[32m0.1455\u001b[0m  0.0130\n",
      "    110        0.0974        \u001b[32m0.1447\u001b[0m  0.0100\n",
      "    111        \u001b[36m0.0914\u001b[0m        \u001b[32m0.1440\u001b[0m  0.0090\n",
      "    112        0.0941        \u001b[32m0.1433\u001b[0m  0.0090\n",
      "    113        0.0957        \u001b[32m0.1426\u001b[0m  0.0090\n",
      "    114        \u001b[36m0.0892\u001b[0m        \u001b[32m0.1419\u001b[0m  0.0090\n",
      "    115        0.0931        \u001b[32m0.1412\u001b[0m  0.0090\n",
      "    116        0.0939        \u001b[32m0.1405\u001b[0m  0.0090\n",
      "    117        0.0901        \u001b[32m0.1398\u001b[0m  0.0100\n",
      "    118        0.0905        \u001b[32m0.1392\u001b[0m  0.0080\n",
      "    119        \u001b[36m0.0882\u001b[0m        \u001b[32m0.1385\u001b[0m  0.0080\n",
      "    120        \u001b[36m0.0877\u001b[0m        \u001b[32m0.1378\u001b[0m  0.0080\n",
      "    121        0.0902        \u001b[32m0.1372\u001b[0m  0.0080\n",
      "    122        0.0881        \u001b[32m0.1365\u001b[0m  0.0080\n",
      "    123        0.0915        \u001b[32m0.1359\u001b[0m  0.0080\n",
      "    124        \u001b[36m0.0860\u001b[0m        \u001b[32m0.1352\u001b[0m  0.0080\n",
      "    125        0.0885        \u001b[32m0.1346\u001b[0m  0.0090\n",
      "    126        \u001b[36m0.0858\u001b[0m        \u001b[32m0.1340\u001b[0m  0.0080\n",
      "    127        \u001b[36m0.0834\u001b[0m        \u001b[32m0.1334\u001b[0m  0.0090\n",
      "    128        0.0836        \u001b[32m0.1328\u001b[0m  0.0080\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "    129        0.0870        \u001b[32m0.1322\u001b[0m  0.0080\n",
      "    130        0.0874        \u001b[32m0.1316\u001b[0m  0.0070\n",
      "    131        0.0843        \u001b[32m0.1310\u001b[0m  0.0080\n",
      "    132        0.0872        \u001b[32m0.1304\u001b[0m  0.0080\n",
      "    133        0.0851        \u001b[32m0.1298\u001b[0m  0.0080\n",
      "    134        \u001b[36m0.0814\u001b[0m        \u001b[32m0.1292\u001b[0m  0.0100\n",
      "    135        0.0851        \u001b[32m0.1287\u001b[0m  0.0090\n",
      "    136        0.0828        \u001b[32m0.1281\u001b[0m  0.0080\n",
      "    137        0.0830        \u001b[32m0.1275\u001b[0m  0.0080\n",
      "    138        \u001b[36m0.0807\u001b[0m        \u001b[32m0.1270\u001b[0m  0.0090\n",
      "    139        0.0818        \u001b[32m0.1264\u001b[0m  0.0100\n",
      "    140        0.0835        \u001b[32m0.1259\u001b[0m  0.0100\n",
      "    141        \u001b[36m0.0803\u001b[0m        \u001b[32m0.1254\u001b[0m  0.0080\n",
      "    142        0.0837        \u001b[32m0.1248\u001b[0m  0.0090\n",
      "    143        0.0807        \u001b[32m0.1243\u001b[0m  0.0100\n",
      "    144        0.0844        \u001b[32m0.1238\u001b[0m  0.0090\n",
      "    145        \u001b[36m0.0785\u001b[0m        \u001b[32m0.1233\u001b[0m  0.0090\n",
      "    146        0.0794        \u001b[32m0.1228\u001b[0m  0.0080\n",
      "    147        0.0795        \u001b[32m0.1223\u001b[0m  0.0080\n",
      "    148        \u001b[36m0.0770\u001b[0m        \u001b[32m0.1218\u001b[0m  0.0080\n",
      "    149        0.0802        \u001b[32m0.1213\u001b[0m  0.0080\n",
      "    150        0.0790        \u001b[32m0.1208\u001b[0m  0.0080\n",
      "  epoch    train_loss    valid_loss     dur\n",
      "-------  ------------  ------------  ------\n",
      "      1        \u001b[36m0.5711\u001b[0m        \u001b[32m0.5050\u001b[0m  0.0120\n",
      "      2        0.5753        \u001b[32m0.5035\u001b[0m  0.0140\n",
      "      3        \u001b[36m0.5654\u001b[0m        \u001b[32m0.5013\u001b[0m  0.0140\n",
      "      4        \u001b[36m0.5650\u001b[0m        \u001b[32m0.4983\u001b[0m  0.0130\n",
      "      5        0.5699        \u001b[32m0.4945\u001b[0m  0.0130\n",
      "      6        \u001b[36m0.5627\u001b[0m        \u001b[32m0.4900\u001b[0m  0.0160\n",
      "      7        \u001b[36m0.5588\u001b[0m        \u001b[32m0.4848\u001b[0m  0.0140\n",
      "      8        \u001b[36m0.5551\u001b[0m        \u001b[32m0.4789\u001b[0m  0.0150\n",
      "      9        \u001b[36m0.5477\u001b[0m        \u001b[32m0.4723\u001b[0m  0.0150\n",
      "     10        \u001b[36m0.5392\u001b[0m        \u001b[32m0.4652\u001b[0m  0.0140\n",
      "     11        \u001b[36m0.5346\u001b[0m        \u001b[32m0.4575\u001b[0m  0.0130\n",
      "     12        \u001b[36m0.5197\u001b[0m        \u001b[32m0.4492\u001b[0m  0.0140\n",
      "     13        \u001b[36m0.5063\u001b[0m        \u001b[32m0.4405\u001b[0m  0.0130\n",
      "     14        \u001b[36m0.5044\u001b[0m        \u001b[32m0.4313\u001b[0m  0.0150\n",
      "     15        \u001b[36m0.4948\u001b[0m        \u001b[32m0.4218\u001b[0m  0.0140\n",
      "     16        \u001b[36m0.4798\u001b[0m        \u001b[32m0.4118\u001b[0m  0.0140\n",
      "     17        \u001b[36m0.4713\u001b[0m        \u001b[32m0.4015\u001b[0m  0.0157\n",
      "     18        \u001b[36m0.4586\u001b[0m        \u001b[32m0.3909\u001b[0m  0.0170\n",
      "     19        \u001b[36m0.4541\u001b[0m        \u001b[32m0.3800\u001b[0m  0.0140\n",
      "     20        \u001b[36m0.4395\u001b[0m        \u001b[32m0.3688\u001b[0m  0.0140\n",
      "     21        \u001b[36m0.4250\u001b[0m        \u001b[32m0.3575\u001b[0m  0.0160\n",
      "     22        \u001b[36m0.4052\u001b[0m        \u001b[32m0.3460\u001b[0m  0.0140\n",
      "     23        \u001b[36m0.4008\u001b[0m        \u001b[32m0.3344\u001b[0m  0.0140\n",
      "     24        \u001b[36m0.3848\u001b[0m        \u001b[32m0.3227\u001b[0m  0.0140\n",
      "     25        \u001b[36m0.3724\u001b[0m        \u001b[32m0.3110\u001b[0m  0.0160\n",
      "     26        \u001b[36m0.3628\u001b[0m        \u001b[32m0.2990\u001b[0m  0.0168\n",
      "     27        \u001b[36m0.3467\u001b[0m        \u001b[32m0.2871\u001b[0m  0.0139\n",
      "     28        \u001b[36m0.3362\u001b[0m        \u001b[32m0.2751\u001b[0m  0.0140\n",
      "     29        \u001b[36m0.3192\u001b[0m        \u001b[32m0.2633\u001b[0m  0.0150\n",
      "     30        \u001b[36m0.3085\u001b[0m        \u001b[32m0.2515\u001b[0m  0.0140\n",
      "     31        \u001b[36m0.2912\u001b[0m        \u001b[32m0.2399\u001b[0m  0.0159\n",
      "     32        \u001b[36m0.2817\u001b[0m        \u001b[32m0.2285\u001b[0m  0.0150\n",
      "     33        \u001b[36m0.2713\u001b[0m        \u001b[32m0.2174\u001b[0m  0.0139\n",
      "     34        \u001b[36m0.2601\u001b[0m        \u001b[32m0.2064\u001b[0m  0.0160\n",
      "     35        \u001b[36m0.2501\u001b[0m        \u001b[32m0.1957\u001b[0m  0.0160\n",
      "     36        \u001b[36m0.2363\u001b[0m        \u001b[32m0.1852\u001b[0m  0.0140\n",
      "     37        \u001b[36m0.2266\u001b[0m        \u001b[32m0.1751\u001b[0m  0.0180\n",
      "     38        \u001b[36m0.2091\u001b[0m        \u001b[32m0.1653\u001b[0m  0.0150\n",
      "     39        \u001b[36m0.1989\u001b[0m        \u001b[32m0.1559\u001b[0m  0.0140\n",
      "     40        \u001b[36m0.1881\u001b[0m        \u001b[32m0.1467\u001b[0m  0.0140\n",
      "     41        \u001b[36m0.1791\u001b[0m        \u001b[32m0.1379\u001b[0m  0.0160\n",
      "     42        \u001b[36m0.1719\u001b[0m        \u001b[32m0.1294\u001b[0m  0.0140\n",
      "     43        \u001b[36m0.1623\u001b[0m        \u001b[32m0.1213\u001b[0m  0.0140\n",
      "     44        \u001b[36m0.1558\u001b[0m        \u001b[32m0.1136\u001b[0m  0.0160\n",
      "     45        \u001b[36m0.1451\u001b[0m        \u001b[32m0.1062\u001b[0m  0.0140\n",
      "     46        \u001b[36m0.1347\u001b[0m        \u001b[32m0.0991\u001b[0m  0.0160\n",
      "     47        \u001b[36m0.1256\u001b[0m        \u001b[32m0.0925\u001b[0m  0.0130\n",
      "     48        \u001b[36m0.1193\u001b[0m        \u001b[32m0.0862\u001b[0m  0.0139\n",
      "     49        \u001b[36m0.1120\u001b[0m        \u001b[32m0.0803\u001b[0m  0.0140\n",
      "     50        \u001b[36m0.1046\u001b[0m        \u001b[32m0.0749\u001b[0m  0.0160\n",
      "     51        \u001b[36m0.0974\u001b[0m        \u001b[32m0.0698\u001b[0m  0.0140\n",
      "     52        \u001b[36m0.0938\u001b[0m        \u001b[32m0.0651\u001b[0m  0.0150\n",
      "     53        \u001b[36m0.0877\u001b[0m        \u001b[32m0.0609\u001b[0m  0.0130\n",
      "     54        \u001b[36m0.0813\u001b[0m        \u001b[32m0.0570\u001b[0m  0.0140\n",
      "     55        \u001b[36m0.0809\u001b[0m        \u001b[32m0.0535\u001b[0m  0.0139\n",
      "     56        \u001b[36m0.0739\u001b[0m        \u001b[32m0.0503\u001b[0m  0.0160\n",
      "     57        \u001b[36m0.0684\u001b[0m        \u001b[32m0.0475\u001b[0m  0.0140\n",
      "     58        \u001b[36m0.0643\u001b[0m        \u001b[32m0.0451\u001b[0m  0.0139\n",
      "     59        \u001b[36m0.0619\u001b[0m        \u001b[32m0.0431\u001b[0m  0.0130\n",
      "     60        \u001b[36m0.0605\u001b[0m        \u001b[32m0.0413\u001b[0m  0.0130\n",
      "     61        \u001b[36m0.0553\u001b[0m        \u001b[32m0.0399\u001b[0m  0.0140\n",
      "     62        0.0565        \u001b[32m0.0389\u001b[0m  0.0140\n",
      "     63        \u001b[36m0.0507\u001b[0m        \u001b[32m0.0381\u001b[0m  0.0149\n",
      "     64        \u001b[36m0.0503\u001b[0m        \u001b[32m0.0376\u001b[0m  0.0139\n",
      "     65        \u001b[36m0.0498\u001b[0m        \u001b[32m0.0374\u001b[0m  0.0130\n",
      "     66        \u001b[36m0.0490\u001b[0m        0.0375  0.0130\n",
      "     67        \u001b[36m0.0464\u001b[0m        0.0378  0.0130\n",
      "     68        \u001b[36m0.0461\u001b[0m        0.0383  0.0140\n",
      "     69        0.0472        0.0391  0.0139\n",
      "     70        0.0480        0.0401  0.0150\n",
      "     71        0.0466        0.0412  0.0139\n",
      "     72        0.0501        0.0426  0.0150\n",
      "     73        0.0511        0.0441  0.0140\n",
      "     74        0.0492        0.0458  0.0130\n",
      "     75        0.0509        0.0476  0.0150\n",
      "     76        0.0540        0.0495  0.0130\n",
      "     77        0.0516        0.0515  0.0140\n",
      "     78        0.0536        0.0537  0.0140\n",
      "     79        0.0550        0.0559  0.0130\n",
      "     80        0.0567        0.0582  0.0130\n",
      "     81        0.0612        0.0605  0.0150\n",
      "     82        0.0595        0.0629  0.0209\n",
      "     83        0.0637        0.0653  0.0150\n",
      "     84        0.0651        0.0677  0.0160\n",
      "     85        0.0685        0.0701  0.0140\n",
      "     86        0.0717        0.0724  0.0150\n",
      "     87        0.0711        0.0748  0.0140\n",
      "     88        0.0774        0.0771  0.0139\n",
      "     89        0.0765        0.0794  0.0150\n",
      "     90        0.0770        0.0816  0.0150\n",
      "     91        0.0798        0.0838  0.0140\n",
      "     92        0.0831        0.0859  0.0140\n",
      "     93        0.0818        0.0879  0.0160\n",
      "     94        0.0829        0.0899  0.0140\n",
      "     95        0.0886        0.0917  0.0140\n",
      "     96        0.0861        0.0935  0.0139\n",
      "     97        0.0913        0.0952  0.0151\n",
      "     98        0.0924        0.0967  0.0150\n",
      "     99        0.0914        0.0982  0.0160\n",
      "    100        0.0962        0.0995  0.0189\n",
      "    101        0.0975        0.1007  0.0140\n",
      "    102        0.0934        0.1018  0.0140\n",
      "    103        0.0947        0.1028  0.0189\n",
      "    104        0.0971        0.1037  0.0165\n",
      "    105        0.1003        0.1045  0.0149\n",
      "    106        0.0985        0.1051  0.0140\n",
      "    107        0.1000        0.1057  0.0140\n",
      "    108        0.0993        0.1061  0.0170\n",
      "    109        0.1004        0.1064  0.0150\n",
      "    110        0.0980        0.1066  0.0154\n",
      "    111        0.1029        0.1066  0.0150\n",
      "    112        0.0967        0.1066  0.0150\n",
      "    113        0.1032        0.1065  0.0140\n",
      "    114        0.0965        0.1063  0.0140\n",
      "    115        0.0988        0.1059  0.0140\n",
      "    116        0.0983        0.1055  0.0160\n",
      "    117        0.1030        0.1050  0.0140\n",
      "    118        0.0975        0.1044  0.0159\n",
      "    119        0.0984        0.1037  0.0130\n",
      "    120        0.1009        0.1029  0.0130\n",
      "    121        0.0968        0.1020  0.0140\n",
      "    122        0.0943        0.1011  0.0140\n",
      "    123        0.0915        0.1001  0.0130\n",
      "    124        0.0927        0.0990  0.0150\n",
      "    125        0.0885        0.0978  0.0150\n",
      "  epoch    train_loss    valid_loss     dur\n",
      "-------  ------------  ------------  ------\n",
      "      1        \u001b[36m0.2626\u001b[0m        \u001b[32m0.3528\u001b[0m  0.0134\n",
      "      2        \u001b[36m0.2586\u001b[0m        \u001b[32m0.3517\u001b[0m  0.0140\n",
      "      3        0.2622        \u001b[32m0.3500\u001b[0m  0.0140\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "      4        \u001b[36m0.2533\u001b[0m        \u001b[32m0.3478\u001b[0m  0.0139\n",
      "      5        0.2590        \u001b[32m0.3450\u001b[0m  0.0140\n",
      "      6        \u001b[36m0.2447\u001b[0m        \u001b[32m0.3416\u001b[0m  0.0140\n",
      "      7        0.2509        \u001b[32m0.3377\u001b[0m  0.0140\n",
      "      8        \u001b[36m0.2406\u001b[0m        \u001b[32m0.3333\u001b[0m  0.0140\n",
      "      9        \u001b[36m0.2398\u001b[0m        \u001b[32m0.3285\u001b[0m  0.0140\n",
      "     10        \u001b[36m0.2330\u001b[0m        \u001b[32m0.3232\u001b[0m  0.0150\n",
      "     11        0.2357        \u001b[32m0.3176\u001b[0m  0.0130\n",
      "     12        \u001b[36m0.2302\u001b[0m        \u001b[32m0.3116\u001b[0m  0.0130\n",
      "     13        \u001b[36m0.2214\u001b[0m        \u001b[32m0.3053\u001b[0m  0.0140\n",
      "     14        \u001b[36m0.2173\u001b[0m        \u001b[32m0.2987\u001b[0m  0.0140\n",
      "     15        \u001b[36m0.2107\u001b[0m        \u001b[32m0.2918\u001b[0m  0.0140\n",
      "     16        \u001b[36m0.2036\u001b[0m        \u001b[32m0.2848\u001b[0m  0.0140\n",
      "     17        \u001b[36m0.2018\u001b[0m        \u001b[32m0.2775\u001b[0m  0.0150\n",
      "     18        \u001b[36m0.1949\u001b[0m        \u001b[32m0.2701\u001b[0m  0.0160\n",
      "     19        \u001b[36m0.1868\u001b[0m        \u001b[32m0.2625\u001b[0m  0.0160\n",
      "     20        \u001b[36m0.1831\u001b[0m        \u001b[32m0.2548\u001b[0m  0.0140\n",
      "     21        \u001b[36m0.1762\u001b[0m        \u001b[32m0.2470\u001b[0m  0.0140\n",
      "     22        \u001b[36m0.1716\u001b[0m        \u001b[32m0.2391\u001b[0m  0.0160\n",
      "     23        \u001b[36m0.1601\u001b[0m        \u001b[32m0.2312\u001b[0m  0.0199\n",
      "     24        \u001b[36m0.1548\u001b[0m        \u001b[32m0.2234\u001b[0m  0.0229\n",
      "     25        \u001b[36m0.1510\u001b[0m        \u001b[32m0.2156\u001b[0m  0.0219\n",
      "     26        \u001b[36m0.1464\u001b[0m        \u001b[32m0.2078\u001b[0m  0.0189\n",
      "     27        \u001b[36m0.1390\u001b[0m        \u001b[32m0.2000\u001b[0m  0.0189\n",
      "     28        \u001b[36m0.1324\u001b[0m        \u001b[32m0.1924\u001b[0m  0.0170\n",
      "     29        \u001b[36m0.1276\u001b[0m        \u001b[32m0.1848\u001b[0m  0.0170\n",
      "     30        \u001b[36m0.1199\u001b[0m        \u001b[32m0.1774\u001b[0m  0.0180\n",
      "     31        \u001b[36m0.1147\u001b[0m        \u001b[32m0.1701\u001b[0m  0.0170\n",
      "     32        \u001b[36m0.1127\u001b[0m        \u001b[32m0.1630\u001b[0m  0.0170\n",
      "     33        \u001b[36m0.1029\u001b[0m        \u001b[32m0.1561\u001b[0m  0.0189\n",
      "     34        \u001b[36m0.1025\u001b[0m        \u001b[32m0.1494\u001b[0m  0.0190\n",
      "     35        \u001b[36m0.0949\u001b[0m        \u001b[32m0.1429\u001b[0m  0.0160\n",
      "     36        \u001b[36m0.0930\u001b[0m        \u001b[32m0.1366\u001b[0m  0.0140\n",
      "     37        \u001b[36m0.0887\u001b[0m        \u001b[32m0.1306\u001b[0m  0.0130\n",
      "     38        \u001b[36m0.0846\u001b[0m        \u001b[32m0.1248\u001b[0m  0.0140\n",
      "     39        \u001b[36m0.0799\u001b[0m        \u001b[32m0.1192\u001b[0m  0.0140\n",
      "     40        \u001b[36m0.0766\u001b[0m        \u001b[32m0.1139\u001b[0m  0.0140\n",
      "     41        \u001b[36m0.0749\u001b[0m        \u001b[32m0.1089\u001b[0m  0.0150\n",
      "     42        \u001b[36m0.0696\u001b[0m        \u001b[32m0.1041\u001b[0m  0.0160\n",
      "     43        \u001b[36m0.0680\u001b[0m        \u001b[32m0.0996\u001b[0m  0.0130\n",
      "     44        \u001b[36m0.0647\u001b[0m        \u001b[32m0.0953\u001b[0m  0.0140\n",
      "     45        \u001b[36m0.0630\u001b[0m        \u001b[32m0.0913\u001b[0m  0.0140\n",
      "     46        \u001b[36m0.0613\u001b[0m        \u001b[32m0.0875\u001b[0m  0.0160\n",
      "     47        \u001b[36m0.0592\u001b[0m        \u001b[32m0.0840\u001b[0m  0.0160\n",
      "     48        \u001b[36m0.0575\u001b[0m        \u001b[32m0.0807\u001b[0m  0.0140\n",
      "     49        \u001b[36m0.0553\u001b[0m        \u001b[32m0.0777\u001b[0m  0.0140\n",
      "     50        \u001b[36m0.0548\u001b[0m        \u001b[32m0.0749\u001b[0m  0.0140\n",
      "     51        \u001b[36m0.0541\u001b[0m        \u001b[32m0.0723\u001b[0m  0.0140\n",
      "     52        \u001b[36m0.0525\u001b[0m        \u001b[32m0.0700\u001b[0m  0.0150\n",
      "     53        \u001b[36m0.0518\u001b[0m        \u001b[32m0.0679\u001b[0m  0.0140\n",
      "     54        0.0521        \u001b[32m0.0659\u001b[0m  0.0150\n",
      "     55        0.0521        \u001b[32m0.0642\u001b[0m  0.0150\n",
      "     56        0.0524        \u001b[32m0.0627\u001b[0m  0.0150\n",
      "     57        0.0520        \u001b[32m0.0614\u001b[0m  0.0140\n",
      "     58        0.0536        \u001b[32m0.0602\u001b[0m  0.0160\n",
      "     59        0.0536        \u001b[32m0.0592\u001b[0m  0.0209\n",
      "     60        0.0535        \u001b[32m0.0584\u001b[0m  0.0160\n",
      "     61        0.0548        \u001b[32m0.0577\u001b[0m  0.0150\n",
      "     62        0.0545        \u001b[32m0.0571\u001b[0m  0.0140\n",
      "     63        0.0576        \u001b[32m0.0567\u001b[0m  0.0140\n",
      "     64        0.0572        \u001b[32m0.0564\u001b[0m  0.0140\n",
      "     65        0.0583        \u001b[32m0.0562\u001b[0m  0.0140\n",
      "     66        0.0609        \u001b[32m0.0561\u001b[0m  0.0130\n",
      "     67        0.0608        \u001b[32m0.0561\u001b[0m  0.0130\n",
      "     68        0.0626        0.0561  0.0140\n",
      "     69        0.0624        0.0563  0.0140\n",
      "     70        0.0653        0.0565  0.0130\n",
      "     71        0.0658        0.0567  0.0150\n",
      "     72        0.0686        0.0570  0.0219\n",
      "     73        0.0682        0.0573  0.0180\n",
      "     74        0.0692        0.0577  0.0140\n",
      "     75        0.0704        0.0581  0.0160\n",
      "     76        0.0723        0.0584  0.0160\n",
      "     77        0.0727        0.0588  0.0140\n",
      "     78        0.0750        0.0592  0.0140\n",
      "     79        0.0764        0.0596  0.0140\n",
      "     80        0.0769        0.0600  0.0130\n",
      "     81        0.0757        0.0604  0.0130\n",
      "     82        0.0770        0.0607  0.0130\n",
      "     83        0.0810        0.0611  0.0130\n",
      "     84        0.0797        0.0614  0.0130\n",
      "     85        0.0787        0.0616  0.0130\n",
      "     86        0.0833        0.0619  0.0130\n",
      "     87        0.0809        0.0621  0.0130\n",
      "     88        0.0849        0.0623  0.0130\n",
      "     89        0.0832        0.0624  0.0130\n",
      "     90        0.0852        0.0625  0.0130\n",
      "     91        0.0816        0.0626  0.0130\n",
      "     92        0.0848        0.0627  0.0130\n",
      "     93        0.0845        0.0627  0.0130\n",
      "     94        0.0823        0.0626  0.0140\n",
      "     95        0.0843        0.0626  0.0160\n",
      "     96        0.0815        0.0625  0.0199\n",
      "     97        0.0820        0.0624  0.0150\n",
      "     98        0.0852        0.0622  0.0150\n",
      "     99        0.0832        0.0621  0.0150\n",
      "    100        0.0815        0.0619  0.0170\n",
      "    101        0.0837        0.0616  0.0199\n",
      "    102        0.0819        0.0614  0.0149\n",
      "    103        0.0816        0.0611  0.0160\n",
      "    104        0.0810        0.0608  0.0140\n",
      "    105        0.0797        0.0606  0.0140\n",
      "    106        0.0797        0.0603  0.0159\n",
      "    107        0.0795        0.0599  0.0140\n",
      "    108        0.0776        0.0596  0.0139\n",
      "    109        0.0769        0.0593  0.0140\n",
      "    110        0.0774        0.0590  0.0140\n",
      "    111        0.0723        0.0587  0.0140\n",
      "    112        0.0723        0.0584  0.0140\n",
      "    113        0.0732        0.0581  0.0140\n",
      "    114        0.0722        0.0578  0.0130\n",
      "    115        0.0712        0.0576  0.0139\n",
      "    116        0.0703        0.0573  0.0130\n",
      "    117        0.0686        0.0571  0.0140\n",
      "    118        0.0667        0.0569  0.0140\n",
      "    119        0.0682        0.0567  0.0139\n",
      "    120        0.0676        0.0565  0.0140\n",
      "    121        0.0688        0.0564  0.0160\n",
      "    122        0.0639        0.0562  0.0190\n",
      "    123        0.0650        0.0562  0.0209\n",
      "    124        0.0627        0.0561  0.0150\n",
      "    125        0.0626        \u001b[32m0.0561\u001b[0m  0.0180\n",
      "  epoch    train_loss    valid_loss     dur\n",
      "-------  ------------  ------------  ------\n",
      "      1        \u001b[36m0.2008\u001b[0m        \u001b[32m0.2292\u001b[0m  0.0180\n",
      "      2        \u001b[36m0.1996\u001b[0m        \u001b[32m0.2282\u001b[0m  0.0199\n",
      "      3        \u001b[36m0.1987\u001b[0m        \u001b[32m0.2267\u001b[0m  0.0190\n",
      "      4        \u001b[36m0.1966\u001b[0m        \u001b[32m0.2247\u001b[0m  0.0180\n",
      "      5        \u001b[36m0.1952\u001b[0m        \u001b[32m0.2221\u001b[0m  0.0180\n",
      "      6        \u001b[36m0.1921\u001b[0m        \u001b[32m0.2192\u001b[0m  0.0180\n",
      "      7        \u001b[36m0.1893\u001b[0m        \u001b[32m0.2157\u001b[0m  0.0180\n",
      "      8        \u001b[36m0.1857\u001b[0m        \u001b[32m0.2119\u001b[0m  0.0180\n",
      "      9        \u001b[36m0.1826\u001b[0m        \u001b[32m0.2077\u001b[0m  0.0199\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\users\\being_aerys\\appdata\\local\\programs\\python\\python36\\lib\\site-packages\\sklearn\\model_selection\\_search.py:841: DeprecationWarning: The default of the `iid` parameter will change from True to False in version 0.22 and will be removed in 0.24. This will change numeric results when test-set sizes are unequal.\n",
      "  DeprecationWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "     10        \u001b[36m0.1778\u001b[0m        \u001b[32m0.2032\u001b[0m  0.0200\n",
      "     11        \u001b[36m0.1733\u001b[0m        \u001b[32m0.1984\u001b[0m  0.0189\n",
      "     12        \u001b[36m0.1694\u001b[0m        \u001b[32m0.1934\u001b[0m  0.0190\n",
      "     13        \u001b[36m0.1641\u001b[0m        \u001b[32m0.1882\u001b[0m  0.0189\n",
      "     14        \u001b[36m0.1587\u001b[0m        \u001b[32m0.1828\u001b[0m  0.0200\n",
      "     15        \u001b[36m0.1541\u001b[0m        \u001b[32m0.1772\u001b[0m  0.0189\n",
      "     16        \u001b[36m0.1483\u001b[0m        \u001b[32m0.1716\u001b[0m  0.0180\n",
      "     17        \u001b[36m0.1427\u001b[0m        \u001b[32m0.1659\u001b[0m  0.0190\n",
      "     18        \u001b[36m0.1374\u001b[0m        \u001b[32m0.1602\u001b[0m  0.0189\n",
      "     19        \u001b[36m0.1321\u001b[0m        \u001b[32m0.1544\u001b[0m  0.0189\n",
      "     20        \u001b[36m0.1264\u001b[0m        \u001b[32m0.1488\u001b[0m  0.0189\n",
      "     21        \u001b[36m0.1211\u001b[0m        \u001b[32m0.1431\u001b[0m  0.0189\n",
      "     22        \u001b[36m0.1159\u001b[0m        \u001b[32m0.1376\u001b[0m  0.0180\n",
      "     23        \u001b[36m0.1102\u001b[0m        \u001b[32m0.1322\u001b[0m  0.0189\n",
      "     24        \u001b[36m0.1050\u001b[0m        \u001b[32m0.1269\u001b[0m  0.0189\n",
      "     25        \u001b[36m0.1006\u001b[0m        \u001b[32m0.1218\u001b[0m  0.0180\n",
      "     26        \u001b[36m0.0959\u001b[0m        \u001b[32m0.1168\u001b[0m  0.0189\n",
      "     27        \u001b[36m0.0913\u001b[0m        \u001b[32m0.1120\u001b[0m  0.0180\n",
      "     28        \u001b[36m0.0865\u001b[0m        \u001b[32m0.1075\u001b[0m  0.0189\n",
      "     29        \u001b[36m0.0825\u001b[0m        \u001b[32m0.1032\u001b[0m  0.0189\n",
      "     30        \u001b[36m0.0784\u001b[0m        \u001b[32m0.0991\u001b[0m  0.0189\n",
      "     31        \u001b[36m0.0748\u001b[0m        \u001b[32m0.0952\u001b[0m  0.0180\n",
      "     32        \u001b[36m0.0713\u001b[0m        \u001b[32m0.0915\u001b[0m  0.0189\n",
      "     33        \u001b[36m0.0682\u001b[0m        \u001b[32m0.0882\u001b[0m  0.0180\n",
      "     34        \u001b[36m0.0650\u001b[0m        \u001b[32m0.0850\u001b[0m  0.0189\n",
      "     35        \u001b[36m0.0621\u001b[0m        \u001b[32m0.0821\u001b[0m  0.0259\n",
      "     36        \u001b[36m0.0599\u001b[0m        \u001b[32m0.0795\u001b[0m  0.0189\n",
      "     37        \u001b[36m0.0569\u001b[0m        \u001b[32m0.0771\u001b[0m  0.0189\n",
      "     38        \u001b[36m0.0553\u001b[0m        \u001b[32m0.0749\u001b[0m  0.0189\n",
      "     39        \u001b[36m0.0535\u001b[0m        \u001b[32m0.0730\u001b[0m  0.0189\n",
      "     40        \u001b[36m0.0517\u001b[0m        \u001b[32m0.0713\u001b[0m  0.0189\n",
      "     41        \u001b[36m0.0506\u001b[0m        \u001b[32m0.0698\u001b[0m  0.0190\n",
      "     42        \u001b[36m0.0496\u001b[0m        \u001b[32m0.0686\u001b[0m  0.0190\n",
      "     43        \u001b[36m0.0486\u001b[0m        \u001b[32m0.0675\u001b[0m  0.0180\n",
      "     44        \u001b[36m0.0480\u001b[0m        \u001b[32m0.0666\u001b[0m  0.0180\n",
      "     45        \u001b[36m0.0472\u001b[0m        \u001b[32m0.0660\u001b[0m  0.0180\n",
      "     46        \u001b[36m0.0467\u001b[0m        \u001b[32m0.0654\u001b[0m  0.0180\n",
      "     47        0.0467        \u001b[32m0.0651\u001b[0m  0.0190\n",
      "     48        \u001b[36m0.0465\u001b[0m        \u001b[32m0.0649\u001b[0m  0.0189\n",
      "     49        \u001b[36m0.0464\u001b[0m        \u001b[32m0.0648\u001b[0m  0.0189\n",
      "     50        0.0468        0.0648  0.0189\n",
      "     51        0.0468        0.0650  0.0189\n",
      "     52        0.0473        0.0653  0.0189\n",
      "     53        0.0476        0.0656  0.0180\n",
      "     54        0.0486        0.0660  0.0180\n",
      "     55        0.0486        0.0665  0.0189\n",
      "     56        0.0499        0.0670  0.0189\n",
      "     57        0.0503        0.0676  0.0189\n",
      "     58        0.0510        0.0682  0.0190\n",
      "     59        0.0517        0.0688  0.0189\n",
      "     60        0.0528        0.0695  0.0189\n",
      "     61        0.0533        0.0701  0.0259\n",
      "     62        0.0541        0.0707  0.0200\n",
      "     63        0.0550        0.0714  0.0189\n",
      "     64        0.0555        0.0720  0.0190\n",
      "     65        0.0563        0.0725  0.0189\n",
      "     66        0.0571        0.0731  0.0199\n",
      "     67        0.0579        0.0736  0.0200\n",
      "     68        0.0580        0.0741  0.0200\n",
      "     69        0.0586        0.0745  0.0190\n",
      "     70        0.0593        0.0749  0.0189\n",
      "     71        0.0596        0.0753  0.0189\n",
      "     72        0.0600        0.0755  0.0190\n",
      "     73        0.0605        0.0758  0.0189\n",
      "     74        0.0610        0.0760  0.0180\n",
      "     75        0.0607        0.0761  0.0189\n",
      "     76        0.0610        0.0762  0.0189\n",
      "     77        0.0609        0.0763  0.0190\n",
      "     78        0.0611        0.0763  0.0180\n",
      "     79        0.0611        0.0762  0.0189\n",
      "     80        0.0608        0.0761  0.0189\n",
      "     81        0.0611        0.0760  0.0189\n",
      "     82        0.0608        0.0758  0.0189\n",
      "     83        0.0606        0.0755  0.0189\n",
      "     84        0.0604        0.0753  0.0189\n",
      "     85        0.0598        0.0750  0.0269\n",
      "     86        0.0596        0.0747  0.0189\n",
      "     87        0.0592        0.0743  0.0189\n",
      "     88        0.0590        0.0740  0.0189\n",
      "     89        0.0580        0.0736  0.0189\n",
      "     90        0.0579        0.0732  0.0189\n",
      "     91        0.0573        0.0727  0.0210\n",
      "     92        0.0567        0.0723  0.0180\n",
      "     93        0.0564        0.0719  0.0190\n",
      "     94        0.0560        0.0714  0.0189\n",
      "     95        0.0552        0.0710  0.0189\n",
      "     96        0.0548        0.0706  0.0189\n",
      "     97        0.0544        0.0701  0.0200\n",
      "     98        0.0539        0.0697  0.0189\n",
      "     99        0.0532        0.0693  0.0189\n",
      "    100        0.0527        0.0689  0.0190\n",
      "    101        0.0523        0.0685  0.0180\n",
      "    102        0.0520        0.0681  0.0200\n",
      "    103        0.0516        0.0677  0.0189\n",
      "    104        0.0513        0.0674  0.0190\n",
      "    105        0.0506        0.0671  0.0189\n",
      "    106        0.0500        0.0668  0.0190\n",
      "    107        0.0498        0.0665  0.0190\n",
      "    108        0.0491        0.0662  0.0189\n",
      "    109        0.0488        0.0660  0.0249\n",
      "    110        0.0487        0.0658  0.0189\n",
      "    111        0.0482        0.0656  0.0189\n",
      "    112        0.0477        0.0654  0.0190\n",
      "    113        0.0480        0.0653  0.0200\n",
      "    114        0.0479        0.0651  0.0200\n",
      "    115        0.0475        0.0650  0.0190\n",
      "    116        0.0472        0.0649  0.0180\n",
      "    117        0.0473        0.0649  0.0189\n",
      "    118        0.0469        0.0648  0.0199\n",
      "    119        0.0469        0.0648  0.0189\n",
      "    120        0.0466        \u001b[32m0.0648\u001b[0m  0.0189\n",
      "    121        0.0468        0.0648  0.0190\n",
      "    122        0.0465        0.0648  0.0189\n",
      "    123        0.0466        0.0649  0.0189\n",
      "    124        0.0466        0.0649  0.0189\n",
      "    125        \u001b[36m0.0464\u001b[0m        0.0650  0.0189\n"
     ]
    }
   ],
   "source": [
    "assert len(X_training) == len(Y_training)\n",
    "randomized_search_result = sgd_randomized_search.fit(X_training.values, Y_training.values)\n",
    "\n",
    "#dataframe.values gives the underlying numpy array of the data frame, doing this conversion since pandas data frame \n",
    "#is not supported."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Save and view the best parameters and hyperparameters obtained from randomized search."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\users\\being_aerys\\appdata\\local\\programs\\python\\python36\\lib\\site-packages\\torch\\serialization.py:360: UserWarning: Couldn't retrieve source code for container of type Regression_Module. It won't be checked for correctness upon loading.\n",
      "  \"type \" + obj.__name__ + \". It won't be checked \"\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'optimizer__weight_decay': 5e-05,\n",
       " 'optimizer__nesterov': False,\n",
       " 'optimizer__momentum': 0.99,\n",
       " 'optimizer__dampening': 0.0,\n",
       " 'optimizer': torch.optim.sgd.SGD,\n",
       " 'module__num_units': 14,\n",
       " 'module__dropout': 0.5,\n",
       " 'max_epochs': 125,\n",
       " 'lr': 0.0001,\n",
       " 'iterator_train__batch_size': 32}"
      ]
     },
     "execution_count": 77,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# randomized_search_result.best_estimator_ gives the neural net with best parameters\n",
    "# randomized_search_result.best_params_ gives the best hyperparameters that were used to get this best estimator\n",
    "\n",
    "#saving the best estimator i.e., the neural network parameters\n",
    "joblib.dump(randomized_search_result.best_estimator_, 'best_params_of_NN_by_randomized_search.pkl', compress = 1)\n",
    "\n",
    "#saving the best hyperparameters\n",
    "joblib.dump(randomized_search_result.best_params_, 'best_hyperparams_for_NN_by_randomized_search.pkl', compress = 1)\n",
    "\n",
    "randomized_search_result.best_params_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [],
   "source": [
    "def report(results, num_of_top_results_to_report = 5):\n",
    "    #this utility method was found online.\n",
    "    \n",
    "    for idx in range(0, num_of_top_results_to_report):\n",
    "        candidates = np.flatnonzero(results['rank_test_score'] == idx)\n",
    "        for candidate in candidates:\n",
    "            print(\"Model with rank: {0}\".format(idx))\n",
    "            print(\"Mean validation score: {0:.3f} (std: {1:.3f})\".format(\n",
    "                  results['mean_test_score'][candidate],\n",
    "                  results['std_test_score'][candidate]))\n",
    "            print(\"Parameters: {0}\".format(results['params'][candidate]))\n",
    "            print(\"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model with rank: 1\n",
      "Mean validation score: -0.079 (std: 0.030)\n",
      "Parameters: {'optimizer__weight_decay': 5e-05, 'optimizer__nesterov': False, 'optimizer__momentum': 0.99, 'optimizer__dampening': 0.0, 'optimizer': <class 'torch.optim.sgd.SGD'>, 'module__num_units': 14, 'module__dropout': 0.5, 'max_epochs': 125, 'lr': 0.0001, 'iterator_train__batch_size': 32}\n",
      "\n",
      "Model with rank: 2\n",
      "Mean validation score: -0.124 (std: 0.036)\n",
      "Parameters: {'optimizer__weight_decay': 0.005, 'optimizer__nesterov': False, 'optimizer__momentum': 0.5, 'optimizer__dampening': 0.0, 'optimizer': <class 'torch.optim.sgd.SGD'>, 'module__num_units': 14, 'module__dropout': 0.5, 'max_epochs': 150, 'lr': 0.001, 'iterator_train__batch_size': 64}\n",
      "\n"
     ]
    }
   ],
   "source": [
    "report(sgd_randomized_search.cv_results_,num_of_top_results_to_report = 3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Visualize training and validation loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get training and validation loss\n",
    "epochs = [i for i in range(len(sgd_randomized_search.best_estimator_.history))]\n",
    "training_loss = sgd_randomized_search.best_estimator_.history[:,'train_loss']\n",
    "validation_loss = sgd_randomized_search.best_estimator_.history[:,'valid_loss']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.plot(epochs,training_loss,'g-');\n",
    "plt.plot(epochs,validation_loss,'r-');\n",
    "plt.title('Training and Validation Loss Curves');\n",
    "plt.xlabel('Epochs');\n",
    "plt.ylabel('Mean Squared Error');\n",
    "plt.legend(['Train','Validation']);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Test on Test Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>% of Cr</th>\n",
       "      <th>% of Hf</th>\n",
       "      <th>% of Mo</th>\n",
       "      <th>% of Nb</th>\n",
       "      <th>% of Ta</th>\n",
       "      <th>% of Ti</th>\n",
       "      <th>% of V</th>\n",
       "      <th>% of Zr</th>\n",
       "      <th>% of Ni</th>\n",
       "      <th>% of Al</th>\n",
       "      <th>% of Mn</th>\n",
       "      <th>%Cu</th>\n",
       "      <th>%C</th>\n",
       "      <th>Entropy</th>\n",
       "      <th>Hardness</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.666667</td>\n",
       "      <td>0.500</td>\n",
       "      <td>0.8</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.8</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.667795</td>\n",
       "      <td>0.296870</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.500</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.250</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.354613</td>\n",
       "      <td>0.462085</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.000</td>\n",
       "      <td>0.666667</td>\n",
       "      <td>0.500</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.333333</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.527139</td>\n",
       "      <td>0.893030</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.833333</td>\n",
       "      <td>0.000</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.833333</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.296870</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.625</td>\n",
       "      <td>0.833333</td>\n",
       "      <td>0.625</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.489351</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   % of Cr  % of Hf  % of Mo   % of Nb  % of Ta  % of Ti    % of V  % of Zr  \\\n",
       "0      0.0      1.0    0.000  0.666667    0.500      0.8  0.000000      0.8   \n",
       "1      0.0      0.0    0.500  1.000000    0.250      0.0  1.000000      0.0   \n",
       "2      0.0      0.0    1.000  0.666667    0.500      0.0  0.333333      0.0   \n",
       "3      0.0      0.0    0.000  0.833333    0.000      1.0  0.833333      1.0   \n",
       "4      0.0      0.0    0.625  0.833333    0.625      0.0  0.000000      0.0   \n",
       "\n",
       "   % of Ni  % of Al  % of Mn  %Cu   %C   Entropy  Hardness  \n",
       "0      0.0      0.0      0.0  0.0  0.0  0.667795  0.296870  \n",
       "1      0.0      0.0      0.0  0.0  0.0  0.354613  0.462085  \n",
       "2      0.0      0.0      0.0  0.0  0.0  0.527139  0.893030  \n",
       "3      0.0      0.0      0.0  0.0  0.0  0.000000  0.296870  \n",
       "4      0.0      0.0      0.0  0.0  0.0  0.000000  0.489351  "
      ]
     },
     "execution_count": 82,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Predict on test data\n",
    "testing_data = pd.read_csv(\"normalized_testing_features_and_targets.csv\", sep = \",\")\n",
    "testing_data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Separate features and targets\n",
    "X_testing = testing_data.iloc[:,0:num_features].values #notice retaining only the underlying numpy arrays\n",
    "Y_testing = pd.DataFrame(testing_data[\"Hardness\"]).values\n",
    "\n",
    "Y_predictions_on_test_data = sgd_randomized_search.best_estimator_.predict(X_testing.astype(np.float32))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0.29686996],\n",
       "       [0.46208454],\n",
       "       [0.89303001],\n",
       "       [0.29686996],\n",
       "       [0.4893514 ],\n",
       "       [1.        ],\n",
       "       [0.98693127],\n",
       "       [0.09035173],\n",
       "       [0.56082607],\n",
       "       [0.87011939],\n",
       "       [0.62003872],\n",
       "       [0.        ]])"
      ]
     },
     "execution_count": 84,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Y_testing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0.47764346],\n",
       "       [0.4777196 ],\n",
       "       [0.47785416],\n",
       "       [0.47773674],\n",
       "       [0.4778147 ],\n",
       "       [0.47785121],\n",
       "       [0.47786573],\n",
       "       [0.47772494],\n",
       "       [0.47774139],\n",
       "       [0.47786519],\n",
       "       [0.47779754],\n",
       "       [0.47778457]])"
      ]
     },
     "execution_count": 85,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Y_predictions_on_test_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.3328240970917553"
      ]
     },
     "execution_count": 86,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# get Root Mean Squared Error\n",
    "MSE(Y_testing,Y_predictions_on_test_data)**(1/2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Kernel Density Estimation Plot\n",
    "\n",
    "# sns.kdeplot(Y_predictions_on_test_data.squeeze(), label='predictions of the model', shade=True)\n",
    "# sns.kdeplot(Y_testing.squeeze(), label='true values', shade=True)\n",
    "# plt.xlabel('Hardness');"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Dist Plot\n",
    "\n",
    "# sns.distplot(Y_testing.squeeze()-Y_predictions_on_test_data.squeeze(),label='error', bins = 10);\n",
    "# plt.xlabel('Error');"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-0.04516772582523787\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# R^2 plot\n",
    "\n",
    "print(r2_score(Y_testing,Y_predictions_on_test_data))\n",
    "\n",
    "plt.plot(Y_predictions_on_test_data,Y_testing,'g*')\n",
    "plt.xlabel('predicted')\n",
    "plt.ylabel('actual')\n",
    "plt.title('$R^{2}$ Plot');"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### View the actual values vs predicted values in real scale"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "True Hardness: [373.65640529]\n",
      "Predicted Hardness: [530.5515354]\n",
      "\n",
      "True Hardness: [517.04779606]\n",
      "Predicted Hardness: [530.61762235]\n",
      "\n",
      "True Hardness: [891.0696757]\n",
      "Predicted Hardness: [530.73440614]\n",
      "\n",
      "True Hardness: [373.65640529]\n",
      "Predicted Hardness: [530.63249515]\n",
      "\n",
      "True Hardness: [540.71297677]\n",
      "Predicted Hardness: [530.7001599]\n",
      "\n",
      "True Hardness: [983.91]\n",
      "Predicted Hardness: [530.73184543]\n",
      "\n",
      "True Hardness: [972.56751694]\n",
      "Predicted Hardness: [530.74444204]\n",
      "\n",
      "True Hardness: [194.41716683]\n",
      "Predicted Hardness: [530.62225231]\n",
      "\n",
      "True Hardness: [602.74655695]\n",
      "Predicted Hardness: [530.6365302]\n",
      "\n",
      "True Hardness: [871.18532268]\n",
      "Predicted Hardness: [530.74397646]\n",
      "\n",
      "True Hardness: [654.13780736]\n",
      "Predicted Hardness: [530.68526124]\n",
      "\n",
      "True Hardness: [116.]\n",
      "Predicted Hardness: [530.67400965]\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#The maximum and minimum targets of the training data were [983.91] and [116.] from feature engineering file\n",
    "\n",
    "max_hardness = 983.91\n",
    "min_hardness = 116.\n",
    "\n",
    "def convert_to_original_scale(scaled_hardness):\n",
    "    original_scale_hardness = (scaled_hardness * (max_hardness - min_hardness)) + min_hardness\n",
    "    return original_scale_hardness\n",
    "\n",
    "for test_sample in range(len(Y_predictions_on_test_data)):\n",
    "    print(\"True Hardness: {}\".format(convert_to_original_scale(Y_testing[test_sample])))\n",
    "    print(\"Predicted Hardness: {}\".format(convert_to_original_scale(Y_predictions_on_test_data[test_sample])))\n",
    "    print(\"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
