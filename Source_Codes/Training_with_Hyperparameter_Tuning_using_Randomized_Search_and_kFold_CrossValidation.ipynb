{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Given the scarcity of data in this task, we will use k-Fold cross-validation for training with k set to a large value to make a good use of the available data of a significantly small size."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Following are the hyperparameters that can be tuned for a neural network\n",
    "\n",
    "1. learning rate\n",
    "2. no of hidden units\n",
    "3. no of epochs to train\n",
    "4. dropout probability\n",
    "5. loss function (NOT TUNED HERE)\n",
    "6. mini batch size\n",
    "7. weights initialization (NOT TUNED HERE)\n",
    "8. l1/ l2 regularizers\n",
    "9. activation function to use at the nodes (NOT TUNED HERE)\n",
    "10. no of layers (NOT TUNED HERE)\n",
    "11. learning rate decay (NOT TUNED HERE)\n",
    "12. optimizer (NOT TUNED HERE)\n",
    "13. momentum (only if sgd or rmsprop optimizer used, not with adam, adagrad)\n",
    "14. momentum_dampening (only if sgd or rmsprop optimizer used, not with adam, adagrad) (NOT TUNED HERE)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There are several methods to tune hyperparameters of a neural network. Two of them are grid search and randomized search that select points in the parameter space and evaluate these points (each point is essestially a unique configuration of hyperparameters in the hyperparameter space) and return the best hyperparameter combination based on the performace on the validation data.\n",
    "\n",
    "#### How is grid search/ randomized search done with k-fold cross validation?\n",
    "1. Select n points in the hyperparameter space. For each point, do:\n",
    "    a. Each point corresponds to a hyperparameter configuration in the hyperparameter space.\n",
    "    b. Train and evaluate this model k times as follows.\n",
    "        i. Randomly divide the training data into k partitions. Call them partition 1, partition 2,.........., partition k.\n",
    "        ii. Repeat for each partition j starting from j = 1 to j = k\n",
    "            . Train the model with this hyperparameter configuration on the partitions except partition j and test on                     partition j.\n",
    "            . Store the performance of the model.\n",
    "        iii. Calculate the average performance of the model with this hyperparameter configuration over the k folds.\n",
    "2. Declare the model that was trained using the hyperparameters corresponding to the point that achieved the best              validation result as the best model and declare this choice of hyperparameters as the best hyperparameters.\n",
    "3. Predict the labels of the testing data using this model."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Import Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "import copy\n",
    "import sys\n",
    "import os\n",
    "from math import sqrt\n",
    "\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "import pandas as pd\n",
    "\n",
    "import seaborn as sns\n",
    "\n",
    "import sklearn\n",
    "from sklearn.externals import joblib\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from skorch import NeuralNetRegressor\n",
    "from sklearn.model_selection import RandomizedSearchCV\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "\n",
    "from sklearn.metrics import mean_squared_error as MSE\n",
    "from sklearn.metrics import r2_score\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The python version used is 3.6.8 (tags/v3.6.8:3c6b436a57, Dec 24 2018, 00:16:47) [MSC v.1916 64 bit (AMD64)]\n",
      "The torch version used is 1.4.0\n",
      "The sklearn version used is 0.20.3\n"
     ]
    }
   ],
   "source": [
    "print(\"The python version used is {}\".format(sys.version))\n",
    "print(\"The torch version used is {}\".format(torch.__version__))\n",
    "print(\"The sklearn version used is {}\".format(sklearn.__version__))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Import Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>% of Cr</th>\n",
       "      <th>% of Hf</th>\n",
       "      <th>% of Mo</th>\n",
       "      <th>% of Nb</th>\n",
       "      <th>% of Ta</th>\n",
       "      <th>% of Ti</th>\n",
       "      <th>% of V</th>\n",
       "      <th>% of Zr</th>\n",
       "      <th>% of Ni</th>\n",
       "      <th>% of Al</th>\n",
       "      <th>% of Mn</th>\n",
       "      <th>%Cu</th>\n",
       "      <th>%C</th>\n",
       "      <th>Entropy</th>\n",
       "      <th>Hardness</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.470</td>\n",
       "      <td>0.44</td>\n",
       "      <td>0.97491</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.992366</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.138686</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.602527</td>\n",
       "      <td>0.429768</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.7500</td>\n",
       "      <td>0.200</td>\n",
       "      <td>0.25</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.4000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.589731</td>\n",
       "      <td>0.539457</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.5815</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.4026</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.396171</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.733380</td>\n",
       "      <td>0.559966</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.5000</td>\n",
       "      <td>0.800</td>\n",
       "      <td>0.25</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.4000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.541151</td>\n",
       "      <td>0.351534</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.468</td>\n",
       "      <td>0.33</td>\n",
       "      <td>0.97491</td>\n",
       "      <td>0.0860</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.138686</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.741308</td>\n",
       "      <td>0.441290</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   % of Cr  % of Hf  % of Mo  % of Nb  % of Ta  % of Ti  % of V   % of Zr  \\\n",
       "0      0.0      0.0   0.0000    0.470     0.44  0.97491  0.0000  0.992366   \n",
       "1      0.0      0.0   0.7500    0.200     0.25  0.00000  0.4000  0.000000   \n",
       "2      0.0      0.0   0.5815    0.000     0.00  0.00000  0.4026  0.000000   \n",
       "3      0.0      0.0   0.5000    0.800     0.25  0.00000  0.4000  0.000000   \n",
       "4      0.0      0.0   0.0000    0.468     0.33  0.97491  0.0860  1.000000   \n",
       "\n",
       "    % of Ni   % of Al  % of Mn  %Cu   %C   Entropy  Hardness  \n",
       "0  0.000000  0.138686      0.0  0.0  0.0  0.602527  0.429768  \n",
       "1  0.000000  0.000000      0.0  0.0  0.0  0.589731  0.539457  \n",
       "2  0.396171  0.000000      0.0  0.0  0.0  0.733380  0.559966  \n",
       "3  0.000000  0.000000      0.0  0.0  0.0  0.541151  0.351534  \n",
       "4  0.000000  0.138686      0.0  0.0  0.0  0.741308  0.441290  "
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "training_data = pd.read_csv(\"normalized_training_features_and_targets.csv\", sep = \",\")\n",
    "training_data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The number of features is 14\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>% of Cr</th>\n",
       "      <th>% of Hf</th>\n",
       "      <th>% of Mo</th>\n",
       "      <th>% of Nb</th>\n",
       "      <th>% of Ta</th>\n",
       "      <th>% of Ti</th>\n",
       "      <th>% of V</th>\n",
       "      <th>% of Zr</th>\n",
       "      <th>% of Ni</th>\n",
       "      <th>% of Al</th>\n",
       "      <th>% of Mn</th>\n",
       "      <th>%Cu</th>\n",
       "      <th>%C</th>\n",
       "      <th>Entropy</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.470</td>\n",
       "      <td>0.44</td>\n",
       "      <td>0.97491</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.992366</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.138686</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.602527</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.7500</td>\n",
       "      <td>0.200</td>\n",
       "      <td>0.25</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.4000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.589731</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.5815</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.4026</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.396171</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.733380</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.5000</td>\n",
       "      <td>0.800</td>\n",
       "      <td>0.25</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.4000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.541151</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.468</td>\n",
       "      <td>0.33</td>\n",
       "      <td>0.97491</td>\n",
       "      <td>0.0860</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.138686</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.741308</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   % of Cr  % of Hf  % of Mo  % of Nb  % of Ta  % of Ti  % of V   % of Zr  \\\n",
       "0      0.0      0.0   0.0000    0.470     0.44  0.97491  0.0000  0.992366   \n",
       "1      0.0      0.0   0.7500    0.200     0.25  0.00000  0.4000  0.000000   \n",
       "2      0.0      0.0   0.5815    0.000     0.00  0.00000  0.4026  0.000000   \n",
       "3      0.0      0.0   0.5000    0.800     0.25  0.00000  0.4000  0.000000   \n",
       "4      0.0      0.0   0.0000    0.468     0.33  0.97491  0.0860  1.000000   \n",
       "\n",
       "    % of Ni   % of Al  % of Mn  %Cu   %C   Entropy  \n",
       "0  0.000000  0.138686      0.0  0.0  0.0  0.602527  \n",
       "1  0.000000  0.000000      0.0  0.0  0.0  0.589731  \n",
       "2  0.396171  0.000000      0.0  0.0  0.0  0.733380  \n",
       "3  0.000000  0.000000      0.0  0.0  0.0  0.541151  \n",
       "4  0.000000  0.138686      0.0  0.0  0.0  0.741308  "
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "num_features = len( training_data.iloc[0,:] ) - 1\n",
    "print(\"The number of features is {}\".format(num_features))\n",
    "X_training = training_data.iloc[:,0:num_features]\n",
    "\n",
    "X_training.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Hardness</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.429768</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.539457</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.559966</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.351534</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.441290</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   Hardness\n",
       "0  0.429768\n",
       "1  0.539457\n",
       "2  0.559966\n",
       "3  0.351534\n",
       "4  0.441290"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Y_training = pd.DataFrame(training_data[\"Hardness\"])\n",
    "Y_training.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Initialize Seeds for Reproducibility"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.random.seed(0)\n",
    "torch.manual_seed(0)\n",
    "torch.cuda.manual_seed(0)\n",
    "\n",
    "device_to_use = \"cuda\" if torch.cuda.device_count() > 0 else \"cpu\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## torch Regression Model definition"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Regression_Module(nn.Module):\n",
    "    def __init__(self, num_units = 10, dropout = 0.5, activation = F.leaky_relu, input_dim = num_features, output_dim = 1):\n",
    "           \n",
    "        \n",
    "        super(Regression_Module, self).__init__()\n",
    "        self.activation = activation\n",
    "        \n",
    "        \n",
    "        self.L1 = nn.Linear(input_dim, num_units)\n",
    "        self.L2 = nn.Linear(num_units, num_units)\n",
    "        \n",
    "        #self.batchnorm_1 = nn.BatchNorm1d(num_units, 1e-12, affine=True, track_running_stats=True)\n",
    "        self.dropout_1 = nn.Dropout(p=dropout)\n",
    "        \n",
    "        self.L3 = nn.Linear(num_units, num_units)\n",
    "        self.L4 = nn.Linear(num_units, num_units)\n",
    "        \n",
    "        #self.batchnorm_2 = nn.BatchNorm1d(num_units, 1e-12, affine=True, track_running_stats=True)\n",
    "        self.dropout_2 = nn.Dropout(p=dropout)\n",
    "        \n",
    "        self.L5 = nn.Linear(num_units, num_units)\n",
    "        self.L6 = nn.Linear(num_units, output_dim)\n",
    "\n",
    "\n",
    "\n",
    "    def forward(self, input):\n",
    "            \n",
    "        input = self.activation(self.L1(input.float()))\n",
    "        input = self.L2(input.float())\n",
    "\n",
    "        # input = input.unsqueeze(0) #https://discuss.pytorch.org/t/batchnorm1d-valueerror-expected-2d-or-3d-input-got-1d-input/42081\n",
    "        # input = self.batchnorm_1(input)\n",
    "\n",
    "        input = self.activation(input.float())\n",
    "        input = self.dropout_1(input.float())\n",
    "\n",
    "        input = self.activation(self.L3(input.float()))\n",
    "        input = self.L4(input.float())\n",
    "\n",
    "        # input = input.unsqueeze(0)\n",
    "        # input = self.batchnorm_2(input)\n",
    "\n",
    "        input = self.activation(input.float())\n",
    "        input = self.dropout_2(input.float())\n",
    "\n",
    "        input = self.activation(self.L5(input.float()))\n",
    "\n",
    "        input = self.L6(input.float())\n",
    "\n",
    "\n",
    "        #VVI: need to return in double format instead of a float\n",
    "        return input.double()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## skorch Regression Model definition\n",
    "Takes torch regressor as an argument."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "skorch_regressor = NeuralNetRegressor(module = Regression_Module,  #pass a torch module class\n",
    "                                      device = device_to_use,\n",
    "                                      iterator_train__shuffle = True,\n",
    "                                      \n",
    "                                     ) \n",
    "                                      "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Generate hyperparameters for hyperparameter tuning using sklearn's Randomized Search\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "#lr = np.random.uniform(low = 0.000001,high = 0.05, size = 20).tolist()\n",
    "lr = [0.00005, 0.0001, 0.0003, 0.0005, 0.001, 0.003, 0.005]\n",
    "\n",
    "weight_decay_for_regularization = [1e-5, 5e-5, 1e-4, 5e-5, 1e-3, 5e-3, 1e-2, 5e-2, 1e-1]      #weight decay equals L2 regularization for SGD\n",
    "\n",
    "momentum_vals = [0.5, 0.75, 0.99]\n",
    "\n",
    "momentum_dampening = [0.]\n",
    "\n",
    "nesterov = [True, False]\n",
    "\n",
    "no_of_nodes_per_layer = [num_features, num_features * int(1.25), num_features * int(1.5), num_features * int(1.75), num_features *2]\n",
    "\n",
    "#max_epochs = [epoch_num for epoch_num in range(25, 400, 25)]\n",
    "max_epochs = [50, 75, 100, 125, 150, 175, 200, 250, 300]\n",
    "\n",
    "dropout_probability_per_node = [0., 0.3, 0.5]\n",
    "\n",
    "#We will use onle mse as the loss function for now. so skip tuning the loss function.\n",
    "\n",
    "minibatch_size = [8, 16, 32, 64] #should always be less than the size of the trianing set\n",
    "\n",
    "#optimizers = [torch.optim.SGD, torch.optim.RMSprop, torch.optim.Adagrad]\n",
    "optimizers = [torch.optim.SGD] #Using only SGD"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Create the grid for selection of random points on the hyperparameter space.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "common_param_grid = {\n",
    "    \n",
    "    \"lr\" : lr,\n",
    "    \"max_epochs\" : max_epochs,\n",
    "    \n",
    "    \n",
    "    \n",
    "    #list all the formal arguments of the torch module that u pass to skorch regressor here beginning with module__\n",
    "    \"module__num_units\" : no_of_nodes_per_layer,\n",
    "    \"module__dropout\" : dropout_probability_per_node,\n",
    "    #not passing the activation here though\n",
    "    \n",
    "    \n",
    "    \n",
    "    \"optimizer\" : optimizers,\n",
    "    \"optimizer__weight_decay\": weight_decay_for_regularization,\n",
    "    \"optimizer__momentum\" : momentum_vals,\n",
    "    \"optimizer__dampening\" : momentum_dampening,\n",
    "    \"optimizer__nesterov\" : nesterov, \n",
    "    \n",
    "    \n",
    "    \n",
    "    \"iterator_train__batch_size\": minibatch_size,\n",
    "    #\"callbacks__scheduler__epoch\": [10, 50, 100], #learning rate scheduler    \n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Create a Randomized Search Instance and pass the estimator, thehyperparameters' grid, the no of random samples to take from the grid, and the value of k for k-Fold cross-validation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "sgd_randomized_search = RandomizedSearchCV(estimator = skorch_regressor,\n",
    "                                          scoring = \"neg_mean_squared_error\", #negative since the scorer tries to maximize this\n",
    "                                          param_distributions = common_param_grid,\n",
    "                                          n_iter = 1, #350,  #no of random samples to take from the grid                                                    \n",
    "                                          cv = 2, #20,\n",
    "                                          return_train_score = True # produce metrics on the train set as well\n",
    "                                          )\n",
    "\n",
    "#skorch_regressor.get_params().keys()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Train the models."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  epoch    train_loss    valid_loss     dur\n",
      "-------  ------------  ------------  ------\n",
      "      1        \u001b[36m0.5185\u001b[0m        \u001b[32m0.4200\u001b[0m  0.2057\n",
      "      2        \u001b[36m0.4923\u001b[0m        \u001b[32m0.3896\u001b[0m  0.0080\n",
      "      3        \u001b[36m0.4594\u001b[0m        \u001b[32m0.3560\u001b[0m  0.0080\n",
      "      4        \u001b[36m0.4230\u001b[0m        \u001b[32m0.3215\u001b[0m  0.0080\n",
      "      5        \u001b[36m0.3854\u001b[0m        \u001b[32m0.2876\u001b[0m  0.0080\n",
      "      6        \u001b[36m0.3483\u001b[0m        \u001b[32m0.2554\u001b[0m  0.0079\n",
      "      7        \u001b[36m0.3128\u001b[0m        \u001b[32m0.2255\u001b[0m  0.0080\n",
      "      8        \u001b[36m0.2797\u001b[0m        \u001b[32m0.1982\u001b[0m  0.0109\n",
      "      9        \u001b[36m0.2492\u001b[0m        \u001b[32m0.1737\u001b[0m  0.0090\n",
      "     10        \u001b[36m0.2217\u001b[0m        \u001b[32m0.1521\u001b[0m  0.0100\n",
      "     11        \u001b[36m0.1970\u001b[0m        \u001b[32m0.1331\u001b[0m  0.0089\n",
      "     12        \u001b[36m0.1752\u001b[0m        \u001b[32m0.1167\u001b[0m  0.0100\n",
      "     13        \u001b[36m0.1560\u001b[0m        \u001b[32m0.1026\u001b[0m  0.0080\n",
      "     14        \u001b[36m0.1393\u001b[0m        \u001b[32m0.0905\u001b[0m  0.0090\n",
      "     15        \u001b[36m0.1249\u001b[0m        \u001b[32m0.0802\u001b[0m  0.0090\n",
      "     16        \u001b[36m0.1123\u001b[0m        \u001b[32m0.0716\u001b[0m  0.0090\n",
      "     17        \u001b[36m0.1016\u001b[0m        \u001b[32m0.0645\u001b[0m  0.0090\n",
      "     18        \u001b[36m0.0925\u001b[0m        \u001b[32m0.0586\u001b[0m  0.0080\n",
      "     19        \u001b[36m0.0848\u001b[0m        \u001b[32m0.0539\u001b[0m  0.0079\n",
      "     20        \u001b[36m0.0783\u001b[0m        \u001b[32m0.0500\u001b[0m  0.0080\n",
      "     21        \u001b[36m0.0729\u001b[0m        \u001b[32m0.0469\u001b[0m  0.0110\n",
      "     22        \u001b[36m0.0684\u001b[0m        \u001b[32m0.0444\u001b[0m  0.0090\n",
      "     23        \u001b[36m0.0646\u001b[0m        \u001b[32m0.0425\u001b[0m  0.0080\n",
      "     24        \u001b[36m0.0615\u001b[0m        \u001b[32m0.0410\u001b[0m  0.0100\n",
      "     25        \u001b[36m0.0589\u001b[0m        \u001b[32m0.0399\u001b[0m  0.0090\n",
      "     26        \u001b[36m0.0568\u001b[0m        \u001b[32m0.0390\u001b[0m  0.0100\n",
      "     27        \u001b[36m0.0551\u001b[0m        \u001b[32m0.0384\u001b[0m  0.0110\n",
      "     28        \u001b[36m0.0537\u001b[0m        \u001b[32m0.0379\u001b[0m  0.0109\n",
      "     29        \u001b[36m0.0525\u001b[0m        \u001b[32m0.0376\u001b[0m  0.0089\n",
      "     30        \u001b[36m0.0515\u001b[0m        \u001b[32m0.0373\u001b[0m  0.0090\n",
      "     31        \u001b[36m0.0507\u001b[0m        \u001b[32m0.0372\u001b[0m  0.0109\n",
      "     32        \u001b[36m0.0501\u001b[0m        \u001b[32m0.0371\u001b[0m  0.0120\n",
      "     33        \u001b[36m0.0496\u001b[0m        \u001b[32m0.0371\u001b[0m  0.0100\n",
      "     34        \u001b[36m0.0491\u001b[0m        \u001b[32m0.0371\u001b[0m  0.0110\n",
      "     35        \u001b[36m0.0488\u001b[0m        0.0371  0.0090\n",
      "     36        \u001b[36m0.0485\u001b[0m        0.0371  0.0090\n",
      "     37        \u001b[36m0.0483\u001b[0m        0.0372  0.0080\n",
      "     38        \u001b[36m0.0481\u001b[0m        0.0372  0.0089\n",
      "     39        \u001b[36m0.0479\u001b[0m        0.0373  0.0090\n",
      "     40        \u001b[36m0.0478\u001b[0m        0.0373  0.0090\n",
      "     41        \u001b[36m0.0477\u001b[0m        0.0374  0.0080\n",
      "     42        \u001b[36m0.0476\u001b[0m        0.0374  0.0110\n",
      "     43        \u001b[36m0.0475\u001b[0m        0.0375  0.0090\n",
      "     44        \u001b[36m0.0474\u001b[0m        0.0375  0.0090\n",
      "     45        \u001b[36m0.0474\u001b[0m        0.0376  0.0130\n",
      "     46        \u001b[36m0.0473\u001b[0m        0.0376  0.0100\n",
      "     47        \u001b[36m0.0473\u001b[0m        0.0376  0.0080\n",
      "     48        \u001b[36m0.0473\u001b[0m        0.0376  0.0100\n",
      "     49        \u001b[36m0.0472\u001b[0m        0.0377  0.0100\n",
      "     50        \u001b[36m0.0472\u001b[0m        0.0377  0.0090\n",
      "     51        \u001b[36m0.0472\u001b[0m        0.0377  0.0110\n",
      "     52        \u001b[36m0.0472\u001b[0m        0.0377  0.0110\n",
      "     53        \u001b[36m0.0472\u001b[0m        0.0378  0.0090\n",
      "     54        \u001b[36m0.0472\u001b[0m        0.0378  0.0080\n",
      "     55        \u001b[36m0.0472\u001b[0m        0.0378  0.0100\n",
      "     56        \u001b[36m0.0472\u001b[0m        0.0378  0.0110\n",
      "     57        \u001b[36m0.0472\u001b[0m        0.0378  0.0110\n",
      "     58        \u001b[36m0.0471\u001b[0m        0.0378  0.0100\n",
      "     59        \u001b[36m0.0471\u001b[0m        0.0378  0.0100\n",
      "     60        \u001b[36m0.0471\u001b[0m        0.0378  0.0100\n",
      "     61        0.0471        0.0378  0.0090\n",
      "     62        0.0471        0.0378  0.0090\n",
      "     63        0.0471        0.0379  0.0080\n",
      "     64        0.0472        0.0379  0.0090\n",
      "     65        0.0472        0.0379  0.0090\n",
      "     66        0.0472        0.0379  0.0090\n",
      "     67        0.0472        0.0379  0.0090\n",
      "     68        0.0472        0.0379  0.0110\n",
      "     69        0.0472        0.0379  0.0080\n",
      "     70        0.0472        0.0379  0.0110\n",
      "     71        0.0472        0.0379  0.0090\n",
      "     72        0.0472        0.0379  0.0120\n",
      "     73        0.0472        0.0379  0.0090\n",
      "     74        0.0472        0.0379  0.0090\n",
      "     75        0.0472        0.0379  0.0090\n",
      "     76        0.0472        0.0379  0.0110\n",
      "     77        0.0472        0.0379  0.0090\n",
      "     78        0.0472        0.0379  0.0110\n",
      "     79        0.0472        0.0379  0.0090\n",
      "     80        0.0472        0.0379  0.0110\n",
      "     81        0.0472        0.0379  0.0110\n",
      "     82        0.0472        0.0379  0.0090\n",
      "     83        0.0472        0.0379  0.0110\n",
      "     84        0.0472        0.0379  0.0110\n",
      "     85        0.0472        0.0379  0.0130\n",
      "     86        0.0472        0.0379  0.0120\n",
      "     87        0.0472        0.0379  0.0120\n",
      "     88        0.0472        0.0379  0.0090\n",
      "     89        0.0472        0.0379  0.0130\n",
      "     90        0.0473        0.0379  0.0110\n",
      "     91        0.0473        0.0379  0.0100\n",
      "     92        0.0473        0.0379  0.0100\n",
      "     93        0.0473        0.0379  0.0090\n",
      "     94        0.0473        0.0379  0.0100\n",
      "     95        0.0473        0.0379  0.0110\n",
      "     96        0.0473        0.0379  0.0110\n",
      "     97        0.0473        0.0379  0.0110\n",
      "     98        0.0473        0.0379  0.0120\n",
      "     99        0.0473        0.0379  0.0110\n",
      "    100        0.0473        0.0379  0.0130\n",
      "    101        0.0473        0.0379  0.0110\n",
      "    102        0.0473        0.0379  0.0080\n",
      "    103        0.0473        0.0379  0.0120\n",
      "    104        0.0473        0.0379  0.0100\n",
      "    105        0.0473        0.0379  0.0110\n",
      "    106        0.0473        0.0380  0.0080\n",
      "    107        0.0473        0.0380  0.0100\n",
      "    108        0.0473        0.0380  0.0080\n",
      "    109        0.0473        0.0380  0.0080\n",
      "    110        0.0473        0.0380  0.0090\n",
      "    111        0.0473        0.0380  0.0090\n",
      "    112        0.0473        0.0380  0.0080\n",
      "    113        0.0473        0.0380  0.0110\n",
      "    114        0.0473        0.0380  0.0080\n",
      "    115        0.0473        0.0380  0.0120\n",
      "    116        0.0473        0.0380  0.0090\n",
      "    117        0.0474        0.0380  0.0090\n",
      "    118        0.0474        0.0380  0.0090\n",
      "    119        0.0474        0.0380  0.0109\n",
      "    120        0.0474        0.0380  0.0090\n",
      "    121        0.0474        0.0380  0.0079\n",
      "    122        0.0474        0.0380  0.0080\n",
      "    123        0.0474        0.0380  0.0080\n",
      "    124        0.0474        0.0380  0.0109\n",
      "    125        0.0474        0.0380  0.0120\n",
      "  epoch    train_loss    valid_loss     dur\n",
      "-------  ------------  ------------  ------\n",
      "      1        \u001b[36m0.3209\u001b[0m        \u001b[32m0.4148\u001b[0m  0.0070\n",
      "      2        \u001b[36m0.3092\u001b[0m        \u001b[32m0.3973\u001b[0m  0.0110\n",
      "      3        \u001b[36m0.2944\u001b[0m        \u001b[32m0.3775\u001b[0m  0.0079\n",
      "      4        \u001b[36m0.2778\u001b[0m        \u001b[32m0.3567\u001b[0m  0.0110\n",
      "      5        \u001b[36m0.2604\u001b[0m        \u001b[32m0.3355\u001b[0m  0.0100\n",
      "      6        \u001b[36m0.2429\u001b[0m        \u001b[32m0.3148\u001b[0m  0.0100\n",
      "      7        \u001b[36m0.2258\u001b[0m        \u001b[32m0.2947\u001b[0m  0.0100\n",
      "      8        \u001b[36m0.2094\u001b[0m        \u001b[32m0.2757\u001b[0m  0.0079\n",
      "      9        \u001b[36m0.1940\u001b[0m        \u001b[32m0.2577\u001b[0m  0.0100\n",
      "     10        \u001b[36m0.1797\u001b[0m        \u001b[32m0.2411\u001b[0m  0.0100\n",
      "     11        \u001b[36m0.1665\u001b[0m        \u001b[32m0.2256\u001b[0m  0.0090\n",
      "     12        \u001b[36m0.1545\u001b[0m        \u001b[32m0.2114\u001b[0m  0.0080\n",
      "     13        \u001b[36m0.1435\u001b[0m        \u001b[32m0.1984\u001b[0m  0.0100\n",
      "     14        \u001b[36m0.1336\u001b[0m        \u001b[32m0.1865\u001b[0m  0.0110\n",
      "     15        \u001b[36m0.1247\u001b[0m        \u001b[32m0.1757\u001b[0m  0.0100\n",
      "     16        \u001b[36m0.1167\u001b[0m        \u001b[32m0.1659\u001b[0m  0.0110\n",
      "     17        \u001b[36m0.1095\u001b[0m        \u001b[32m0.1569\u001b[0m  0.0110\n",
      "     18        \u001b[36m0.1031\u001b[0m        \u001b[32m0.1489\u001b[0m  0.0100\n",
      "     19        \u001b[36m0.0974\u001b[0m        \u001b[32m0.1415\u001b[0m  0.0080\n",
      "     20        \u001b[36m0.0924\u001b[0m        \u001b[32m0.1349\u001b[0m  0.0100\n",
      "     21        \u001b[36m0.0879\u001b[0m        \u001b[32m0.1289\u001b[0m  0.0080\n",
      "     22        \u001b[36m0.0839\u001b[0m        \u001b[32m0.1235\u001b[0m  0.0090\n",
      "     23        \u001b[36m0.0803\u001b[0m        \u001b[32m0.1186\u001b[0m  0.0090\n",
      "     24        \u001b[36m0.0772\u001b[0m        \u001b[32m0.1142\u001b[0m  0.0080\n",
      "     25        \u001b[36m0.0744\u001b[0m        \u001b[32m0.1101\u001b[0m  0.0110\n",
      "     26        \u001b[36m0.0720\u001b[0m        \u001b[32m0.1065\u001b[0m  0.0080\n",
      "     27        \u001b[36m0.0698\u001b[0m        \u001b[32m0.1032\u001b[0m  0.0080\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "     28        \u001b[36m0.0679\u001b[0m        \u001b[32m0.1003\u001b[0m  0.0130\n",
      "     29        \u001b[36m0.0662\u001b[0m        \u001b[32m0.0976\u001b[0m  0.0090\n",
      "     30        \u001b[36m0.0647\u001b[0m        \u001b[32m0.0951\u001b[0m  0.0100\n",
      "     31        \u001b[36m0.0634\u001b[0m        \u001b[32m0.0929\u001b[0m  0.0090\n",
      "     32        \u001b[36m0.0622\u001b[0m        \u001b[32m0.0909\u001b[0m  0.0080\n",
      "     33        \u001b[36m0.0612\u001b[0m        \u001b[32m0.0891\u001b[0m  0.0090\n",
      "     34        \u001b[36m0.0603\u001b[0m        \u001b[32m0.0874\u001b[0m  0.0090\n",
      "     35        \u001b[36m0.0595\u001b[0m        \u001b[32m0.0859\u001b[0m  0.0080\n",
      "     36        \u001b[36m0.0588\u001b[0m        \u001b[32m0.0845\u001b[0m  0.0092\n",
      "     37        \u001b[36m0.0581\u001b[0m        \u001b[32m0.0832\u001b[0m  0.0080\n",
      "     38        \u001b[36m0.0576\u001b[0m        \u001b[32m0.0821\u001b[0m  0.0080\n",
      "     39        \u001b[36m0.0571\u001b[0m        \u001b[32m0.0810\u001b[0m  0.0080\n",
      "     40        \u001b[36m0.0566\u001b[0m        \u001b[32m0.0801\u001b[0m  0.0080\n",
      "     41        \u001b[36m0.0562\u001b[0m        \u001b[32m0.0792\u001b[0m  0.0090\n",
      "     42        \u001b[36m0.0559\u001b[0m        \u001b[32m0.0784\u001b[0m  0.0090\n",
      "     43        \u001b[36m0.0556\u001b[0m        \u001b[32m0.0777\u001b[0m  0.0090\n",
      "     44        \u001b[36m0.0553\u001b[0m        \u001b[32m0.0770\u001b[0m  0.0100\n",
      "     45        \u001b[36m0.0550\u001b[0m        \u001b[32m0.0764\u001b[0m  0.0079\n",
      "     46        \u001b[36m0.0548\u001b[0m        \u001b[32m0.0758\u001b[0m  0.0100\n",
      "     47        \u001b[36m0.0546\u001b[0m        \u001b[32m0.0753\u001b[0m  0.0090\n",
      "     48        \u001b[36m0.0545\u001b[0m        \u001b[32m0.0748\u001b[0m  0.0100\n",
      "     49        \u001b[36m0.0543\u001b[0m        \u001b[32m0.0744\u001b[0m  0.0110\n",
      "     50        \u001b[36m0.0542\u001b[0m        \u001b[32m0.0740\u001b[0m  0.0090\n",
      "     51        \u001b[36m0.0540\u001b[0m        \u001b[32m0.0736\u001b[0m  0.0090\n",
      "     52        \u001b[36m0.0539\u001b[0m        \u001b[32m0.0733\u001b[0m  0.0090\n",
      "     53        \u001b[36m0.0538\u001b[0m        \u001b[32m0.0729\u001b[0m  0.0080\n",
      "     54        \u001b[36m0.0537\u001b[0m        \u001b[32m0.0727\u001b[0m  0.0080\n",
      "     55        \u001b[36m0.0536\u001b[0m        \u001b[32m0.0724\u001b[0m  0.0110\n",
      "     56        \u001b[36m0.0536\u001b[0m        \u001b[32m0.0721\u001b[0m  0.0100\n",
      "     57        \u001b[36m0.0535\u001b[0m        \u001b[32m0.0719\u001b[0m  0.0110\n",
      "     58        \u001b[36m0.0534\u001b[0m        \u001b[32m0.0717\u001b[0m  0.0089\n",
      "     59        \u001b[36m0.0534\u001b[0m        \u001b[32m0.0715\u001b[0m  0.0080\n",
      "     60        \u001b[36m0.0533\u001b[0m        \u001b[32m0.0713\u001b[0m  0.0090\n",
      "     61        \u001b[36m0.0533\u001b[0m        \u001b[32m0.0712\u001b[0m  0.0100\n",
      "     62        \u001b[36m0.0532\u001b[0m        \u001b[32m0.0710\u001b[0m  0.0080\n",
      "     63        \u001b[36m0.0532\u001b[0m        \u001b[32m0.0709\u001b[0m  0.0080\n",
      "     64        \u001b[36m0.0532\u001b[0m        \u001b[32m0.0707\u001b[0m  0.0089\n",
      "     65        \u001b[36m0.0531\u001b[0m        \u001b[32m0.0706\u001b[0m  0.0090\n",
      "     66        \u001b[36m0.0531\u001b[0m        \u001b[32m0.0705\u001b[0m  0.0090\n",
      "     67        \u001b[36m0.0531\u001b[0m        \u001b[32m0.0704\u001b[0m  0.0090\n",
      "     68        \u001b[36m0.0531\u001b[0m        \u001b[32m0.0703\u001b[0m  0.0100\n",
      "     69        \u001b[36m0.0530\u001b[0m        \u001b[32m0.0702\u001b[0m  0.0100\n",
      "     70        \u001b[36m0.0530\u001b[0m        \u001b[32m0.0701\u001b[0m  0.0080\n",
      "     71        \u001b[36m0.0530\u001b[0m        \u001b[32m0.0700\u001b[0m  0.0090\n",
      "     72        \u001b[36m0.0530\u001b[0m        \u001b[32m0.0699\u001b[0m  0.0110\n",
      "     73        \u001b[36m0.0530\u001b[0m        \u001b[32m0.0699\u001b[0m  0.0090\n",
      "     74        \u001b[36m0.0530\u001b[0m        \u001b[32m0.0698\u001b[0m  0.0100\n",
      "     75        \u001b[36m0.0529\u001b[0m        \u001b[32m0.0698\u001b[0m  0.0080\n",
      "     76        \u001b[36m0.0529\u001b[0m        \u001b[32m0.0697\u001b[0m  0.0100\n",
      "     77        \u001b[36m0.0529\u001b[0m        \u001b[32m0.0697\u001b[0m  0.0090\n",
      "     78        \u001b[36m0.0529\u001b[0m        \u001b[32m0.0696\u001b[0m  0.0090\n",
      "     79        \u001b[36m0.0529\u001b[0m        \u001b[32m0.0696\u001b[0m  0.0080\n",
      "     80        \u001b[36m0.0529\u001b[0m        \u001b[32m0.0695\u001b[0m  0.0100\n",
      "     81        \u001b[36m0.0529\u001b[0m        \u001b[32m0.0695\u001b[0m  0.0080\n",
      "     82        \u001b[36m0.0529\u001b[0m        \u001b[32m0.0694\u001b[0m  0.0080\n",
      "     83        \u001b[36m0.0529\u001b[0m        \u001b[32m0.0694\u001b[0m  0.0090\n",
      "     84        \u001b[36m0.0529\u001b[0m        \u001b[32m0.0694\u001b[0m  0.0100\n",
      "     85        \u001b[36m0.0529\u001b[0m        \u001b[32m0.0694\u001b[0m  0.0100\n",
      "     86        \u001b[36m0.0529\u001b[0m        \u001b[32m0.0693\u001b[0m  0.0089\n",
      "     87        \u001b[36m0.0529\u001b[0m        \u001b[32m0.0693\u001b[0m  0.0100\n",
      "     88        \u001b[36m0.0528\u001b[0m        \u001b[32m0.0693\u001b[0m  0.0089\n",
      "     89        \u001b[36m0.0528\u001b[0m        \u001b[32m0.0693\u001b[0m  0.0110\n",
      "     90        \u001b[36m0.0528\u001b[0m        \u001b[32m0.0692\u001b[0m  0.0109\n",
      "     91        \u001b[36m0.0528\u001b[0m        \u001b[32m0.0692\u001b[0m  0.0100\n",
      "     92        \u001b[36m0.0528\u001b[0m        \u001b[32m0.0692\u001b[0m  0.0090\n",
      "     93        \u001b[36m0.0528\u001b[0m        \u001b[32m0.0692\u001b[0m  0.0080\n",
      "     94        \u001b[36m0.0528\u001b[0m        \u001b[32m0.0692\u001b[0m  0.0080\n",
      "     95        \u001b[36m0.0528\u001b[0m        \u001b[32m0.0692\u001b[0m  0.0100\n",
      "     96        \u001b[36m0.0528\u001b[0m        \u001b[32m0.0692\u001b[0m  0.0090\n",
      "     97        \u001b[36m0.0528\u001b[0m        \u001b[32m0.0691\u001b[0m  0.0090\n",
      "     98        \u001b[36m0.0528\u001b[0m        \u001b[32m0.0691\u001b[0m  0.0110\n",
      "     99        \u001b[36m0.0528\u001b[0m        \u001b[32m0.0691\u001b[0m  0.0110\n",
      "    100        \u001b[36m0.0528\u001b[0m        \u001b[32m0.0691\u001b[0m  0.0090\n",
      "    101        \u001b[36m0.0528\u001b[0m        \u001b[32m0.0691\u001b[0m  0.0110\n",
      "    102        \u001b[36m0.0528\u001b[0m        \u001b[32m0.0691\u001b[0m  0.0110\n",
      "    103        \u001b[36m0.0528\u001b[0m        \u001b[32m0.0691\u001b[0m  0.0080\n",
      "    104        \u001b[36m0.0528\u001b[0m        \u001b[32m0.0691\u001b[0m  0.0100\n",
      "    105        \u001b[36m0.0528\u001b[0m        \u001b[32m0.0691\u001b[0m  0.0109\n",
      "    106        \u001b[36m0.0528\u001b[0m        \u001b[32m0.0691\u001b[0m  0.0089\n",
      "    107        \u001b[36m0.0528\u001b[0m        \u001b[32m0.0691\u001b[0m  0.0090\n",
      "    108        \u001b[36m0.0528\u001b[0m        \u001b[32m0.0691\u001b[0m  0.0100\n",
      "    109        \u001b[36m0.0528\u001b[0m        \u001b[32m0.0691\u001b[0m  0.0090\n",
      "    110        \u001b[36m0.0528\u001b[0m        \u001b[32m0.0690\u001b[0m  0.0100\n",
      "    111        \u001b[36m0.0528\u001b[0m        \u001b[32m0.0690\u001b[0m  0.0110\n",
      "    112        \u001b[36m0.0528\u001b[0m        \u001b[32m0.0690\u001b[0m  0.0090\n",
      "    113        \u001b[36m0.0528\u001b[0m        \u001b[32m0.0690\u001b[0m  0.0080\n",
      "    114        \u001b[36m0.0528\u001b[0m        \u001b[32m0.0690\u001b[0m  0.0090\n",
      "    115        \u001b[36m0.0528\u001b[0m        \u001b[32m0.0690\u001b[0m  0.0110\n",
      "    116        \u001b[36m0.0528\u001b[0m        \u001b[32m0.0690\u001b[0m  0.0110\n",
      "    117        \u001b[36m0.0528\u001b[0m        \u001b[32m0.0690\u001b[0m  0.0110\n",
      "    118        \u001b[36m0.0528\u001b[0m        \u001b[32m0.0690\u001b[0m  0.0100\n",
      "    119        \u001b[36m0.0528\u001b[0m        \u001b[32m0.0690\u001b[0m  0.0099\n",
      "    120        \u001b[36m0.0528\u001b[0m        \u001b[32m0.0690\u001b[0m  0.0080\n",
      "    121        \u001b[36m0.0528\u001b[0m        \u001b[32m0.0690\u001b[0m  0.0090\n",
      "    122        \u001b[36m0.0528\u001b[0m        \u001b[32m0.0690\u001b[0m  0.0100\n",
      "    123        \u001b[36m0.0528\u001b[0m        \u001b[32m0.0690\u001b[0m  0.0080\n",
      "    124        \u001b[36m0.0528\u001b[0m        \u001b[32m0.0690\u001b[0m  0.0079\n",
      "    125        \u001b[36m0.0528\u001b[0m        \u001b[32m0.0690\u001b[0m  0.0110\n",
      "  epoch    train_loss    valid_loss     dur\n",
      "-------  ------------  ------------  ------\n",
      "      1        \u001b[36m0.5371\u001b[0m        \u001b[32m0.5231\u001b[0m  0.0160\n",
      "      2        \u001b[36m0.4750\u001b[0m        \u001b[32m0.4483\u001b[0m  0.0189\n",
      "      3        \u001b[36m0.4009\u001b[0m        \u001b[32m0.3745\u001b[0m  0.0170\n",
      "      4        \u001b[36m0.3313\u001b[0m        \u001b[32m0.3124\u001b[0m  0.0169\n",
      "      5        \u001b[36m0.2729\u001b[0m        \u001b[32m0.2627\u001b[0m  0.0150\n",
      "      6        \u001b[36m0.2263\u001b[0m        \u001b[32m0.2225\u001b[0m  0.0150\n",
      "      7        \u001b[36m0.1884\u001b[0m        \u001b[32m0.1902\u001b[0m  0.0180\n",
      "      8        \u001b[36m0.1582\u001b[0m        \u001b[32m0.1643\u001b[0m  0.0189\n",
      "      9        \u001b[36m0.1349\u001b[0m        \u001b[32m0.1442\u001b[0m  0.0149\n",
      "     10        \u001b[36m0.1158\u001b[0m        \u001b[32m0.1283\u001b[0m  0.0189\n",
      "     11        \u001b[36m0.1012\u001b[0m        \u001b[32m0.1156\u001b[0m  0.0170\n",
      "     12        \u001b[36m0.0898\u001b[0m        \u001b[32m0.1056\u001b[0m  0.0160\n",
      "     13        \u001b[36m0.0805\u001b[0m        \u001b[32m0.0976\u001b[0m  0.0180\n",
      "     14        \u001b[36m0.0733\u001b[0m        \u001b[32m0.0911\u001b[0m  0.0170\n",
      "     15        \u001b[36m0.0678\u001b[0m        \u001b[32m0.0861\u001b[0m  0.0160\n",
      "     16        \u001b[36m0.0634\u001b[0m        \u001b[32m0.0822\u001b[0m  0.0170\n",
      "     17        \u001b[36m0.0599\u001b[0m        \u001b[32m0.0791\u001b[0m  0.0149\n",
      "     18        \u001b[36m0.0573\u001b[0m        \u001b[32m0.0767\u001b[0m  0.0139\n",
      "     19        \u001b[36m0.0552\u001b[0m        \u001b[32m0.0747\u001b[0m  0.0170\n",
      "     20        \u001b[36m0.0537\u001b[0m        \u001b[32m0.0733\u001b[0m  0.0150\n",
      "     21        \u001b[36m0.0524\u001b[0m        \u001b[32m0.0720\u001b[0m  0.0189\n",
      "     22        \u001b[36m0.0514\u001b[0m        \u001b[32m0.0710\u001b[0m  0.0169\n",
      "     23        \u001b[36m0.0506\u001b[0m        \u001b[32m0.0702\u001b[0m  0.0170\n",
      "     24        \u001b[36m0.0500\u001b[0m        \u001b[32m0.0696\u001b[0m  0.0180\n",
      "     25        \u001b[36m0.0495\u001b[0m        \u001b[32m0.0691\u001b[0m  0.0170\n",
      "     26        \u001b[36m0.0491\u001b[0m        \u001b[32m0.0686\u001b[0m  0.0159\n",
      "     27        \u001b[36m0.0487\u001b[0m        \u001b[32m0.0683\u001b[0m  0.0159\n",
      "     28        \u001b[36m0.0485\u001b[0m        \u001b[32m0.0679\u001b[0m  0.0170\n",
      "     29        \u001b[36m0.0481\u001b[0m        \u001b[32m0.0675\u001b[0m  0.0140\n",
      "     30        \u001b[36m0.0478\u001b[0m        \u001b[32m0.0672\u001b[0m  0.0150\n",
      "     31        \u001b[36m0.0476\u001b[0m        \u001b[32m0.0670\u001b[0m  0.0189\n",
      "     32        \u001b[36m0.0475\u001b[0m        \u001b[32m0.0668\u001b[0m  0.0179\n",
      "     33        \u001b[36m0.0474\u001b[0m        \u001b[32m0.0667\u001b[0m  0.0179\n",
      "     34        \u001b[36m0.0473\u001b[0m        \u001b[32m0.0666\u001b[0m  0.0160\n",
      "     35        \u001b[36m0.0472\u001b[0m        \u001b[32m0.0665\u001b[0m  0.0149\n",
      "     36        \u001b[36m0.0472\u001b[0m        \u001b[32m0.0664\u001b[0m  0.0160\n",
      "     37        \u001b[36m0.0471\u001b[0m        \u001b[32m0.0664\u001b[0m  0.0150\n",
      "     38        \u001b[36m0.0471\u001b[0m        \u001b[32m0.0664\u001b[0m  0.0170\n",
      "     39        \u001b[36m0.0471\u001b[0m        0.0664  0.0170\n",
      "     40        0.0471        0.0664  0.0160\n",
      "     41        0.0471        0.0664  0.0140\n",
      "     42        0.0471        0.0665  0.0145\n",
      "     43        0.0472        0.0665  0.0170\n",
      "     44        0.0472        0.0665  0.0150\n",
      "     45        0.0472        0.0664  0.0150\n",
      "     46        0.0471        \u001b[32m0.0664\u001b[0m  0.0159\n",
      "     47        \u001b[36m0.0471\u001b[0m        \u001b[32m0.0663\u001b[0m  0.0180\n",
      "     48        \u001b[36m0.0471\u001b[0m        0.0663  0.0150\n",
      "     49        \u001b[36m0.0471\u001b[0m        0.0663  0.0149\n",
      "     50        0.0471        \u001b[32m0.0663\u001b[0m  0.0150\n",
      "     51        \u001b[36m0.0470\u001b[0m        \u001b[32m0.0663\u001b[0m  0.0140\n",
      "     52        \u001b[36m0.0470\u001b[0m        \u001b[32m0.0662\u001b[0m  0.0149\n",
      "     53        \u001b[36m0.0470\u001b[0m        \u001b[32m0.0662\u001b[0m  0.0160\n",
      "     54        \u001b[36m0.0470\u001b[0m        \u001b[32m0.0661\u001b[0m  0.0150\n",
      "     55        \u001b[36m0.0469\u001b[0m        \u001b[32m0.0661\u001b[0m  0.0150\n",
      "     56        0.0470        \u001b[32m0.0661\u001b[0m  0.0140\n",
      "     57        \u001b[36m0.0469\u001b[0m        \u001b[32m0.0661\u001b[0m  0.0160\n",
      "     58        \u001b[36m0.0469\u001b[0m        0.0661  0.0150\n",
      "     59        0.0469        0.0661  0.0150\n",
      "     60        0.0470        0.0662  0.0169\n",
      "     61        0.0470        0.0663  0.0150\n",
      "     62        0.0470        0.0663  0.0150\n",
      "     63        0.0470        0.0662  0.0160\n",
      "     64        0.0470        0.0662  0.0160\n",
      "     65        0.0469        \u001b[32m0.0661\u001b[0m  0.0150\n",
      "     66        \u001b[36m0.0469\u001b[0m        \u001b[32m0.0660\u001b[0m  0.0149\n",
      "     67        \u001b[36m0.0469\u001b[0m        0.0660  0.0149\n",
      "     68        0.0469        0.0661  0.0170\n",
      "     69        0.0469        0.0661  0.0150\n",
      "     70        0.0469        0.0660  0.0150\n",
      "     71        0.0469        0.0661  0.0150\n",
      "     72        0.0469        0.0662  0.0150\n",
      "     73        0.0470        0.0662  0.0170\n",
      "     74        0.0470        0.0662  0.0150\n",
      "     75        0.0470        0.0662  0.0140\n",
      "     76        0.0470        0.0662  0.0150\n",
      "     77        0.0469        0.0662  0.0150\n",
      "     78        0.0470        0.0662  0.0150\n",
      "     79        0.0469        0.0661  0.0150\n",
      "     80        0.0469        0.0661  0.0150\n",
      "     81        \u001b[36m0.0469\u001b[0m        \u001b[32m0.0660\u001b[0m  0.0150\n",
      "     82        \u001b[36m0.0469\u001b[0m        \u001b[32m0.0660\u001b[0m  0.0150\n",
      "     83        \u001b[36m0.0468\u001b[0m        \u001b[32m0.0659\u001b[0m  0.0170\n",
      "     84        \u001b[36m0.0468\u001b[0m        0.0659  0.0200\n",
      "     85        0.0468        \u001b[32m0.0659\u001b[0m  0.0150\n",
      "     86        \u001b[36m0.0468\u001b[0m        \u001b[32m0.0659\u001b[0m  0.0150\n",
      "     87        \u001b[36m0.0468\u001b[0m        0.0659  0.0170\n",
      "     88        0.0468        0.0660  0.0152\n",
      "     89        0.0469        0.0661  0.0229\n",
      "     90        0.0469        0.0661  0.0170\n",
      "     91        0.0469        0.0661  0.0160\n",
      "     92        0.0469        0.0661  0.0168\n",
      "     93        0.0469        0.0661  0.0149\n",
      "     94        0.0469        0.0661  0.0150\n",
      "     95        0.0469        0.0661  0.0170\n",
      "     96        0.0469        0.0661  0.0149\n",
      "     97        0.0469        0.0660  0.0159\n",
      "     98        0.0469        0.0660  0.0170\n",
      "     99        0.0468        0.0660  0.0150\n",
      "    100        0.0469        0.0660  0.0140\n",
      "    101        0.0469        0.0661  0.0160\n",
      "    102        0.0469        0.0661  0.0189\n",
      "    103        0.0469        0.0661  0.0170\n",
      "    104        0.0469        0.0661  0.0150\n",
      "    105        0.0469        0.0661  0.0149\n",
      "    106        0.0469        0.0661  0.0150\n",
      "    107        0.0469        0.0661  0.0170\n",
      "    108        0.0469        0.0660  0.0170\n",
      "    109        0.0469        0.0660  0.0150\n",
      "    110        0.0469        0.0660  0.0169\n",
      "    111        0.0468        0.0660  0.0150\n",
      "    112        0.0469        0.0660  0.0170\n",
      "    113        0.0469        0.0660  0.0170\n",
      "    114        0.0469        0.0661  0.0170\n",
      "    115        0.0469        0.0661  0.0170\n",
      "    116        0.0469        0.0661  0.0150\n",
      "    117        0.0469        0.0661  0.0170\n",
      "    118        0.0469        0.0662  0.0170\n",
      "    119        0.0469        0.0662  0.0170\n",
      "    120        0.0470        0.0662  0.0150\n",
      "    121        0.0469        0.0661  0.0169\n",
      "    122        0.0469        0.0661  0.0140\n",
      "    123        0.0469        0.0661  0.0150\n",
      "    124        0.0469        0.0662  0.0170\n",
      "    125        0.0470        0.0662  0.0140\n"
     ]
    }
   ],
   "source": [
    "assert len(X_training) == len(Y_training)\n",
    "randomized_search_result = sgd_randomized_search.fit(X_training.values, Y_training.values)\n",
    "\n",
    "#dataframe.values gives the underlying numpy array of the data frame, doing this conversion since pandas data frame \n",
    "#is not supported."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Save and view the best parameters and hyperparameters obtained from randomized search."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>mean_fit_time</th>\n",
       "      <th>std_fit_time</th>\n",
       "      <th>mean_score_time</th>\n",
       "      <th>std_score_time</th>\n",
       "      <th>param_optimizer__weight_decay</th>\n",
       "      <th>param_optimizer__nesterov</th>\n",
       "      <th>param_optimizer__momentum</th>\n",
       "      <th>param_optimizer__dampening</th>\n",
       "      <th>param_optimizer</th>\n",
       "      <th>param_module__num_units</th>\n",
       "      <th>...</th>\n",
       "      <th>params</th>\n",
       "      <th>split0_test_score</th>\n",
       "      <th>split1_test_score</th>\n",
       "      <th>mean_test_score</th>\n",
       "      <th>std_test_score</th>\n",
       "      <th>rank_test_score</th>\n",
       "      <th>split0_train_score</th>\n",
       "      <th>split1_train_score</th>\n",
       "      <th>mean_train_score</th>\n",
       "      <th>std_train_score</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1.582486</td>\n",
       "      <td>0.128185</td>\n",
       "      <td>0.002476</td>\n",
       "      <td>0.000516</td>\n",
       "      <td>0.1</td>\n",
       "      <td>True</td>\n",
       "      <td>0.75</td>\n",
       "      <td>0</td>\n",
       "      <td>&lt;class 'torch.optim.sgd.SGD'&gt;</td>\n",
       "      <td>14</td>\n",
       "      <td>...</td>\n",
       "      <td>{'optimizer__weight_decay': 0.1, 'optimizer__n...</td>\n",
       "      <td>-0.054545</td>\n",
       "      <td>-0.049608</td>\n",
       "      <td>-0.052098</td>\n",
       "      <td>0.002469</td>\n",
       "      <td>1</td>\n",
       "      <td>-0.045399</td>\n",
       "      <td>-0.056154</td>\n",
       "      <td>-0.050776</td>\n",
       "      <td>0.005377</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>1 rows  24 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "   mean_fit_time  std_fit_time  mean_score_time  std_score_time  \\\n",
       "0       1.582486      0.128185         0.002476        0.000516   \n",
       "\n",
       "  param_optimizer__weight_decay param_optimizer__nesterov  \\\n",
       "0                           0.1                      True   \n",
       "\n",
       "  param_optimizer__momentum param_optimizer__dampening  \\\n",
       "0                      0.75                          0   \n",
       "\n",
       "                 param_optimizer param_module__num_units  ...  \\\n",
       "0  <class 'torch.optim.sgd.SGD'>                      14  ...   \n",
       "\n",
       "                                              params split0_test_score  \\\n",
       "0  {'optimizer__weight_decay': 0.1, 'optimizer__n...         -0.054545   \n",
       "\n",
       "  split1_test_score mean_test_score std_test_score  rank_test_score  \\\n",
       "0         -0.049608       -0.052098       0.002469                1   \n",
       "\n",
       "   split0_train_score  split1_train_score  mean_train_score  std_train_score  \n",
       "0           -0.045399           -0.056154         -0.050776         0.005377  \n",
       "\n",
       "[1 rows x 24 columns]"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Import the parameter combinations and the corresponding results into a pandas dataframe\n",
    "pd.DataFrame(randomized_search_result.cv_results_)\n",
    "\n",
    "#Each row of this dataframe gives one combination of the hyperparameters used and the corresponding performance of the\n",
    "#estimator used."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Interpretation of cv_results_\n",
    "Column \"mean_test_score\" is the average of columns split_0_test_score, split_1_test_score, split_2_test_score...split_k_test_score for \"this\" hyperparameter configuration.<br><br>\n",
    "\"randomized_search_result.best_score_\" will return the max value of the column mean_test_score.<br><br>\n",
    "Column \"rank_test_score\" ranks all hyperparameter combinations by the values of mean_test_score.<br><br>\n",
    "Column \"std_test_score\" is the standard deviation of split_0_test_score, split_1_test_score,...., split_k_test_score. It shows how consistently \"this\" set of hyperparameters is performing on the hold-out data of each k-fold validation."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Save the results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\users\\being_aerys\\appdata\\local\\programs\\python\\python36\\lib\\site-packages\\torch\\serialization.py:360: UserWarning: Couldn't retrieve source code for container of type Regression_Module. It won't be checked for correctness upon loading.\n",
      "  \"type \" + obj.__name__ + \". It won't be checked \"\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'optimizer__weight_decay': 0.1,\n",
       " 'optimizer__nesterov': True,\n",
       " 'optimizer__momentum': 0.75,\n",
       " 'optimizer__dampening': 0.0,\n",
       " 'optimizer': torch.optim.sgd.SGD,\n",
       " 'module__num_units': 14,\n",
       " 'module__dropout': 0.0,\n",
       " 'max_epochs': 125,\n",
       " 'lr': 0.005,\n",
       " 'iterator_train__batch_size': 64}"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#saving the best estimator i.e., the best neural network parameters\n",
    "# randomized_search_result.best_estimator_ gives the neural net with best parameters\n",
    "joblib.dump(randomized_search_result.best_estimator_, 'best_params_of_NN_by_randomized_search.pkl', compress = 1)\n",
    "\n",
    "#saving the hyperparameters that were used to get this best estimator\n",
    "# randomized_search_result.best_params_ gives the best hyperparameters that were used to get this best estimator\n",
    "joblib.dump(randomized_search_result.best_params_, 'best_hyperparams_for_NN_by_randomized_search.pkl', compress = 1)\n",
    "\n",
    "#saving all 350 models' hyperparameter configurations and the neural network parameters\n",
    "\n",
    "\n",
    "\n",
    "# Displaying the best hyperparameters configuration\n",
    "randomized_search_result.best_params_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def report(results, num_of_top_results_to_report = 5):\n",
    "    #this utility method was found online.\n",
    "    \n",
    "    for idx in range(0, num_of_top_results_to_report):\n",
    "        candidates = np.flatnonzero(results['rank_test_score'] == idx)\n",
    "        for candidate in candidates:\n",
    "            print(\"Model with rank: {0}\".format(idx))\n",
    "            print(\"Mean validation score: {0:.3f} (std: {1:.3f})\".format(\n",
    "                  results['mean_test_score'][candidate],\n",
    "                  results['std_test_score'][candidate]))\n",
    "            print(\"Parameters: {0}\".format(results['params'][candidate]))\n",
    "            print(\"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model with rank: 1\n",
      "Mean validation score: -0.052 (std: 0.002)\n",
      "Parameters: {'optimizer__weight_decay': 0.1, 'optimizer__nesterov': True, 'optimizer__momentum': 0.75, 'optimizer__dampening': 0.0, 'optimizer': <class 'torch.optim.sgd.SGD'>, 'module__num_units': 14, 'module__dropout': 0.0, 'max_epochs': 125, 'lr': 0.005, 'iterator_train__batch_size': 64}\n",
      "\n"
     ]
    }
   ],
   "source": [
    "report(sgd_randomized_search.cv_results_,num_of_top_results_to_report = 3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Visualize training and validation loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get training and validation loss\n",
    "epochs = [i for i in range(len(sgd_randomized_search.best_estimator_.history))]\n",
    "training_loss = sgd_randomized_search.best_estimator_.history[:,'train_loss']\n",
    "validation_loss = sgd_randomized_search.best_estimator_.history[:,'valid_loss']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(epochs,training_loss,'g-');\n",
    "plt.plot(epochs,validation_loss,'r-');\n",
    "plt.title('Training and Validation Loss Curves');\n",
    "plt.xlabel('Epochs');\n",
    "plt.ylabel('Mean Squared Error');\n",
    "plt.legend(['Train','Validation']);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Test on Test Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Predict on test data\n",
    "testing_data = pd.read_csv(\"normalized_testing_features_and_targets.csv\", sep = \",\")\n",
    "testing_data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Separate features and targets\n",
    "X_testing = testing_data.iloc[:,0:num_features].values #notice retaining only the underlying numpy arrays\n",
    "Y_testing = pd.DataFrame(testing_data[\"Hardness\"]).values\n",
    "\n",
    "Y_predictions_on_test_data = sgd_randomized_search.best_estimator_.predict(X_testing.astype(np.float32))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Y_predictions_on_test_data = sgd_randomized_search.best_estimator_.predict(X_testing.astype(np.float32))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Y_testing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Y_predictions_on_test_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get Root Mean Squared Error\n",
    "MSE(Y_testing,Y_predictions_on_test_data)**(1/2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Kernel Density Estimation Plot\n",
    "\n",
    "# sns.kdeplot(Y_predictions_on_test_data.squeeze(), label='predictions of the model', shade=True)\n",
    "# sns.kdeplot(Y_testing.squeeze(), label='true values', shade=True)\n",
    "# plt.xlabel('Hardness');"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Dist Plot\n",
    "\n",
    "# sns.distplot(Y_testing.squeeze()-Y_predictions_on_test_data.squeeze(),label='error', bins = 10);\n",
    "# plt.xlabel('Error');"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# R^2 plot\n",
    "\n",
    "print(r2_score(Y_testing,Y_predictions_on_test_data))\n",
    "\n",
    "plt.plot(Y_predictions_on_test_data,Y_testing,'g*')\n",
    "plt.xlabel('predicted')\n",
    "plt.ylabel('actual')\n",
    "plt.title('$R^{2}$ Plot');"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### View the actual values vs predicted values in real scale"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#The maximum and minimum targets of the training data were [983.91] and [116.] from feature engineering file\n",
    "\n",
    "max_hardness = 983.91\n",
    "min_hardness = 116.\n",
    "\n",
    "def convert_to_original_scale(scaled_hardness):\n",
    "    original_scale_hardness = (scaled_hardness * (max_hardness - min_hardness)) + min_hardness\n",
    "    return original_scale_hardness\n",
    "\n",
    "for test_sample in range(len(Y_predictions_on_test_data)):\n",
    "    print(\"True Hardness: {}\".format(convert_to_original_scale(Y_testing[test_sample])))\n",
    "    print(\"Predicted Hardness: {}\".format(convert_to_original_scale(Y_predictions_on_test_data[test_sample])))\n",
    "    print(\"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
