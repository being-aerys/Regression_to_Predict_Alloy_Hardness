{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Given the scarcity of data in this task, we will use k-Fold cross-validation for training with k set to a large value to make a good use of the available data of a significantly small size."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Following are the hyperparameters that can be tuned for a neural network\n",
    "\n",
    "1. learning rate\n",
    "2. no of hidden units\n",
    "3. no of epochs to train\n",
    "4. dropout probability\n",
    "5. loss function (NOT TUNED HERE)\n",
    "6. mini batch size\n",
    "7. weights initialization (NOT TUNED HERE)\n",
    "8. l1/ l2 regularizers\n",
    "9. activation function to use at the nodes (NOT TUNED HERE)\n",
    "10. no of layers (NOT TUNED HERE)\n",
    "11. learning rate decay (NOT TUNED HERE)\n",
    "12. optimizer (NOT TUNED HERE)\n",
    "13. momentum (only if sgd or rmsprop optimizer used, not with adam, adagrad)\n",
    "14. momentum_dampening (only if sgd or rmsprop optimizer used, not with adam, adagrad) (NOT TUNED HERE)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There are several methods to tune hyperparameters of a neural network. Two of them are grid search and randomized search that select points in the parameter space and evaluate these points (each point is essestially a unique configuration of hyperparameters in the hyperparameter space) and return the best hyperparameter combination based on the performace on the validation data.\n",
    "\n",
    "#### How is grid search/ randomized search done with k-fold cross validation?\n",
    "1. Select n points in the hyperparameter space. For each point, do:\n",
    "    a. Each point corresponds to a hyperparameter configuration in the hyperparameter space.\n",
    "    b. Train and evaluate this model k times as follows.\n",
    "        i. Randomly divide the training data into k partitions. Call them partition 1, partition 2,.........., partition k.\n",
    "        ii. Repeat for each partition j starting from j = 1 to j = k\n",
    "            . Train the model with this hyperparameter configuration on the partitions except partition j and test on                     partition j.\n",
    "            . Store the performance of the model.\n",
    "        iii. Calculate the average performance of the model with this hyperparameter configuration over the k folds.\n",
    "2. Declare the model that was trained using the hyperparameters corresponding to the point that achieved the best              validation result as the best model and declare this choice of hyperparameters as the best hyperparameters.\n",
    "3. Predict the labels of the testing data using this model."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Import Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "import copy\n",
    "import sys\n",
    "import os\n",
    "import random\n",
    "import csv\n",
    "from math import sqrt\n",
    "\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "import pandas as pd\n",
    "\n",
    "import seaborn as sns\n",
    "\n",
    "\n",
    "import sklearn\n",
    "from sklearn.externals import joblib\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from skorch import NeuralNetRegressor\n",
    "from sklearn.model_selection import RandomizedSearchCV\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.model_selection import cross_validate\n",
    "\n",
    "\n",
    "from sklearn.metrics import mean_squared_error as MSE\n",
    "from sklearn.metrics import r2_score\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from joblib import dump, load"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The python version used is 3.6.8 (tags/v3.6.8:3c6b436a57, Dec 24 2018, 00:16:47) [MSC v.1916 64 bit (AMD64)]\n",
      "The torch version used is 1.4.0\n",
      "The sklearn version used is 0.20.3\n"
     ]
    }
   ],
   "source": [
    "print(\"The python version used is {}\".format(sys.version))\n",
    "print(\"The torch version used is {}\".format(torch.__version__))\n",
    "print(\"The sklearn version used is {}\".format(sklearn.__version__))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Import Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>% of Cr</th>\n",
       "      <th>% of Hf</th>\n",
       "      <th>% of Mo</th>\n",
       "      <th>% of Nb</th>\n",
       "      <th>% of Ta</th>\n",
       "      <th>% of Ti</th>\n",
       "      <th>% of V</th>\n",
       "      <th>% of Zr</th>\n",
       "      <th>% of Ni</th>\n",
       "      <th>% of Al</th>\n",
       "      <th>% of Mn</th>\n",
       "      <th>%Cu</th>\n",
       "      <th>%C</th>\n",
       "      <th>Entropy</th>\n",
       "      <th>Hardness</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.470</td>\n",
       "      <td>0.44</td>\n",
       "      <td>0.97491</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.992366</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.138686</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.602527</td>\n",
       "      <td>0.429768</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.7500</td>\n",
       "      <td>0.200</td>\n",
       "      <td>0.25</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.4000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.589731</td>\n",
       "      <td>0.539457</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.5815</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.4026</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.396171</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.733380</td>\n",
       "      <td>0.559966</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.5000</td>\n",
       "      <td>0.800</td>\n",
       "      <td>0.25</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.4000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.541151</td>\n",
       "      <td>0.351534</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.468</td>\n",
       "      <td>0.33</td>\n",
       "      <td>0.97491</td>\n",
       "      <td>0.0860</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.138686</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.741308</td>\n",
       "      <td>0.441290</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   % of Cr  % of Hf  % of Mo  % of Nb  % of Ta  % of Ti  % of V   % of Zr  \\\n",
       "0      0.0      0.0   0.0000    0.470     0.44  0.97491  0.0000  0.992366   \n",
       "1      0.0      0.0   0.7500    0.200     0.25  0.00000  0.4000  0.000000   \n",
       "2      0.0      0.0   0.5815    0.000     0.00  0.00000  0.4026  0.000000   \n",
       "3      0.0      0.0   0.5000    0.800     0.25  0.00000  0.4000  0.000000   \n",
       "4      0.0      0.0   0.0000    0.468     0.33  0.97491  0.0860  1.000000   \n",
       "\n",
       "    % of Ni   % of Al  % of Mn  %Cu   %C   Entropy  Hardness  \n",
       "0  0.000000  0.138686      0.0  0.0  0.0  0.602527  0.429768  \n",
       "1  0.000000  0.000000      0.0  0.0  0.0  0.589731  0.539457  \n",
       "2  0.396171  0.000000      0.0  0.0  0.0  0.733380  0.559966  \n",
       "3  0.000000  0.000000      0.0  0.0  0.0  0.541151  0.351534  \n",
       "4  0.000000  0.138686      0.0  0.0  0.0  0.741308  0.441290  "
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "training_data = pd.read_csv(\"normalized_training_features_and_targets.csv\", sep = \",\")\n",
    "training_data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The number of features is 14\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>% of Cr</th>\n",
       "      <th>% of Hf</th>\n",
       "      <th>% of Mo</th>\n",
       "      <th>% of Nb</th>\n",
       "      <th>% of Ta</th>\n",
       "      <th>% of Ti</th>\n",
       "      <th>% of V</th>\n",
       "      <th>% of Zr</th>\n",
       "      <th>% of Ni</th>\n",
       "      <th>% of Al</th>\n",
       "      <th>% of Mn</th>\n",
       "      <th>%Cu</th>\n",
       "      <th>%C</th>\n",
       "      <th>Entropy</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.470</td>\n",
       "      <td>0.44</td>\n",
       "      <td>0.97491</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.992366</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.138686</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.602527</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.7500</td>\n",
       "      <td>0.200</td>\n",
       "      <td>0.25</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.4000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.589731</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.5815</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.4026</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.396171</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.733380</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.5000</td>\n",
       "      <td>0.800</td>\n",
       "      <td>0.25</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.4000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.541151</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.468</td>\n",
       "      <td>0.33</td>\n",
       "      <td>0.97491</td>\n",
       "      <td>0.0860</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.138686</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.741308</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   % of Cr  % of Hf  % of Mo  % of Nb  % of Ta  % of Ti  % of V   % of Zr  \\\n",
       "0      0.0      0.0   0.0000    0.470     0.44  0.97491  0.0000  0.992366   \n",
       "1      0.0      0.0   0.7500    0.200     0.25  0.00000  0.4000  0.000000   \n",
       "2      0.0      0.0   0.5815    0.000     0.00  0.00000  0.4026  0.000000   \n",
       "3      0.0      0.0   0.5000    0.800     0.25  0.00000  0.4000  0.000000   \n",
       "4      0.0      0.0   0.0000    0.468     0.33  0.97491  0.0860  1.000000   \n",
       "\n",
       "    % of Ni   % of Al  % of Mn  %Cu   %C   Entropy  \n",
       "0  0.000000  0.138686      0.0  0.0  0.0  0.602527  \n",
       "1  0.000000  0.000000      0.0  0.0  0.0  0.589731  \n",
       "2  0.396171  0.000000      0.0  0.0  0.0  0.733380  \n",
       "3  0.000000  0.000000      0.0  0.0  0.0  0.541151  \n",
       "4  0.000000  0.138686      0.0  0.0  0.0  0.741308  "
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "num_features = len( training_data.iloc[0,:] ) - 1\n",
    "print(\"The number of features is {}\".format(num_features))\n",
    "X_training = training_data.iloc[:,0:num_features]\n",
    "\n",
    "X_training.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Hardness</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.429768</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.539457</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.559966</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.351534</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.441290</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   Hardness\n",
       "0  0.429768\n",
       "1  0.539457\n",
       "2  0.559966\n",
       "3  0.351534\n",
       "4  0.441290"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Y_training = pd.DataFrame(training_data[\"Hardness\"])\n",
    "Y_training.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Initialize Seeds for Reproducibility"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.random.seed(0)\n",
    "torch.manual_seed(0)\n",
    "torch.cuda.manual_seed(0)\n",
    "\n",
    "device_to_use = \"cuda\" if torch.cuda.device_count() > 0 else \"cpu\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## torch Regression Model definition"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Regression_Module(nn.Module):\n",
    "    def __init__(self, num_units = 10, dropout = 0.5, activation = F.leaky_relu, input_dim = num_features, output_dim = 1):\n",
    "           \n",
    "        \n",
    "        super(Regression_Module, self).__init__()\n",
    "        self.activation = activation\n",
    "        \n",
    "        \n",
    "        self.L1 = nn.Linear(input_dim, num_units)\n",
    "        self.L2 = nn.Linear(num_units, num_units)\n",
    "        \n",
    "        #self.batchnorm_1 = nn.BatchNorm1d(num_units, 1e-12, affine=True, track_running_stats=True)\n",
    "        self.dropout_1 = nn.Dropout(p=dropout)\n",
    "        \n",
    "        self.L3 = nn.Linear(num_units, num_units)\n",
    "        self.L4 = nn.Linear(num_units, num_units)\n",
    "        \n",
    "        #self.batchnorm_2 = nn.BatchNorm1d(num_units, 1e-12, affine=True, track_running_stats=True)\n",
    "        self.dropout_2 = nn.Dropout(p=dropout)\n",
    "        \n",
    "        self.L5 = nn.Linear(num_units, num_units)\n",
    "        self.L6 = nn.Linear(num_units, output_dim)\n",
    "\n",
    "\n",
    "\n",
    "    def forward(self, input):\n",
    "            \n",
    "        input = self.activation(self.L1(input.float()))\n",
    "        input = self.L2(input.float())\n",
    "\n",
    "        # input = input.unsqueeze(0) #https://discuss.pytorch.org/t/batchnorm1d-valueerror-expected-2d-or-3d-input-got-1d-input/42081\n",
    "        # input = self.batchnorm_1(input)\n",
    "\n",
    "        input = self.activation(input.float())\n",
    "        input = self.dropout_1(input.float())\n",
    "\n",
    "        input = self.activation(self.L3(input.float()))\n",
    "        input = self.L4(input.float())\n",
    "\n",
    "        # input = input.unsqueeze(0)\n",
    "        # input = self.batchnorm_2(input)\n",
    "\n",
    "        input = self.activation(input.float())\n",
    "        input = self.dropout_2(input.float())\n",
    "\n",
    "        input = self.activation(self.L5(input.float()))\n",
    "\n",
    "        input = self.L6(input.float())\n",
    "\n",
    "\n",
    "        #VVI: need to return in double format instead of a float\n",
    "        return input.double()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## skorch Regression Model definition\n",
    "Takes torch regressor as an argument."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "skorch_regressor = NeuralNetRegressor(module = Regression_Module,  #pass a torch module class\n",
    "                                      device = device_to_use,\n",
    "                                      iterator_train__shuffle = True,\n",
    "                                      \n",
    "                                     ) \n",
    "                                      "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Generate hyperparameters for hyperparameter tuning using sklearn's Randomized Search\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "#lr = np.random.uniform(low = 0.000001,high = 0.05, size = 20).tolist()\n",
    "lr = [0.00005, 0.0001, 0.0003, 0.0005, 0.001, 0.003, 0.005]\n",
    "\n",
    "weight_decay_for_regularization = [1e-5, 5e-5, 1e-4, 5e-5, 1e-3, 5e-3, 1e-2, 5e-2, 1e-1]      #weight decay equals L2 regularization for SGD\n",
    "\n",
    "momentum_vals = [0.5, 0.75, 0.99]\n",
    "\n",
    "momentum_dampening = [0.]\n",
    "\n",
    "nesterov = [True, False]\n",
    "\n",
    "no_of_nodes_per_layer = [num_features, int(num_features * 1.25), int(num_features * 1.5), int(num_features * 1.75), int(num_features *2), int(num_features * 2.5)]\n",
    "\n",
    "#max_epochs = [epoch_num for epoch_num in range(25, 400, 25)]\n",
    "max_epochs = [50, 75, 100, 125, 150, 175, 200, 225, 250, 275, 300, 325, 350, 375, 400]\n",
    "\n",
    "dropout_probability_per_node = [0., 0.3, 0.5]\n",
    "\n",
    "#We will use onle mse as the loss function for now. so skip tuning the loss function.\n",
    "\n",
    "minibatch_size = [1, 2, 4, 8, 16, 32, 64] #should always be less than the size of the trianing set\n",
    "\n",
    "#optimizers = [torch.optim.SGD, torch.optim.RMSprop, torch.optim.Adagrad]\n",
    "optimizers = [torch.optim.SGD] #Using only SGD"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Manually generate a list of random points in the hyperparameter space"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[5e-05, 0.0001, 0.99, 0.0, False, 21, 350, 0.0, 32, <class 'torch.optim.sgd.SGD'>]\n",
      "[0.003, 5e-05, 0.75, 0.0, True, 17, 100, 0.5, 16, <class 'torch.optim.sgd.SGD'>]\n"
     ]
    }
   ],
   "source": [
    "hyperparam_space_list = [lr,weight_decay_for_regularization, momentum_vals, momentum_dampening, nesterov, \n",
    "                        no_of_nodes_per_layer, max_epochs, dropout_probability_per_node, minibatch_size, optimizers]\n",
    "\n",
    "random_hyperparameter_configurations = []\n",
    "\n",
    "no_of_hyperparam_configs = 2\n",
    "no_of_cv_folds = 2\n",
    "\n",
    "for idx in range(no_of_hyperparam_configs):\n",
    "    configuration = []\n",
    "        \n",
    "    for idx in range(len(hyperparam_space_list)):\n",
    "        configuration.append(random.choice(hyperparam_space_list[idx]))\n",
    "\n",
    "    random_hyperparameter_configurations.append(configuration)\n",
    "\n",
    "    configuration = []\n",
    "\n",
    "# Print and save the random hyperparameters' configurations\n",
    "for idx in range(no_of_hyperparam_configs):\n",
    "    print(random_hyperparameter_configurations[idx])\n",
    "\n",
    "#random_hyperparam_configs = np.savetxt(\"random_hyperparam_configs.csv\", np.array(random_hyperparameter_configurations), delimiter = \",\")\n",
    "\n",
    "column_names = [\"lr\", \"weight_decay_for_regularization\", \"momentum_vals\", \"momentum_dampening\", \"nesterov\", \"no_of_nodes_per_layer\", \n",
    "                \"max_epochs\", \"dropout_probability_per_node\", \"minibatch_size\", \"optimizers\"]\n",
    "with open('random_hyperparam_configs', 'w') as f: \n",
    "\n",
    "    write = csv.writer(f) \n",
    "    write.writerow(column_names) \n",
    "    write.writerows(random_hyperparameter_configurations)            \n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Run a for loop for each of the hyperparameter configuration generated above\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  epoch    train_loss    valid_loss     dur\n",
      "-------  ------------  ------------  ------\n",
      "      1        \u001b[36m0.2886\u001b[0m        \u001b[32m0.2336\u001b[0m  0.0040\n",
      "      2        \u001b[36m0.2884\u001b[0m        \u001b[32m0.2332\u001b[0m  0.0050\n",
      "      3        \u001b[36m0.2878\u001b[0m        \u001b[32m0.2325\u001b[0m  0.0040\n",
      "      4        \u001b[36m0.2870\u001b[0m        \u001b[32m0.2315\u001b[0m  0.0040\n",
      "      5        \u001b[36m0.2859\u001b[0m        \u001b[32m0.2303\u001b[0m  0.0040\n",
      "      6        \u001b[36m0.2846\u001b[0m        \u001b[32m0.2289\u001b[0m  0.0050\n",
      "      7        \u001b[36m0.2830\u001b[0m        \u001b[32m0.2273\u001b[0m  0.0040\n",
      "      8        \u001b[36m0.2811\u001b[0m        \u001b[32m0.2255\u001b[0m  0.0049\n",
      "      9        \u001b[36m0.2791\u001b[0m        \u001b[32m0.2234\u001b[0m  0.0050\n",
      "     10        \u001b[36m0.2768\u001b[0m        \u001b[32m0.2212\u001b[0m  0.0040\n",
      "     11        \u001b[36m0.2743\u001b[0m        \u001b[32m0.2188\u001b[0m  0.0040\n",
      "     12        \u001b[36m0.2715\u001b[0m        \u001b[32m0.2162\u001b[0m  0.0040\n",
      "     13        \u001b[36m0.2686\u001b[0m        \u001b[32m0.2134\u001b[0m  0.0050\n",
      "     14        \u001b[36m0.2655\u001b[0m        \u001b[32m0.2105\u001b[0m  0.0040\n",
      "     15        \u001b[36m0.2622\u001b[0m        \u001b[32m0.2074\u001b[0m  0.0050\n",
      "     16        \u001b[36m0.2588\u001b[0m        \u001b[32m0.2042\u001b[0m  0.0050\n",
      "     17        \u001b[36m0.2552\u001b[0m        \u001b[32m0.2009\u001b[0m  0.0040\n",
      "     18        \u001b[36m0.2514\u001b[0m        \u001b[32m0.1974\u001b[0m  0.0040\n",
      "     19        \u001b[36m0.2475\u001b[0m        \u001b[32m0.1938\u001b[0m  0.0050\n",
      "     20        \u001b[36m0.2435\u001b[0m        \u001b[32m0.1902\u001b[0m  0.0040\n",
      "     21        \u001b[36m0.2393\u001b[0m        \u001b[32m0.1864\u001b[0m  0.0040\n",
      "     22        \u001b[36m0.2351\u001b[0m        \u001b[32m0.1825\u001b[0m  0.0050\n",
      "     23        \u001b[36m0.2308\u001b[0m        \u001b[32m0.1786\u001b[0m  0.0040\n",
      "     24        \u001b[36m0.2263\u001b[0m        \u001b[32m0.1747\u001b[0m  0.0040\n",
      "     25        \u001b[36m0.2218\u001b[0m        \u001b[32m0.1706\u001b[0m  0.0040\n",
      "     26        \u001b[36m0.2173\u001b[0m        \u001b[32m0.1666\u001b[0m  0.0050\n",
      "     27        \u001b[36m0.2126\u001b[0m        \u001b[32m0.1624\u001b[0m  0.0040\n",
      "     28        \u001b[36m0.2080\u001b[0m        \u001b[32m0.1583\u001b[0m  0.0040\n",
      "     29        \u001b[36m0.2033\u001b[0m        \u001b[32m0.1542\u001b[0m  0.0060\n",
      "     30        \u001b[36m0.1985\u001b[0m        \u001b[32m0.1500\u001b[0m  0.0040\n",
      "     31        \u001b[36m0.1938\u001b[0m        \u001b[32m0.1458\u001b[0m  0.0040\n",
      "     32        \u001b[36m0.1890\u001b[0m        \u001b[32m0.1417\u001b[0m  0.0060\n",
      "     33        \u001b[36m0.1842\u001b[0m        \u001b[32m0.1375\u001b[0m  0.0050\n",
      "     34        \u001b[36m0.1795\u001b[0m        \u001b[32m0.1334\u001b[0m  0.0050\n",
      "     35        \u001b[36m0.1747\u001b[0m        \u001b[32m0.1293\u001b[0m  0.0040\n",
      "     36        \u001b[36m0.1700\u001b[0m        \u001b[32m0.1253\u001b[0m  0.0050\n",
      "     37        \u001b[36m0.1653\u001b[0m        \u001b[32m0.1212\u001b[0m  0.0040\n",
      "     38        \u001b[36m0.1606\u001b[0m        \u001b[32m0.1173\u001b[0m  0.0040\n",
      "     39        \u001b[36m0.1560\u001b[0m        \u001b[32m0.1134\u001b[0m  0.0040\n",
      "     40        \u001b[36m0.1514\u001b[0m        \u001b[32m0.1095\u001b[0m  0.0050\n",
      "     41        \u001b[36m0.1469\u001b[0m        \u001b[32m0.1057\u001b[0m  0.0050\n",
      "     42        \u001b[36m0.1424\u001b[0m        \u001b[32m0.1020\u001b[0m  0.0050\n",
      "     43        \u001b[36m0.1380\u001b[0m        \u001b[32m0.0984\u001b[0m  0.0040\n",
      "     44        \u001b[36m0.1337\u001b[0m        \u001b[32m0.0949\u001b[0m  0.0050\n",
      "     45        \u001b[36m0.1295\u001b[0m        \u001b[32m0.0914\u001b[0m  0.0050\n",
      "     46        \u001b[36m0.1253\u001b[0m        \u001b[32m0.0880\u001b[0m  0.0040\n",
      "     47        \u001b[36m0.1212\u001b[0m        \u001b[32m0.0848\u001b[0m  0.0040\n",
      "     48        \u001b[36m0.1173\u001b[0m        \u001b[32m0.0816\u001b[0m  0.0050\n",
      "     49        \u001b[36m0.1134\u001b[0m        \u001b[32m0.0785\u001b[0m  0.0050\n",
      "     50        \u001b[36m0.1096\u001b[0m        \u001b[32m0.0755\u001b[0m  0.0040\n",
      "     51        \u001b[36m0.1059\u001b[0m        \u001b[32m0.0727\u001b[0m  0.0050\n",
      "     52        \u001b[36m0.1023\u001b[0m        \u001b[32m0.0699\u001b[0m  0.0050\n",
      "     53        \u001b[36m0.0989\u001b[0m        \u001b[32m0.0673\u001b[0m  0.0040\n",
      "     54        \u001b[36m0.0955\u001b[0m        \u001b[32m0.0647\u001b[0m  0.0040\n",
      "     55        \u001b[36m0.0923\u001b[0m        \u001b[32m0.0623\u001b[0m  0.0040\n",
      "     56        \u001b[36m0.0891\u001b[0m        \u001b[32m0.0600\u001b[0m  0.0050\n",
      "     57        \u001b[36m0.0861\u001b[0m        \u001b[32m0.0578\u001b[0m  0.0040\n",
      "     58        \u001b[36m0.0832\u001b[0m        \u001b[32m0.0557\u001b[0m  0.0040\n",
      "     59        \u001b[36m0.0804\u001b[0m        \u001b[32m0.0538\u001b[0m  0.0050\n",
      "     60        \u001b[36m0.0778\u001b[0m        \u001b[32m0.0519\u001b[0m  0.0050\n",
      "     61        \u001b[36m0.0752\u001b[0m        \u001b[32m0.0502\u001b[0m  0.0050\n",
      "     62        \u001b[36m0.0728\u001b[0m        \u001b[32m0.0486\u001b[0m  0.0050\n",
      "     63        \u001b[36m0.0705\u001b[0m        \u001b[32m0.0471\u001b[0m  0.0050\n",
      "     64        \u001b[36m0.0683\u001b[0m        \u001b[32m0.0457\u001b[0m  0.0050\n",
      "     65        \u001b[36m0.0662\u001b[0m        \u001b[32m0.0444\u001b[0m  0.0060\n",
      "     66        \u001b[36m0.0643\u001b[0m        \u001b[32m0.0433\u001b[0m  0.0050\n",
      "     67        \u001b[36m0.0625\u001b[0m        \u001b[32m0.0422\u001b[0m  0.0050\n",
      "     68        \u001b[36m0.0607\u001b[0m        \u001b[32m0.0413\u001b[0m  0.0050\n",
      "     69        \u001b[36m0.0591\u001b[0m        \u001b[32m0.0404\u001b[0m  0.0050\n",
      "     70        \u001b[36m0.0576\u001b[0m        \u001b[32m0.0397\u001b[0m  0.0050\n",
      "     71        \u001b[36m0.0562\u001b[0m        \u001b[32m0.0391\u001b[0m  0.0050\n",
      "     72        \u001b[36m0.0550\u001b[0m        \u001b[32m0.0386\u001b[0m  0.0050\n",
      "     73        \u001b[36m0.0538\u001b[0m        \u001b[32m0.0381\u001b[0m  0.0040\n",
      "     74        \u001b[36m0.0527\u001b[0m        \u001b[32m0.0378\u001b[0m  0.0040\n",
      "     75        \u001b[36m0.0517\u001b[0m        \u001b[32m0.0375\u001b[0m  0.0050\n",
      "     76        \u001b[36m0.0509\u001b[0m        \u001b[32m0.0374\u001b[0m  0.0050\n",
      "     77        \u001b[36m0.0501\u001b[0m        \u001b[32m0.0373\u001b[0m  0.0050\n",
      "     78        \u001b[36m0.0494\u001b[0m        0.0373  0.0040\n",
      "     79        \u001b[36m0.0488\u001b[0m        0.0374  0.0040\n",
      "     80        \u001b[36m0.0483\u001b[0m        0.0375  0.0040\n",
      "     81        \u001b[36m0.0479\u001b[0m        0.0378  0.0040\n",
      "     82        \u001b[36m0.0475\u001b[0m        0.0381  0.0040\n",
      "     83        \u001b[36m0.0473\u001b[0m        0.0384  0.0050\n",
      "     84        \u001b[36m0.0471\u001b[0m        0.0388  0.0050\n",
      "     85        \u001b[36m0.0469\u001b[0m        0.0393  0.0050\n",
      "     86        \u001b[36m0.0469\u001b[0m        0.0398  0.0040\n",
      "     87        \u001b[36m0.0469\u001b[0m        0.0404  0.0040\n",
      "     88        0.0469        0.0410  0.0040\n",
      "     89        0.0470        0.0417  0.0040\n",
      "     90        0.0472        0.0424  0.0040\n",
      "     91        0.0474        0.0431  0.0040\n",
      "     92        0.0477        0.0439  0.0040\n",
      "     93        0.0480        0.0447  0.0040\n",
      "     94        0.0483        0.0455  0.0040\n",
      "     95        0.0487        0.0463  0.0050\n",
      "     96        0.0491        0.0472  0.0040\n",
      "     97        0.0495        0.0480  0.0040\n",
      "     98        0.0499        0.0489  0.0050\n",
      "     99        0.0504        0.0498  0.0040\n",
      "    100        0.0509        0.0506  0.0050\n",
      "    101        0.0514        0.0515  0.0040\n",
      "    102        0.0519        0.0524  0.0040\n",
      "    103        0.0524        0.0532  0.0050\n",
      "    104        0.0530        0.0541  0.0040\n",
      "    105        0.0535        0.0549  0.0040\n",
      "    106        0.0540        0.0558  0.0040\n",
      "    107        0.0546        0.0566  0.0050\n",
      "    108        0.0551        0.0574  0.0040\n",
      "    109        0.0556        0.0582  0.0050\n",
      "    110        0.0561        0.0589  0.0040\n",
      "    111        0.0566        0.0596  0.0050\n",
      "    112        0.0571        0.0603  0.0040\n",
      "    113        0.0576        0.0610  0.0040\n",
      "    114        0.0580        0.0617  0.0040\n",
      "    115        0.0585        0.0623  0.0040\n",
      "    116        0.0589        0.0628  0.0040\n",
      "    117        0.0593        0.0634  0.0040\n",
      "    118        0.0597        0.0639  0.0050\n",
      "    119        0.0600        0.0644  0.0040\n",
      "    120        0.0603        0.0648  0.0080\n",
      "    121        0.0607        0.0653  0.0040\n",
      "    122        0.0609        0.0656  0.0040\n",
      "    123        0.0612        0.0660  0.0050\n",
      "    124        0.0614        0.0663  0.0050\n",
      "    125        0.0617        0.0665  0.0050\n",
      "    126        0.0618        0.0668  0.0040\n",
      "    127        0.0620        0.0669  0.0040\n",
      "    128        0.0621        0.0671  0.0040\n",
      "    129        0.0622        0.0672  0.0050\n",
      "    130        0.0623        0.0673  0.0040\n",
      "    131        0.0624        0.0674  0.0050\n",
      "    132        0.0624        0.0674  0.0040\n",
      "    133        0.0624        0.0674  0.0050\n",
      "    134        0.0624        0.0673  0.0040\n",
      "    135        0.0624        0.0672  0.0040\n",
      "    136        0.0623        0.0671  0.0040\n",
      "    137        0.0622        0.0670  0.0040\n",
      "    138        0.0621        0.0668  0.0050\n",
      "    139        0.0620        0.0666  0.0060\n",
      "    140        0.0618        0.0664  0.0050\n",
      "    141        0.0617        0.0661  0.0040\n",
      "    142        0.0615        0.0658  0.0040\n",
      "    143        0.0613        0.0655  0.0040\n",
      "    144        0.0610        0.0652  0.0050\n",
      "    145        0.0608        0.0648  0.0080\n",
      "    146        0.0606        0.0645  0.0040\n",
      "    147        0.0603        0.0641  0.0050\n",
      "    148        0.0600        0.0637  0.0040\n",
      "    149        0.0597        0.0633  0.0050\n",
      "    150        0.0594        0.0628  0.0040\n",
      "    151        0.0591        0.0624  0.0050\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "    152        0.0588        0.0619  0.0040\n",
      "    153        0.0585        0.0614  0.0040\n",
      "    154        0.0582        0.0609  0.0040\n",
      "    155        0.0578        0.0604  0.0040\n",
      "    156        0.0575        0.0599  0.0040\n",
      "    157        0.0571        0.0594  0.0040\n",
      "    158        0.0568        0.0589  0.0050\n",
      "    159        0.0564        0.0584  0.0050\n",
      "    160        0.0561        0.0578  0.0050\n",
      "    161        0.0557        0.0573  0.0043\n",
      "    162        0.0554        0.0568  0.0040\n",
      "    163        0.0550        0.0562  0.0040\n",
      "    164        0.0547        0.0557  0.0050\n",
      "    165        0.0544        0.0552  0.0050\n",
      "    166        0.0540        0.0547  0.0050\n",
      "    167        0.0537        0.0541  0.0050\n",
      "    168        0.0534        0.0536  0.0040\n",
      "    169        0.0530        0.0531  0.0040\n",
      "    170        0.0527        0.0526  0.0040\n",
      "    171        0.0524        0.0520  0.0050\n",
      "    172        0.0521        0.0515  0.0040\n",
      "    173        0.0518        0.0510  0.0050\n",
      "    174        0.0515        0.0506  0.0050\n",
      "    175        0.0512        0.0501  0.0040\n",
      "    176        0.0509        0.0496  0.0040\n",
      "    177        0.0507        0.0491  0.0040\n",
      "    178        0.0504        0.0487  0.0040\n",
      "    179        0.0501        0.0482  0.0050\n",
      "    180        0.0499        0.0478  0.0050\n",
      "    181        0.0497        0.0473  0.0040\n",
      "    182        0.0495        0.0469  0.0040\n",
      "    183        0.0492        0.0465  0.0040\n",
      "    184        0.0490        0.0461  0.0040\n",
      "    185        0.0488        0.0457  0.0040\n",
      "    186        0.0487        0.0453  0.0040\n",
      "    187        0.0485        0.0450  0.0040\n",
      "    188        0.0483        0.0446  0.0040\n",
      "    189        0.0482        0.0443  0.0040\n",
      "    190        0.0480        0.0439  0.0050\n",
      "    191        0.0479        0.0436  0.0050\n",
      "    192        0.0478        0.0433  0.0050\n",
      "    193        0.0476        0.0430  0.0040\n",
      "    194        0.0475        0.0427  0.0040\n",
      "    195        0.0474        0.0424  0.0050\n",
      "    196        0.0473        0.0421  0.0050\n",
      "    197        0.0473        0.0419  0.0049\n",
      "    198        0.0472        0.0416  0.0040\n",
      "    199        0.0471        0.0414  0.0040\n",
      "    200        0.0470        0.0412  0.0050\n",
      "    201        0.0470        0.0409  0.0050\n",
      "    202        0.0470        0.0407  0.0050\n",
      "    203        0.0469        0.0405  0.0039\n",
      "    204        0.0469        0.0403  0.0040\n",
      "    205        \u001b[36m0.0469\u001b[0m        0.0401  0.0050\n",
      "    206        \u001b[36m0.0468\u001b[0m        0.0400  0.0040\n",
      "    207        \u001b[36m0.0468\u001b[0m        0.0398  0.0040\n",
      "    208        \u001b[36m0.0468\u001b[0m        0.0396  0.0040\n",
      "    209        \u001b[36m0.0468\u001b[0m        0.0395  0.0050\n",
      "    210        0.0468        0.0394  0.0040\n",
      "    211        0.0468        0.0392  0.0040\n",
      "    212        0.0468        0.0391  0.0050\n",
      "    213        0.0469        0.0390  0.0050\n",
      "    214        0.0469        0.0389  0.0040\n",
      "    215        0.0469        0.0387  0.0040\n",
      "    216        0.0469        0.0386  0.0050\n",
      "    217        0.0470        0.0386  0.0040\n",
      "    218        0.0470        0.0385  0.0040\n",
      "    219        0.0470        0.0384  0.0040\n",
      "    220        0.0471        0.0383  0.0050\n",
      "    221        0.0471        0.0382  0.0049\n",
      "    222        0.0472        0.0382  0.0040\n",
      "    223        0.0472        0.0381  0.0050\n",
      "    224        0.0472        0.0380  0.0040\n",
      "    225        0.0473        0.0380  0.0040\n",
      "    226        0.0473        0.0379  0.0049\n",
      "    227        0.0474        0.0379  0.0050\n",
      "    228        0.0474        0.0378  0.0050\n",
      "    229        0.0475        0.0378  0.0050\n",
      "    230        0.0475        0.0377  0.0050\n",
      "    231        0.0476        0.0377  0.0050\n",
      "    232        0.0476        0.0377  0.0050\n",
      "    233        0.0477        0.0376  0.0040\n",
      "    234        0.0477        0.0376  0.0040\n",
      "    235        0.0478        0.0376  0.0040\n",
      "    236        0.0478        0.0376  0.0050\n",
      "    237        0.0478        0.0376  0.0040\n",
      "    238        0.0479        0.0375  0.0039\n",
      "    239        0.0479        0.0375  0.0050\n",
      "    240        0.0480        0.0375  0.0060\n",
      "    241        0.0480        0.0375  0.0050\n",
      "    242        0.0480        0.0375  0.0040\n",
      "    243        0.0481        0.0375  0.0050\n",
      "    244        0.0481        0.0374  0.0050\n",
      "    245        0.0481        0.0374  0.0040\n",
      "    246        0.0482        0.0374  0.0040\n",
      "    247        0.0482        0.0374  0.0040\n",
      "    248        0.0482        0.0374  0.0040\n",
      "    249        0.0482        0.0374  0.0040\n",
      "    250        0.0483        0.0374  0.0040\n",
      "    251        0.0483        0.0374  0.0040\n",
      "    252        0.0483        0.0374  0.0060\n",
      "    253        0.0483        0.0374  0.0050\n",
      "    254        0.0483        0.0374  0.0040\n",
      "    255        0.0483        0.0374  0.0050\n",
      "    256        0.0483        0.0374  0.0050\n",
      "    257        0.0484        0.0374  0.0040\n",
      "    258        0.0484        0.0374  0.0040\n",
      "    259        0.0484        0.0374  0.0050\n",
      "    260        0.0484        0.0374  0.0040\n",
      "    261        0.0484        0.0374  0.0040\n",
      "    262        0.0484        0.0374  0.0040\n",
      "    263        0.0484        0.0374  0.0040\n",
      "    264        0.0483        0.0374  0.0040\n",
      "    265        0.0483        0.0374  0.0040\n",
      "    266        0.0483        0.0374  0.0050\n",
      "    267        0.0483        0.0374  0.0040\n",
      "    268        0.0483        0.0374  0.0040\n",
      "    269        0.0483        0.0374  0.0050\n",
      "    270        0.0483        0.0374  0.0040\n",
      "    271        0.0483        0.0374  0.0050\n",
      "    272        0.0482        0.0374  0.0050\n",
      "    273        0.0482        0.0374  0.0040\n",
      "    274        0.0482        0.0374  0.0050\n",
      "    275        0.0482        0.0374  0.0040\n",
      "    276        0.0482        0.0374  0.0040\n",
      "    277        0.0481        0.0374  0.0040\n",
      "    278        0.0481        0.0374  0.0040\n",
      "    279        0.0481        0.0375  0.0040\n",
      "    280        0.0481        0.0375  0.0050\n",
      "    281        0.0480        0.0375  0.0050\n",
      "    282        0.0480        0.0375  0.0060\n",
      "    283        0.0480        0.0375  0.0050\n",
      "    284        0.0480        0.0375  0.0050\n",
      "    285        0.0479        0.0375  0.0050\n",
      "    286        0.0479        0.0375  0.0040\n",
      "    287        0.0479        0.0376  0.0050\n",
      "    288        0.0478        0.0376  0.0040\n",
      "    289        0.0478        0.0376  0.0050\n",
      "    290        0.0478        0.0376  0.0050\n",
      "    291        0.0477        0.0376  0.0050\n",
      "    292        0.0477        0.0376  0.0040\n",
      "    293        0.0477        0.0377  0.0050\n",
      "    294        0.0477        0.0377  0.0050\n",
      "    295        0.0476        0.0377  0.0060\n",
      "    296        0.0476        0.0377  0.0049\n",
      "    297        0.0476        0.0377  0.0049\n",
      "    298        0.0475        0.0378  0.0050\n",
      "    299        0.0475        0.0378  0.0049\n",
      "    300        0.0475        0.0378  0.0040\n",
      "    301        0.0475        0.0378  0.0040\n",
      "    302        0.0474        0.0379  0.0050\n",
      "    303        0.0474        0.0379  0.0049\n",
      "    304        0.0474        0.0379  0.0050\n",
      "    305        0.0473        0.0380  0.0040\n",
      "    306        0.0473        0.0380  0.0049\n",
      "    307        0.0473        0.0380  0.0040\n",
      "    308        0.0473        0.0380  0.0050\n",
      "    309        0.0472        0.0381  0.0040\n",
      "    310        0.0472        0.0381  0.0050\n",
      "    311        0.0472        0.0381  0.0050\n",
      "    312        0.0472        0.0382  0.0040\n",
      "    313        0.0472        0.0382  0.0040\n",
      "    314        0.0471        0.0382  0.0050\n",
      "    315        0.0471        0.0383  0.0040\n",
      "    316        0.0471        0.0383  0.0040\n",
      "    317        0.0471        0.0383  0.0040\n",
      "    318        0.0471        0.0384  0.0040\n",
      "    319        0.0470        0.0384  0.0050\n",
      "    320        0.0470        0.0385  0.0040\n",
      "    321        0.0470        0.0385  0.0040\n",
      "    322        0.0470        0.0385  0.0050\n",
      "    323        0.0470        0.0386  0.0050\n",
      "    324        0.0470        0.0386  0.0040\n",
      "    325        0.0470        0.0386  0.0050\n",
      "    326        0.0469        0.0387  0.0050\n",
      "    327        0.0469        0.0387  0.0050\n",
      "    328        0.0469        0.0388  0.0040\n",
      "    329        0.0469        0.0388  0.0040\n",
      "    330        0.0469        0.0388  0.0050\n",
      "    331        0.0469        0.0389  0.0040\n",
      "    332        0.0469        0.0389  0.0050\n",
      "    333        0.0469        0.0389  0.0050\n",
      "    334        0.0469        0.0390  0.0040\n",
      "    335        0.0469        0.0390  0.0040\n",
      "    336        0.0468        0.0391  0.0040\n",
      "    337        0.0468        0.0391  0.0040\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "    338        0.0468        0.0391  0.0040\n",
      "    339        0.0468        0.0392  0.0040\n",
      "    340        0.0468        0.0392  0.0040\n",
      "    341        0.0468        0.0392  0.0050\n",
      "    342        \u001b[36m0.0468\u001b[0m        0.0393  0.0040\n",
      "    343        \u001b[36m0.0468\u001b[0m        0.0393  0.0040\n",
      "    344        \u001b[36m0.0468\u001b[0m        0.0393  0.0050\n",
      "    345        \u001b[36m0.0468\u001b[0m        0.0394  0.0040\n",
      "    346        \u001b[36m0.0468\u001b[0m        0.0394  0.0040\n",
      "    347        \u001b[36m0.0468\u001b[0m        0.0395  0.0040\n",
      "    348        \u001b[36m0.0468\u001b[0m        0.0395  0.0040\n",
      "    349        \u001b[36m0.0468\u001b[0m        0.0395  0.0050\n",
      "    350        \u001b[36m0.0468\u001b[0m        0.0395  0.0050\n",
      "  epoch    train_loss    valid_loss     dur\n",
      "-------  ------------  ------------  ------\n",
      "      1        \u001b[36m0.2245\u001b[0m        \u001b[32m0.3129\u001b[0m  0.0040\n",
      "      2        \u001b[36m0.2243\u001b[0m        \u001b[32m0.3125\u001b[0m  0.0050\n",
      "      3        \u001b[36m0.2239\u001b[0m        \u001b[32m0.3118\u001b[0m  0.0050\n",
      "      4        \u001b[36m0.2234\u001b[0m        \u001b[32m0.3110\u001b[0m  0.0040\n",
      "      5        \u001b[36m0.2227\u001b[0m        \u001b[32m0.3099\u001b[0m  0.0040\n",
      "      6        \u001b[36m0.2218\u001b[0m        \u001b[32m0.3086\u001b[0m  0.0050\n",
      "      7        \u001b[36m0.2207\u001b[0m        \u001b[32m0.3071\u001b[0m  0.0040\n",
      "      8        \u001b[36m0.2195\u001b[0m        \u001b[32m0.3055\u001b[0m  0.0050\n",
      "      9        \u001b[36m0.2181\u001b[0m        \u001b[32m0.3036\u001b[0m  0.0050\n",
      "     10        \u001b[36m0.2165\u001b[0m        \u001b[32m0.3016\u001b[0m  0.0040\n",
      "     11        \u001b[36m0.2148\u001b[0m        \u001b[32m0.2993\u001b[0m  0.0050\n",
      "     12        \u001b[36m0.2130\u001b[0m        \u001b[32m0.2969\u001b[0m  0.0040\n",
      "     13        \u001b[36m0.2110\u001b[0m        \u001b[32m0.2944\u001b[0m  0.0040\n",
      "     14        \u001b[36m0.2090\u001b[0m        \u001b[32m0.2917\u001b[0m  0.0070\n",
      "     15        \u001b[36m0.2067\u001b[0m        \u001b[32m0.2888\u001b[0m  0.0060\n",
      "     16        \u001b[36m0.2044\u001b[0m        \u001b[32m0.2858\u001b[0m  0.0060\n",
      "     17        \u001b[36m0.2020\u001b[0m        \u001b[32m0.2827\u001b[0m  0.0080\n",
      "     18        \u001b[36m0.1994\u001b[0m        \u001b[32m0.2794\u001b[0m  0.0060\n",
      "     19        \u001b[36m0.1968\u001b[0m        \u001b[32m0.2761\u001b[0m  0.0060\n",
      "     20        \u001b[36m0.1941\u001b[0m        \u001b[32m0.2726\u001b[0m  0.0060\n",
      "     21        \u001b[36m0.1913\u001b[0m        \u001b[32m0.2690\u001b[0m  0.0050\n",
      "     22        \u001b[36m0.1884\u001b[0m        \u001b[32m0.2654\u001b[0m  0.0060\n",
      "     23        \u001b[36m0.1855\u001b[0m        \u001b[32m0.2616\u001b[0m  0.0050\n",
      "     24        \u001b[36m0.1825\u001b[0m        \u001b[32m0.2578\u001b[0m  0.0040\n",
      "     25        \u001b[36m0.1794\u001b[0m        \u001b[32m0.2539\u001b[0m  0.0070\n",
      "     26        \u001b[36m0.1763\u001b[0m        \u001b[32m0.2499\u001b[0m  0.0050\n",
      "     27        \u001b[36m0.1732\u001b[0m        \u001b[32m0.2459\u001b[0m  0.0050\n",
      "     28        \u001b[36m0.1700\u001b[0m        \u001b[32m0.2418\u001b[0m  0.0070\n",
      "     29        \u001b[36m0.1668\u001b[0m        \u001b[32m0.2377\u001b[0m  0.0060\n",
      "     30        \u001b[36m0.1635\u001b[0m        \u001b[32m0.2335\u001b[0m  0.0070\n",
      "     31        \u001b[36m0.1603\u001b[0m        \u001b[32m0.2293\u001b[0m  0.0070\n",
      "     32        \u001b[36m0.1570\u001b[0m        \u001b[32m0.2251\u001b[0m  0.0050\n",
      "     33        \u001b[36m0.1537\u001b[0m        \u001b[32m0.2209\u001b[0m  0.0060\n",
      "     34        \u001b[36m0.1505\u001b[0m        \u001b[32m0.2166\u001b[0m  0.0050\n",
      "     35        \u001b[36m0.1472\u001b[0m        \u001b[32m0.2124\u001b[0m  0.0060\n",
      "     36        \u001b[36m0.1439\u001b[0m        \u001b[32m0.2081\u001b[0m  0.0050\n",
      "     37        \u001b[36m0.1407\u001b[0m        \u001b[32m0.2038\u001b[0m  0.0050\n",
      "     38        \u001b[36m0.1374\u001b[0m        \u001b[32m0.1996\u001b[0m  0.0070\n",
      "     39        \u001b[36m0.1342\u001b[0m        \u001b[32m0.1954\u001b[0m  0.0070\n",
      "     40        \u001b[36m0.1310\u001b[0m        \u001b[32m0.1912\u001b[0m  0.0050\n",
      "     41        \u001b[36m0.1279\u001b[0m        \u001b[32m0.1870\u001b[0m  0.0060\n",
      "     42        \u001b[36m0.1248\u001b[0m        \u001b[32m0.1829\u001b[0m  0.0050\n",
      "     43        \u001b[36m0.1217\u001b[0m        \u001b[32m0.1788\u001b[0m  0.0060\n",
      "     44        \u001b[36m0.1187\u001b[0m        \u001b[32m0.1747\u001b[0m  0.0050\n",
      "     45        \u001b[36m0.1157\u001b[0m        \u001b[32m0.1707\u001b[0m  0.0040\n",
      "     46        \u001b[36m0.1128\u001b[0m        \u001b[32m0.1667\u001b[0m  0.0050\n",
      "     47        \u001b[36m0.1099\u001b[0m        \u001b[32m0.1628\u001b[0m  0.0040\n",
      "     48        \u001b[36m0.1071\u001b[0m        \u001b[32m0.1591\u001b[0m  0.0040\n",
      "     49        \u001b[36m0.1045\u001b[0m        \u001b[32m0.1554\u001b[0m  0.0040\n",
      "     50        \u001b[36m0.1019\u001b[0m        \u001b[32m0.1518\u001b[0m  0.0040\n",
      "     51        \u001b[36m0.0994\u001b[0m        \u001b[32m0.1483\u001b[0m  0.0040\n",
      "     52        \u001b[36m0.0969\u001b[0m        \u001b[32m0.1448\u001b[0m  0.0040\n",
      "     53        \u001b[36m0.0945\u001b[0m        \u001b[32m0.1414\u001b[0m  0.0050\n",
      "     54        \u001b[36m0.0922\u001b[0m        \u001b[32m0.1380\u001b[0m  0.0040\n",
      "     55        \u001b[36m0.0899\u001b[0m        \u001b[32m0.1347\u001b[0m  0.0040\n",
      "     56        \u001b[36m0.0877\u001b[0m        \u001b[32m0.1315\u001b[0m  0.0050\n",
      "     57        \u001b[36m0.0855\u001b[0m        \u001b[32m0.1283\u001b[0m  0.0040\n",
      "     58        \u001b[36m0.0835\u001b[0m        \u001b[32m0.1252\u001b[0m  0.0050\n",
      "     59        \u001b[36m0.0815\u001b[0m        \u001b[32m0.1222\u001b[0m  0.0040\n",
      "     60        \u001b[36m0.0795\u001b[0m        \u001b[32m0.1193\u001b[0m  0.0040\n",
      "     61        \u001b[36m0.0777\u001b[0m        \u001b[32m0.1164\u001b[0m  0.0050\n",
      "     62        \u001b[36m0.0759\u001b[0m        \u001b[32m0.1136\u001b[0m  0.0040\n",
      "     63        \u001b[36m0.0742\u001b[0m        \u001b[32m0.1109\u001b[0m  0.0040\n",
      "     64        \u001b[36m0.0725\u001b[0m        \u001b[32m0.1083\u001b[0m  0.0050\n",
      "     65        \u001b[36m0.0709\u001b[0m        \u001b[32m0.1057\u001b[0m  0.0040\n",
      "     66        \u001b[36m0.0694\u001b[0m        \u001b[32m0.1032\u001b[0m  0.0040\n",
      "     67        \u001b[36m0.0680\u001b[0m        \u001b[32m0.1008\u001b[0m  0.0040\n",
      "     68        \u001b[36m0.0667\u001b[0m        \u001b[32m0.0985\u001b[0m  0.0040\n",
      "     69        \u001b[36m0.0654\u001b[0m        \u001b[32m0.0963\u001b[0m  0.0040\n",
      "     70        \u001b[36m0.0642\u001b[0m        \u001b[32m0.0941\u001b[0m  0.0050\n",
      "     71        \u001b[36m0.0630\u001b[0m        \u001b[32m0.0920\u001b[0m  0.0040\n",
      "     72        \u001b[36m0.0619\u001b[0m        \u001b[32m0.0900\u001b[0m  0.0040\n",
      "     73        \u001b[36m0.0609\u001b[0m        \u001b[32m0.0880\u001b[0m  0.0040\n",
      "     74        \u001b[36m0.0600\u001b[0m        \u001b[32m0.0862\u001b[0m  0.0040\n",
      "     75        \u001b[36m0.0591\u001b[0m        \u001b[32m0.0844\u001b[0m  0.0040\n",
      "     76        \u001b[36m0.0583\u001b[0m        \u001b[32m0.0826\u001b[0m  0.0040\n",
      "     77        \u001b[36m0.0575\u001b[0m        \u001b[32m0.0810\u001b[0m  0.0040\n",
      "     78        \u001b[36m0.0569\u001b[0m        \u001b[32m0.0794\u001b[0m  0.0040\n",
      "     79        \u001b[36m0.0562\u001b[0m        \u001b[32m0.0779\u001b[0m  0.0040\n",
      "     80        \u001b[36m0.0557\u001b[0m        \u001b[32m0.0765\u001b[0m  0.0050\n",
      "     81        \u001b[36m0.0551\u001b[0m        \u001b[32m0.0751\u001b[0m  0.0040\n",
      "     82        \u001b[36m0.0547\u001b[0m        \u001b[32m0.0738\u001b[0m  0.0040\n",
      "     83        \u001b[36m0.0543\u001b[0m        \u001b[32m0.0725\u001b[0m  0.0040\n",
      "     84        \u001b[36m0.0539\u001b[0m        \u001b[32m0.0713\u001b[0m  0.0040\n",
      "     85        \u001b[36m0.0536\u001b[0m        \u001b[32m0.0702\u001b[0m  0.0040\n",
      "     86        \u001b[36m0.0534\u001b[0m        \u001b[32m0.0692\u001b[0m  0.0050\n",
      "     87        \u001b[36m0.0532\u001b[0m        \u001b[32m0.0682\u001b[0m  0.0040\n",
      "     88        \u001b[36m0.0530\u001b[0m        \u001b[32m0.0672\u001b[0m  0.0040\n",
      "     89        \u001b[36m0.0529\u001b[0m        \u001b[32m0.0663\u001b[0m  0.0060\n",
      "     90        \u001b[36m0.0528\u001b[0m        \u001b[32m0.0655\u001b[0m  0.0040\n",
      "     91        \u001b[36m0.0528\u001b[0m        \u001b[32m0.0647\u001b[0m  0.0050\n",
      "     92        \u001b[36m0.0527\u001b[0m        \u001b[32m0.0640\u001b[0m  0.0040\n",
      "     93        0.0528        \u001b[32m0.0633\u001b[0m  0.0040\n",
      "     94        0.0528        \u001b[32m0.0626\u001b[0m  0.0050\n",
      "     95        0.0529        \u001b[32m0.0620\u001b[0m  0.0040\n",
      "     96        0.0530        \u001b[32m0.0615\u001b[0m  0.0040\n",
      "     97        0.0532        \u001b[32m0.0609\u001b[0m  0.0040\n",
      "     98        0.0534        \u001b[32m0.0605\u001b[0m  0.0040\n",
      "     99        0.0535        \u001b[32m0.0600\u001b[0m  0.0040\n",
      "    100        0.0538        \u001b[32m0.0596\u001b[0m  0.0050\n",
      "    101        0.0540        \u001b[32m0.0592\u001b[0m  0.0040\n",
      "    102        0.0542        \u001b[32m0.0589\u001b[0m  0.0040\n",
      "    103        0.0545        \u001b[32m0.0586\u001b[0m  0.0030\n",
      "    104        0.0548        \u001b[32m0.0583\u001b[0m  0.0040\n",
      "    105        0.0551        \u001b[32m0.0580\u001b[0m  0.0050\n",
      "    106        0.0554        \u001b[32m0.0578\u001b[0m  0.0040\n",
      "    107        0.0557        \u001b[32m0.0576\u001b[0m  0.0040\n",
      "    108        0.0560        \u001b[32m0.0574\u001b[0m  0.0040\n",
      "    109        0.0563        \u001b[32m0.0572\u001b[0m  0.0050\n",
      "    110        0.0566        \u001b[32m0.0571\u001b[0m  0.0040\n",
      "    111        0.0569        \u001b[32m0.0569\u001b[0m  0.0040\n",
      "    112        0.0573        \u001b[32m0.0568\u001b[0m  0.0040\n",
      "    113        0.0576        \u001b[32m0.0567\u001b[0m  0.0040\n",
      "    114        0.0579        \u001b[32m0.0566\u001b[0m  0.0040\n",
      "    115        0.0582        \u001b[32m0.0566\u001b[0m  0.0050\n",
      "    116        0.0585        \u001b[32m0.0565\u001b[0m  0.0050\n",
      "    117        0.0588        \u001b[32m0.0565\u001b[0m  0.0050\n",
      "    118        0.0591        \u001b[32m0.0564\u001b[0m  0.0050\n",
      "    119        0.0594        \u001b[32m0.0564\u001b[0m  0.0050\n",
      "    120        0.0597        \u001b[32m0.0563\u001b[0m  0.0040\n",
      "    121        0.0600        \u001b[32m0.0563\u001b[0m  0.0040\n",
      "    122        0.0603        \u001b[32m0.0563\u001b[0m  0.0040\n",
      "    123        0.0605        \u001b[32m0.0563\u001b[0m  0.0040\n",
      "    124        0.0608        \u001b[32m0.0563\u001b[0m  0.0040\n",
      "    125        0.0610        \u001b[32m0.0563\u001b[0m  0.0040\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "    126        0.0612        0.0563  0.0080\n",
      "    127        0.0614        0.0563  0.0040\n",
      "    128        0.0616        0.0563  0.0040\n",
      "    129        0.0618        0.0563  0.0040\n",
      "    130        0.0620        0.0563  0.0050\n",
      "    131        0.0621        0.0563  0.0040\n",
      "    132        0.0623        0.0563  0.0050\n",
      "    133        0.0624        0.0563  0.0040\n",
      "    134        0.0625        0.0563  0.0040\n",
      "    135        0.0626        0.0563  0.0050\n",
      "    136        0.0627        0.0563  0.0050\n",
      "    137        0.0628        0.0564  0.0040\n",
      "    138        0.0628        0.0564  0.0050\n",
      "    139        0.0629        0.0564  0.0040\n",
      "    140        0.0629        0.0564  0.0050\n",
      "    141        0.0629        0.0564  0.0040\n",
      "    142        0.0629        0.0564  0.0040\n",
      "    143        0.0629        0.0564  0.0040\n",
      "    144        0.0629        0.0564  0.0050\n",
      "    145        0.0628        0.0563  0.0050\n",
      "    146        0.0628        0.0563  0.0050\n",
      "    147        0.0627        0.0563  0.0040\n",
      "    148        0.0626        0.0563  0.0050\n",
      "    149        0.0625        0.0563  0.0040\n",
      "    150        0.0624        0.0563  0.0050\n",
      "    151        0.0623        0.0563  0.0040\n",
      "    152        0.0622        0.0563  0.0050\n",
      "    153        0.0621        0.0563  0.0040\n",
      "    154        0.0620        0.0563  0.0040\n",
      "    155        0.0618        \u001b[32m0.0563\u001b[0m  0.0039\n",
      "    156        0.0617        \u001b[32m0.0563\u001b[0m  0.0050\n",
      "    157        0.0615        \u001b[32m0.0563\u001b[0m  0.0050\n",
      "    158        0.0613        \u001b[32m0.0563\u001b[0m  0.0040\n",
      "    159        0.0612        0.0563  0.0050\n",
      "    160        0.0610        0.0563  0.0040\n",
      "    161        0.0608        0.0563  0.0040\n",
      "    162        0.0606        0.0563  0.0040\n",
      "    163        0.0604        0.0563  0.0050\n",
      "    164        0.0603        0.0563  0.0039\n",
      "    165        0.0601        0.0563  0.0050\n",
      "    166        0.0599        0.0564  0.0049\n",
      "    167        0.0597        0.0564  0.0050\n",
      "    168        0.0595        0.0564  0.0040\n",
      "    169        0.0592        0.0564  0.0040\n",
      "    170        0.0590        0.0565  0.0050\n",
      "    171        0.0588        0.0565  0.0040\n",
      "    172        0.0586        0.0566  0.0040\n",
      "    173        0.0584        0.0566  0.0040\n",
      "    174        0.0582        0.0566  0.0040\n",
      "    175        0.0580        0.0567  0.0049\n",
      "    176        0.0578        0.0568  0.0040\n",
      "    177        0.0576        0.0568  0.0040\n",
      "    178        0.0574        0.0569  0.0040\n",
      "    179        0.0572        0.0570  0.0050\n",
      "    180        0.0570        0.0571  0.0050\n",
      "    181        0.0568        0.0571  0.0040\n",
      "    182        0.0566        0.0572  0.0050\n",
      "    183        0.0564        0.0573  0.0040\n",
      "    184        0.0563        0.0574  0.0040\n",
      "    185        0.0561        0.0575  0.0050\n",
      "    186        0.0559        0.0576  0.0052\n",
      "    187        0.0557        0.0578  0.0039\n",
      "    188        0.0556        0.0579  0.0040\n",
      "    189        0.0554        0.0580  0.0040\n",
      "    190        0.0553        0.0581  0.0040\n",
      "    191        0.0551        0.0583  0.0049\n",
      "    192        0.0550        0.0584  0.0040\n",
      "    193        0.0548        0.0585  0.0040\n",
      "    194        0.0547        0.0587  0.0040\n",
      "    195        0.0545        0.0588  0.0050\n",
      "    196        0.0544        0.0590  0.0039\n",
      "    197        0.0543        0.0591  0.0040\n",
      "    198        0.0542        0.0593  0.0050\n",
      "    199        0.0541        0.0595  0.0040\n",
      "    200        0.0540        0.0596  0.0040\n",
      "    201        0.0539        0.0598  0.0040\n",
      "    202        0.0538        0.0600  0.0040\n",
      "    203        0.0537        0.0602  0.0050\n",
      "    204        0.0536        0.0604  0.0050\n",
      "    205        0.0535        0.0605  0.0040\n",
      "    206        0.0534        0.0607  0.0049\n",
      "    207        0.0533        0.0609  0.0040\n",
      "    208        0.0533        0.0611  0.0040\n",
      "    209        0.0532        0.0613  0.0050\n",
      "    210        0.0532        0.0615  0.0040\n",
      "    211        0.0531        0.0617  0.0050\n",
      "    212        0.0530        0.0619  0.0040\n",
      "    213        0.0530        0.0621  0.0039\n",
      "    214        0.0530        0.0623  0.0049\n",
      "    215        0.0529        0.0625  0.0039\n",
      "    216        0.0529        0.0627  0.0050\n",
      "    217        0.0528        0.0629  0.0040\n",
      "    218        0.0528        0.0630  0.0039\n",
      "    219        0.0528        0.0632  0.0039\n",
      "    220        0.0528        0.0634  0.0039\n",
      "    221        0.0528        0.0636  0.0050\n",
      "    222        \u001b[36m0.0527\u001b[0m        0.0638  0.0040\n",
      "    223        \u001b[36m0.0527\u001b[0m        0.0640  0.0040\n",
      "    224        \u001b[36m0.0527\u001b[0m        0.0642  0.0050\n",
      "    225        \u001b[36m0.0527\u001b[0m        0.0644  0.0039\n",
      "    226        \u001b[36m0.0527\u001b[0m        0.0646  0.0050\n",
      "    227        \u001b[36m0.0527\u001b[0m        0.0648  0.0050\n",
      "    228        \u001b[36m0.0527\u001b[0m        0.0650  0.0050\n",
      "    229        0.0527        0.0652  0.0039\n",
      "    230        0.0527        0.0653  0.0040\n",
      "    231        0.0527        0.0655  0.0039\n",
      "    232        0.0527        0.0657  0.0040\n",
      "    233        0.0527        0.0659  0.0040\n",
      "    234        0.0527        0.0660  0.0040\n",
      "    235        0.0527        0.0662  0.0050\n",
      "    236        0.0527        0.0664  0.0040\n",
      "    237        0.0528        0.0665  0.0039\n",
      "    238        0.0528        0.0667  0.0040\n",
      "    239        0.0528        0.0668  0.0040\n",
      "    240        0.0528        0.0670  0.0039\n",
      "    241        0.0528        0.0671  0.0040\n",
      "    242        0.0528        0.0673  0.0040\n",
      "    243        0.0529        0.0674  0.0040\n",
      "    244        0.0529        0.0676  0.0040\n",
      "    245        0.0529        0.0677  0.0040\n",
      "    246        0.0529        0.0678  0.0040\n",
      "    247        0.0529        0.0680  0.0040\n",
      "    248        0.0529        0.0681  0.0050\n",
      "    249        0.0530        0.0682  0.0040\n",
      "    250        0.0530        0.0683  0.0039\n",
      "    251        0.0530        0.0684  0.0050\n",
      "    252        0.0530        0.0685  0.0040\n",
      "    253        0.0530        0.0686  0.0040\n",
      "    254        0.0530        0.0687  0.0040\n",
      "    255        0.0531        0.0688  0.0040\n",
      "    256        0.0531        0.0689  0.0040\n",
      "    257        0.0531        0.0690  0.0049\n",
      "    258        0.0531        0.0690  0.0040\n",
      "    259        0.0531        0.0691  0.0050\n",
      "    260        0.0531        0.0692  0.0040\n",
      "    261        0.0531        0.0693  0.0050\n",
      "    262        0.0532        0.0693  0.0040\n",
      "    263        0.0532        0.0694  0.0040\n",
      "    264        0.0532        0.0694  0.0040\n",
      "    265        0.0532        0.0695  0.0040\n",
      "    266        0.0532        0.0695  0.0040\n",
      "    267        0.0532        0.0695  0.0040\n",
      "    268        0.0532        0.0696  0.0050\n",
      "    269        0.0532        0.0696  0.0040\n",
      "    270        0.0532        0.0696  0.0039\n",
      "    271        0.0532        0.0697  0.0040\n",
      "    272        0.0532        0.0697  0.0040\n",
      "    273        0.0532        0.0697  0.0050\n",
      "    274        0.0532        0.0697  0.0040\n",
      "    275        0.0532        0.0697  0.0039\n",
      "    276        0.0532        0.0697  0.0040\n",
      "    277        0.0532        0.0697  0.0040\n",
      "    278        0.0532        0.0697  0.0050\n",
      "    279        0.0532        0.0697  0.0040\n",
      "    280        0.0532        0.0697  0.0050\n",
      "    281        0.0532        0.0697  0.0040\n",
      "    282        0.0532        0.0697  0.0040\n",
      "    283        0.0532        0.0696  0.0050\n",
      "    284        0.0532        0.0696  0.0040\n",
      "    285        0.0532        0.0696  0.0040\n",
      "    286        0.0532        0.0696  0.0040\n",
      "    287        0.0532        0.0695  0.0040\n",
      "    288        0.0532        0.0695  0.0050\n",
      "    289        0.0532        0.0695  0.0050\n",
      "    290        0.0532        0.0694  0.0050\n",
      "    291        0.0532        0.0694  0.0040\n",
      "    292        0.0532        0.0693  0.0050\n",
      "    293        0.0532        0.0693  0.0059\n",
      "    294        0.0532        0.0692  0.0049\n",
      "    295        0.0531        0.0692  0.0040\n",
      "    296        0.0531        0.0691  0.0050\n",
      "    297        0.0531        0.0691  0.0050\n",
      "    298        0.0531        0.0690  0.0050\n",
      "    299        0.0531        0.0690  0.0050\n",
      "    300        0.0531        0.0689  0.0039\n",
      "    301        0.0531        0.0689  0.0050\n",
      "    302        0.0531        0.0688  0.0040\n",
      "    303        0.0531        0.0687  0.0040\n",
      "    304        0.0530        0.0687  0.0040\n",
      "    305        0.0530        0.0686  0.0039\n",
      "    306        0.0530        0.0685  0.0050\n",
      "    307        0.0530        0.0685  0.0040\n",
      "    308        0.0530        0.0684  0.0040\n",
      "    309        0.0530        0.0683  0.0049\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "    310        0.0530        0.0682  0.0050\n",
      "    311        0.0530        0.0682  0.0040\n",
      "    312        0.0530        0.0681  0.0050\n",
      "    313        0.0529        0.0680  0.0040\n",
      "    314        0.0529        0.0679  0.0039\n",
      "    315        0.0529        0.0679  0.0040\n",
      "    316        0.0529        0.0678  0.0040\n",
      "    317        0.0529        0.0677  0.0050\n",
      "    318        0.0529        0.0676  0.0050\n",
      "    319        0.0529        0.0676  0.0039\n",
      "    320        0.0529        0.0675  0.0040\n",
      "    321        0.0529        0.0674  0.0050\n",
      "    322        0.0528        0.0673  0.0040\n",
      "    323        0.0528        0.0672  0.0039\n",
      "    324        0.0528        0.0672  0.0040\n",
      "    325        0.0528        0.0671  0.0050\n",
      "    326        0.0528        0.0670  0.0040\n",
      "    327        0.0528        0.0669  0.0039\n",
      "    328        0.0528        0.0669  0.0040\n",
      "    329        0.0528        0.0668  0.0050\n",
      "    330        0.0528        0.0667  0.0040\n",
      "    331        0.0528        0.0666  0.0050\n",
      "    332        0.0528        0.0666  0.0040\n",
      "    333        0.0528        0.0665  0.0040\n",
      "    334        0.0527        0.0664  0.0039\n",
      "    335        0.0527        0.0663  0.0039\n",
      "    336        0.0527        0.0663  0.0039\n",
      "    337        0.0527        0.0662  0.0039\n",
      "    338        0.0527        0.0661  0.0039\n",
      "    339        0.0527        0.0660  0.0040\n",
      "    340        0.0527        0.0660  0.0040\n",
      "    341        0.0527        0.0659  0.0050\n",
      "    342        0.0527        0.0658  0.0040\n",
      "    343        0.0527        0.0658  0.0040\n",
      "    344        \u001b[36m0.0527\u001b[0m        0.0657  0.0040\n",
      "    345        \u001b[36m0.0527\u001b[0m        0.0656  0.0050\n",
      "    346        \u001b[36m0.0527\u001b[0m        0.0656  0.0040\n",
      "    347        \u001b[36m0.0527\u001b[0m        0.0655  0.0040\n",
      "    348        \u001b[36m0.0527\u001b[0m        0.0654  0.0039\n",
      "    349        \u001b[36m0.0527\u001b[0m        0.0654  0.0040\n",
      "    350        \u001b[36m0.0527\u001b[0m        0.0653  0.0039\n",
      "\n",
      "\n",
      "module : <class '__main__.Regression_Module'>\n",
      "criterion : <class 'torch.nn.modules.loss.MSELoss'>\n",
      "optimizer : <class 'torch.optim.sgd.SGD'>\n",
      "lr : 0.01\n",
      "max_epochs : 350\n",
      "batch_size : 128\n",
      "iterator_train : <class 'torch.utils.data.dataloader.DataLoader'>\n",
      "iterator_valid : <class 'torch.utils.data.dataloader.DataLoader'>\n",
      "dataset : <class 'skorch.dataset.Dataset'>\n",
      "train_split : <skorch.dataset.CVSplit object at 0x000001F9CB8D39E8>\n",
      "callbacks : None\n",
      "predict_nonlinearity : auto\n",
      "warm_start : False\n",
      "verbose : 1\n",
      "device : cpu\n",
      "optimizer__lr : 5e-05\n",
      "optimizer__weight_decay : 0.0001\n",
      "optimizer__momentum : 0.99\n",
      "optimizer__dampening : 0.0\n",
      "optimizer__nesterov : False\n",
      "module__num_units : 21\n",
      "module__dropout : 0.0\n",
      "iterator_train__batch_size : 32\n",
      "callbacks__epoch_timer : <skorch.callbacks.logging.EpochTimer object at 0x000001F9CE6BCB70>\n",
      "callbacks__train_loss : <skorch.callbacks.scoring.PassthroughScoring object at 0x000001F9CB7A0470>\n",
      "callbacks__train_loss__name : train_loss\n",
      "callbacks__train_loss__lower_is_better : True\n",
      "callbacks__train_loss__on_train : True\n",
      "callbacks__valid_loss : <skorch.callbacks.scoring.PassthroughScoring object at 0x000001F9C81D89E8>\n",
      "callbacks__valid_loss__name : valid_loss\n",
      "callbacks__valid_loss__lower_is_better : True\n",
      "callbacks__valid_loss__on_train : False\n",
      "callbacks__print_log : <skorch.callbacks.logging.PrintLog object at 0x000001F9C8361D68>\n",
      "callbacks__print_log__keys_ignored : None\n",
      "callbacks__print_log__sink : <built-in function print>\n",
      "callbacks__print_log__tablefmt : simple\n",
      "callbacks__print_log__floatfmt : .4f\n",
      "callbacks__print_log__stralign : right\n",
      "\n",
      "\n",
      "  epoch    train_loss    valid_loss     dur\n",
      "-------  ------------  ------------  ------\n",
      "      1        \u001b[36m0.4236\u001b[0m        \u001b[32m0.3330\u001b[0m  0.0050\n",
      "      2        \u001b[36m0.3803\u001b[0m        \u001b[32m0.2845\u001b[0m  0.0059\n",
      "      3        \u001b[36m0.3284\u001b[0m        \u001b[32m0.2383\u001b[0m  0.0070\n",
      "      4        \u001b[36m0.2786\u001b[0m        \u001b[32m0.1983\u001b[0m  0.0060\n",
      "      5        \u001b[36m0.2354\u001b[0m        \u001b[32m0.1652\u001b[0m  0.0060\n",
      "      6        \u001b[36m0.2015\u001b[0m        \u001b[32m0.1384\u001b[0m  0.0049\n",
      "      7        \u001b[36m0.1718\u001b[0m        \u001b[32m0.1168\u001b[0m  0.0060\n",
      "      8        \u001b[36m0.1489\u001b[0m        \u001b[32m0.0995\u001b[0m  0.0059\n",
      "      9        \u001b[36m0.1292\u001b[0m        \u001b[32m0.0857\u001b[0m  0.0050\n",
      "     10        \u001b[36m0.1136\u001b[0m        \u001b[32m0.0747\u001b[0m  0.0060\n",
      "     11        \u001b[36m0.1022\u001b[0m        \u001b[32m0.0660\u001b[0m  0.0060\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\users\\being_aerys\\appdata\\local\\programs\\python\\python36\\lib\\site-packages\\torch\\serialization.py:360: UserWarning: Couldn't retrieve source code for container of type Regression_Module. It won't be checked for correctness upon loading.\n",
      "  \"type \" + obj.__name__ + \". It won't be checked \"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "     12        \u001b[36m0.0920\u001b[0m        \u001b[32m0.0591\u001b[0m  0.0060\n",
      "     13        \u001b[36m0.0815\u001b[0m        \u001b[32m0.0538\u001b[0m  0.0060\n",
      "     14        \u001b[36m0.0770\u001b[0m        \u001b[32m0.0496\u001b[0m  0.0050\n",
      "     15        \u001b[36m0.0703\u001b[0m        \u001b[32m0.0464\u001b[0m  0.0069\n",
      "     16        \u001b[36m0.0675\u001b[0m        \u001b[32m0.0440\u001b[0m  0.0069\n",
      "     17        \u001b[36m0.0605\u001b[0m        \u001b[32m0.0421\u001b[0m  0.0060\n",
      "     18        \u001b[36m0.0604\u001b[0m        \u001b[32m0.0407\u001b[0m  0.0070\n",
      "     19        \u001b[36m0.0574\u001b[0m        \u001b[32m0.0396\u001b[0m  0.0060\n",
      "     20        \u001b[36m0.0551\u001b[0m        \u001b[32m0.0389\u001b[0m  0.0060\n",
      "     21        \u001b[36m0.0544\u001b[0m        \u001b[32m0.0383\u001b[0m  0.0050\n",
      "     22        \u001b[36m0.0506\u001b[0m        \u001b[32m0.0379\u001b[0m  0.0060\n",
      "     23        0.0523        \u001b[32m0.0377\u001b[0m  0.0060\n",
      "     24        0.0543        \u001b[32m0.0376\u001b[0m  0.0060\n",
      "     25        \u001b[36m0.0502\u001b[0m        \u001b[32m0.0375\u001b[0m  0.0060\n",
      "     26        \u001b[36m0.0499\u001b[0m        \u001b[32m0.0375\u001b[0m  0.0050\n",
      "     27        0.0503        0.0375  0.0059\n",
      "     28        \u001b[36m0.0492\u001b[0m        0.0376  0.0050\n",
      "     29        \u001b[36m0.0492\u001b[0m        0.0377  0.0060\n",
      "     30        \u001b[36m0.0485\u001b[0m        0.0378  0.0060\n",
      "     31        \u001b[36m0.0479\u001b[0m        0.0379  0.0070\n",
      "     32        \u001b[36m0.0473\u001b[0m        0.0380  0.0060\n",
      "     33        \u001b[36m0.0469\u001b[0m        0.0381  0.0060\n",
      "     34        0.0472        0.0382  0.0050\n",
      "     35        0.0483        0.0383  0.0060\n",
      "     36        0.0479        0.0385  0.0060\n",
      "     37        \u001b[36m0.0466\u001b[0m        0.0386  0.0060\n",
      "     38        0.0493        0.0386  0.0060\n",
      "     39        0.0485        0.0387  0.0060\n",
      "     40        0.0495        0.0387  0.0060\n",
      "     41        0.0514        0.0388  0.0050\n",
      "     42        0.0481        0.0388  0.0050\n",
      "     43        0.0503        0.0389  0.0050\n",
      "     44        \u001b[36m0.0462\u001b[0m        0.0389  0.0059\n",
      "     45        0.0473        0.0390  0.0050\n",
      "     46        0.0493        0.0390  0.0060\n",
      "     47        0.0494        0.0390  0.0050\n",
      "     48        0.0477        0.0390  0.0060\n",
      "     49        \u001b[36m0.0458\u001b[0m        0.0391  0.0060\n",
      "     50        0.0493        0.0391  0.0060\n",
      "     51        0.0480        0.0391  0.0060\n",
      "     52        \u001b[36m0.0449\u001b[0m        0.0392  0.0060\n",
      "     53        0.0472        0.0392  0.0050\n",
      "     54        0.0480        0.0392  0.0060\n",
      "     55        0.0465        0.0393  0.0050\n",
      "     56        0.0466        0.0393  0.0060\n",
      "     57        0.0481        0.0393  0.0050\n",
      "     58        0.0476        0.0393  0.0060\n",
      "     59        0.0474        0.0393  0.0060\n",
      "     60        0.0482        0.0393  0.0060\n",
      "     61        0.0460        0.0393  0.0060\n",
      "     62        0.0457        0.0394  0.0060\n",
      "     63        0.0462        0.0394  0.0049\n",
      "     64        0.0486        0.0394  0.0060\n",
      "     65        0.0475        0.0394  0.0060\n",
      "     66        0.0463        0.0394  0.0060\n",
      "     67        0.0472        0.0394  0.0060\n",
      "     68        0.0463        0.0394  0.0060\n",
      "     69        0.0477        0.0394  0.0060\n",
      "     70        0.0476        0.0394  0.0060\n",
      "     71        0.0462        0.0394  0.0060\n",
      "     72        0.0458        0.0395  0.0060\n",
      "     73        0.0479        0.0395  0.0060\n",
      "     74        0.0465        0.0395  0.0050\n",
      "     75        0.0489        0.0395  0.0060\n",
      "     76        0.0497        0.0395  0.0060\n",
      "     77        0.0486        0.0395  0.0059\n",
      "     78        0.0465        0.0395  0.0050\n",
      "     79        0.0461        0.0395  0.0070\n",
      "     80        0.0467        0.0395  0.0060\n",
      "     81        0.0474        0.0395  0.0070\n",
      "     82        0.0452        0.0395  0.0060\n",
      "     83        0.0461        0.0395  0.0060\n",
      "     84        0.0473        0.0395  0.0049\n",
      "     85        0.0481        0.0395  0.0060\n",
      "     86        0.0465        0.0395  0.0060\n",
      "     87        0.0475        0.0395  0.0060\n",
      "     88        0.0455        0.0395  0.0060\n",
      "     89        0.0470        0.0395  0.0060\n",
      "     90        0.0467        0.0395  0.0060\n",
      "     91        0.0449        0.0395  0.0060\n",
      "     92        0.0466        0.0395  0.0059\n",
      "     93        0.0489        0.0396  0.0060\n",
      "     94        0.0465        0.0396  0.0070\n",
      "     95        0.0471        0.0396  0.0060\n",
      "     96        0.0484        0.0396  0.0060\n",
      "     97        0.0460        0.0396  0.0060\n",
      "     98        0.0496        0.0396  0.0059\n",
      "     99        0.0460        0.0396  0.0060\n",
      "    100        0.0474        0.0396  0.0050\n",
      "  epoch    train_loss    valid_loss     dur\n",
      "-------  ------------  ------------  ------\n",
      "      1        \u001b[36m0.0971\u001b[0m        \u001b[32m0.1428\u001b[0m  0.0050\n",
      "      2        \u001b[36m0.0920\u001b[0m        \u001b[32m0.1330\u001b[0m  0.0050\n",
      "      3        \u001b[36m0.0858\u001b[0m        \u001b[32m0.1233\u001b[0m  0.0060\n",
      "      4        \u001b[36m0.0789\u001b[0m        \u001b[32m0.1145\u001b[0m  0.0050\n",
      "      5        \u001b[36m0.0741\u001b[0m        \u001b[32m0.1069\u001b[0m  0.0060\n",
      "      6        \u001b[36m0.0702\u001b[0m        \u001b[32m0.1005\u001b[0m  0.0050\n",
      "      7        \u001b[36m0.0649\u001b[0m        \u001b[32m0.0951\u001b[0m  0.0070\n",
      "      8        \u001b[36m0.0599\u001b[0m        \u001b[32m0.0906\u001b[0m  0.0060\n",
      "      9        0.0616        \u001b[32m0.0868\u001b[0m  0.0060\n",
      "     10        0.0602        \u001b[32m0.0837\u001b[0m  0.0060\n",
      "     11        \u001b[36m0.0581\u001b[0m        \u001b[32m0.0810\u001b[0m  0.0070\n",
      "     12        \u001b[36m0.0570\u001b[0m        \u001b[32m0.0786\u001b[0m  0.0079\n",
      "     13        \u001b[36m0.0570\u001b[0m        \u001b[32m0.0766\u001b[0m  0.0060\n",
      "     14        \u001b[36m0.0543\u001b[0m        \u001b[32m0.0749\u001b[0m  0.0070\n",
      "     15        0.0544        \u001b[32m0.0735\u001b[0m  0.0060\n",
      "     16        \u001b[36m0.0523\u001b[0m        \u001b[32m0.0723\u001b[0m  0.0060\n",
      "     17        0.0543        \u001b[32m0.0712\u001b[0m  0.0049\n",
      "     18        0.0560        \u001b[32m0.0704\u001b[0m  0.0060\n",
      "     19        0.0549        \u001b[32m0.0697\u001b[0m  0.0050\n",
      "     20        0.0541        \u001b[32m0.0691\u001b[0m  0.0049\n",
      "     21        0.0528        \u001b[32m0.0685\u001b[0m  0.0060\n",
      "     22        0.0523        \u001b[32m0.0680\u001b[0m  0.0060\n",
      "     23        0.0534        \u001b[32m0.0676\u001b[0m  0.0060\n",
      "     24        0.0531        \u001b[32m0.0671\u001b[0m  0.0060\n",
      "     25        0.0527        \u001b[32m0.0668\u001b[0m  0.0059\n",
      "     26        0.0551        \u001b[32m0.0664\u001b[0m  0.0060\n",
      "     27        0.0557        \u001b[32m0.0662\u001b[0m  0.0049\n",
      "     28        0.0545        \u001b[32m0.0660\u001b[0m  0.0060\n",
      "     29        0.0539        \u001b[32m0.0659\u001b[0m  0.0050\n",
      "     30        \u001b[36m0.0501\u001b[0m        \u001b[32m0.0657\u001b[0m  0.0089\n",
      "     31        0.0543        \u001b[32m0.0656\u001b[0m  0.0060\n",
      "     32        0.0534        \u001b[32m0.0655\u001b[0m  0.0060\n",
      "     33        0.0518        \u001b[32m0.0654\u001b[0m  0.0050\n",
      "     34        \u001b[36m0.0500\u001b[0m        \u001b[32m0.0654\u001b[0m  0.0070\n",
      "     35        0.0522        \u001b[32m0.0653\u001b[0m  0.0060\n",
      "     36        0.0528        \u001b[32m0.0652\u001b[0m  0.0060\n",
      "     37        0.0529        \u001b[32m0.0651\u001b[0m  0.0059\n",
      "     38        0.0545        \u001b[32m0.0650\u001b[0m  0.0059\n",
      "     39        0.0523        \u001b[32m0.0648\u001b[0m  0.0060\n",
      "     40        0.0534        \u001b[32m0.0647\u001b[0m  0.0060\n",
      "     41        0.0527        \u001b[32m0.0647\u001b[0m  0.0060\n",
      "     42        0.0556        0.0647  0.0060\n",
      "     43        0.0525        \u001b[32m0.0647\u001b[0m  0.0050\n",
      "     44        0.0517        \u001b[32m0.0646\u001b[0m  0.0069\n",
      "     45        \u001b[36m0.0498\u001b[0m        \u001b[32m0.0646\u001b[0m  0.0060\n",
      "     46        0.0515        \u001b[32m0.0646\u001b[0m  0.0060\n",
      "     47        0.0514        \u001b[32m0.0645\u001b[0m  0.0060\n",
      "     48        0.0515        \u001b[32m0.0645\u001b[0m  0.0050\n",
      "     49        0.0536        \u001b[32m0.0645\u001b[0m  0.0060\n",
      "     50        0.0535        \u001b[32m0.0645\u001b[0m  0.0060\n",
      "     51        0.0541        \u001b[32m0.0645\u001b[0m  0.0060\n",
      "     52        0.0541        0.0645  0.0060\n",
      "     53        0.0533        0.0645  0.0060\n",
      "     54        0.0530        0.0645  0.0060\n",
      "     55        0.0514        0.0645  0.0050\n",
      "     56        0.0526        \u001b[32m0.0644\u001b[0m  0.0060\n",
      "     57        0.0535        0.0645  0.0060\n",
      "     58        0.0559        0.0645  0.0060\n",
      "     59        0.0511        0.0645  0.0050\n",
      "     60        0.0512        0.0645  0.0059\n",
      "     61        0.0534        0.0645  0.0050\n",
      "     62        0.0526        0.0645  0.0060\n",
      "     63        0.0527        \u001b[32m0.0644\u001b[0m  0.0050\n",
      "     64        0.0526        0.0644  0.0059\n",
      "     65        0.0523        \u001b[32m0.0644\u001b[0m  0.0050\n",
      "     66        0.0513        \u001b[32m0.0643\u001b[0m  0.0060\n",
      "     67        0.0549        0.0644  0.0049\n",
      "     68        0.0537        0.0644  0.0060\n",
      "     69        0.0530        0.0644  0.0060\n",
      "     70        0.0539        0.0644  0.0060\n",
      "     71        0.0513        0.0645  0.0060\n",
      "     72        0.0514        0.0645  0.0050\n",
      "     73        0.0527        0.0645  0.0060\n",
      "     74        0.0517        0.0645  0.0060\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "     75        0.0512        0.0645  0.0060\n",
      "     76        0.0522        0.0645  0.0060\n",
      "     77        0.0534        0.0645  0.0060\n",
      "     78        0.0529        0.0645  0.0060\n",
      "     79        0.0538        0.0645  0.0060\n",
      "     80        0.0533        0.0645  0.0060\n",
      "     81        0.0527        0.0645  0.0059\n",
      "     82        0.0530        0.0645  0.0049\n",
      "     83        0.0532        0.0645  0.0060\n",
      "     84        0.0535        0.0645  0.0049\n",
      "     85        0.0525        0.0644  0.0060\n",
      "     86        0.0526        0.0644  0.0049\n",
      "     87        0.0523        0.0644  0.0060\n",
      "     88        0.0522        0.0644  0.0060\n",
      "     89        0.0536        0.0644  0.0060\n",
      "     90        0.0532        0.0644  0.0060\n",
      "     91        0.0529        0.0645  0.0050\n",
      "     92        0.0532        0.0645  0.0060\n",
      "     93        0.0523        0.0645  0.0049\n",
      "     94        0.0527        0.0645  0.0069\n",
      "     95        0.0508        0.0645  0.0060\n",
      "     96        0.0544        0.0645  0.0050\n",
      "     97        0.0553        0.0646  0.0059\n",
      "     98        0.0536        0.0646  0.0050\n",
      "     99        0.0522        0.0645  0.0060\n",
      "    100        0.0519        0.0645  0.0050\n",
      "\n",
      "\n",
      "module : <class '__main__.Regression_Module'>\n",
      "criterion : <class 'torch.nn.modules.loss.MSELoss'>\n",
      "optimizer : <class 'torch.optim.sgd.SGD'>\n",
      "lr : 0.01\n",
      "max_epochs : 100\n",
      "batch_size : 128\n",
      "iterator_train : <class 'torch.utils.data.dataloader.DataLoader'>\n",
      "iterator_valid : <class 'torch.utils.data.dataloader.DataLoader'>\n",
      "dataset : <class 'skorch.dataset.Dataset'>\n",
      "train_split : <skorch.dataset.CVSplit object at 0x000001F9CB89CE80>\n",
      "callbacks : None\n",
      "predict_nonlinearity : auto\n",
      "warm_start : False\n",
      "verbose : 1\n",
      "device : cpu\n",
      "optimizer__lr : 0.003\n",
      "optimizer__weight_decay : 5e-05\n",
      "optimizer__momentum : 0.75\n",
      "optimizer__dampening : 0.0\n",
      "optimizer__nesterov : True\n",
      "module__num_units : 17\n",
      "module__dropout : 0.5\n",
      "iterator_train__batch_size : 16\n",
      "callbacks__epoch_timer : <skorch.callbacks.logging.EpochTimer object at 0x000001F9CB7AFBA8>\n",
      "callbacks__train_loss : <skorch.callbacks.scoring.PassthroughScoring object at 0x000001F9C843BCF8>\n",
      "callbacks__train_loss__name : train_loss\n",
      "callbacks__train_loss__lower_is_better : True\n",
      "callbacks__train_loss__on_train : True\n",
      "callbacks__valid_loss : <skorch.callbacks.scoring.PassthroughScoring object at 0x000001F9C842EF28>\n",
      "callbacks__valid_loss__name : valid_loss\n",
      "callbacks__valid_loss__lower_is_better : True\n",
      "callbacks__valid_loss__on_train : False\n",
      "callbacks__print_log : <skorch.callbacks.logging.PrintLog object at 0x000001F9CB89C2B0>\n",
      "callbacks__print_log__keys_ignored : None\n",
      "callbacks__print_log__sink : <built-in function print>\n",
      "callbacks__print_log__tablefmt : simple\n",
      "callbacks__print_log__floatfmt : .4f\n",
      "callbacks__print_log__stralign : right\n",
      "\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\users\\being_aerys\\appdata\\local\\programs\\python\\python36\\lib\\site-packages\\torch\\serialization.py:360: UserWarning: Couldn't retrieve source code for container of type Regression_Module. It won't be checked for correctness upon loading.\n",
      "  \"type \" + obj.__name__ + \". It won't be checked \"\n"
     ]
    }
   ],
   "source": [
    "individual_cv_estimators_for_all_splits = []\n",
    "mse_on_each_validation_split = []\n",
    "average_mse_on_validation_list = []\n",
    "\n",
    "total_unique_estimators =  no_of_hyperparam_configs * no_of_cv_folds\n",
    "estimators_counter = 0\n",
    "\n",
    "for idx in range(len(random_hyperparameter_configurations)):\n",
    "    \n",
    "    net = NeuralNetRegressor(       module=Regression_Module,\n",
    "                        \n",
    "                                    optimizer__lr = random_hyperparameter_configurations[idx][0],\n",
    "                                    optimizer__weight_decay = random_hyperparameter_configurations[idx][1],\n",
    "                                    optimizer__momentum = random_hyperparameter_configurations[idx][2],\n",
    "                                    optimizer__dampening = random_hyperparameter_configurations[idx][3],\n",
    "                                    optimizer__nesterov = random_hyperparameter_configurations[idx][4],\n",
    "                                    module__num_units = random_hyperparameter_configurations[idx][5],\n",
    "                                    max_epochs = random_hyperparameter_configurations[idx][6],\n",
    "                                    module__dropout = random_hyperparameter_configurations[idx][7],\n",
    "                                    iterator_train__batch_size = random_hyperparameter_configurations[idx][8],\n",
    "                                    optimizer = random_hyperparameter_configurations[idx][9],\n",
    "\n",
    "                                    )\n",
    "    \n",
    "    scores = cross_validate( estimator = net,\n",
    "                             X = X_training.values,\n",
    "                             y = Y_training.values,\n",
    "                             scoring='neg_mean_squared_error',\n",
    "                             cv = 2,\n",
    "                             return_estimator = True,\n",
    "                             verbose = 0)\n",
    "    \n",
    "    \n",
    "    # Store each split's estimator from cross-validation\n",
    "    individual_cv_estimators_for_all_splits.append(scores[\"estimator\"])\n",
    "    \n",
    "    dict_of_params = scores[\"estimator\"][0].get_params()\n",
    "    \n",
    "    \n",
    "    print(\"\\n\")\n",
    "    for param in dict_of_params:\n",
    "        \n",
    "        print(param + \" : \" + str(dict_of_params[param]))\n",
    "    print(\"\\n\")\n",
    "    \n",
    "    \n",
    "    \n",
    "    \n",
    "    # Save uncertainty among different folds for the same estimator\n",
    "    mse_on_each_validation_split.append(scores[\"test_score\"])\n",
    "    \n",
    "    # Salve also the average validatoin set MSE for this hyperparam config\n",
    "    avg_mse = (-1 * scores[\"test_score\"].mean()) # using -1 because the scoring is the negative of mse\n",
    "    average_mse_on_validation_list.append(avg_mse)\n",
    "    \n",
    "    \n",
    "    \n",
    "    # Save the current estimator parameters\n",
    "    for idx in range(len(scores[\"estimator\"])):\n",
    "        \n",
    "        dump(scores[\"estimator\"][idx],\"../Saved_Models/NN_model_hyperparam_config_\"+ str(estimators_counter)+\".joblib\")\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Plot variance of average mse on validation data for each hyperparameter configuration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "mean_of_average_mse_on_validation_list = np.mean(average_mse_on_validation_list)\n",
    "\n",
    "std_of_average_mse_on_validation_list = np.std(average_mse_on_validation_list)\n",
    "\n",
    "# Define individual_agents_name_list, positions, bar heights and list_of_standard_deviations_of_costs_of_all_agents bar heights\n",
    "individual_agents_name_list = ['.',\".\",]\n",
    "x_pos = np.arange(len(individual_agents_name_list))\n",
    "list_of_mean_of_average_mse_on_validation_list = [mean_of_average_mse_on_validation_list,mean_of_average_mse_on_validation_list]\n",
    "list_of_std_of_average_mse_on_validation_list = [std_of_average_mse_on_validation_list,std_of_average_mse_on_validation_list]\n",
    "\n",
    "\n",
    "# Build the plot\n",
    "fig, ax = plt.subplots()\n",
    "ax.bar(x_pos, list_of_mean_of_average_mse_on_validation_list,\n",
    "       yerr=list_of_std_of_average_mse_on_validation_list,\n",
    "       align='center',\n",
    "       alpha=0.5,\n",
    "       ecolor='black',\n",
    "       capsize=10,\n",
    "       width = 0.5, # defines the width of each bar, can pass a list of distince bar widths as well\n",
    "        )\n",
    "ax.set_ylabel('Average MSE on Validation Data ')\n",
    "ax.set_xticks(x_pos)\n",
    "ax.set_xticklabels(individual_agents_name_list)\n",
    "ax.set_title('Uncertainty in Validation MSE for different hyperparameter configurations')\n",
    "ax.yaxis.grid(True)\n",
    "\n",
    "# Save the figure and show\n",
    "plt.tight_layout()\n",
    "plt.savefig('../plots/bar_plot_with_error_bars_for_average_mse.png')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train all 350 models on all the training samples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "last_epoch_training_losses = []\n",
    "\n",
    "for idx in range(len(random_hyperparameter_configurations)):\n",
    "    \n",
    "    net = NeuralNetRegressor(       module=Regression_Module,\n",
    "                        \n",
    "                                    optimizer__lr = random_hyperparameter_configurations[idx][0],\n",
    "                                    optimizer__weight_decay = random_hyperparameter_configurations[idx][1],\n",
    "                                    optimizer__momentum = random_hyperparameter_configurations[idx][2],\n",
    "                                    optimizer__dampening = random_hyperparameter_configurations[idx][3],\n",
    "                                    optimizer__nesterov = random_hyperparameter_configurations[idx][4],\n",
    "                                    module__num_units = random_hyperparameter_configurations[idx][5],\n",
    "                                    max_epochs = random_hyperparameter_configurations[idx][6],\n",
    "                                    module__dropout = random_hyperparameter_configurations[idx][7],\n",
    "                                    iterator_train__batch_size = random_hyperparameter_configurations[idx][8],\n",
    "                                    optimizer = random_hyperparameter_configurations[idx][9],\n",
    "\n",
    "                                    )\n",
    "    net.fit(X = X_training.values, y = Y_training.values,scoring='neg_mean_squared_error',)\n",
    "    \n",
    "    net.save_params(f_params=\"../Saved_Models/nn_model_\" + str(idx) + \".pkl\")\n",
    "    # https://skorch.readthedocs.io/en/stable/user/save_load.html\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Plot Uncertainty among different models trained for each test sample"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>% of Cr</th>\n",
       "      <th>% of Hf</th>\n",
       "      <th>% of Mo</th>\n",
       "      <th>% of Nb</th>\n",
       "      <th>% of Ta</th>\n",
       "      <th>% of Ti</th>\n",
       "      <th>% of V</th>\n",
       "      <th>% of Zr</th>\n",
       "      <th>% of Ni</th>\n",
       "      <th>% of Al</th>\n",
       "      <th>% of Mn</th>\n",
       "      <th>%Cu</th>\n",
       "      <th>%C</th>\n",
       "      <th>Entropy</th>\n",
       "      <th>Hardness</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.666667</td>\n",
       "      <td>0.500</td>\n",
       "      <td>0.8</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.8</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.667795</td>\n",
       "      <td>0.296870</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.500</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.250</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.354613</td>\n",
       "      <td>0.462085</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.000</td>\n",
       "      <td>0.666667</td>\n",
       "      <td>0.500</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.333333</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.527139</td>\n",
       "      <td>0.893030</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.833333</td>\n",
       "      <td>0.000</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.833333</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.296870</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.625</td>\n",
       "      <td>0.833333</td>\n",
       "      <td>0.625</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.489351</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   % of Cr  % of Hf  % of Mo   % of Nb  % of Ta  % of Ti    % of V  % of Zr  \\\n",
       "0      0.0      1.0    0.000  0.666667    0.500      0.8  0.000000      0.8   \n",
       "1      0.0      0.0    0.500  1.000000    0.250      0.0  1.000000      0.0   \n",
       "2      0.0      0.0    1.000  0.666667    0.500      0.0  0.333333      0.0   \n",
       "3      0.0      0.0    0.000  0.833333    0.000      1.0  0.833333      1.0   \n",
       "4      0.0      0.0    0.625  0.833333    0.625      0.0  0.000000      0.0   \n",
       "\n",
       "   % of Ni  % of Al  % of Mn  %Cu   %C   Entropy  Hardness  \n",
       "0      0.0      0.0      0.0  0.0  0.0  0.667795  0.296870  \n",
       "1      0.0      0.0      0.0  0.0  0.0  0.354613  0.462085  \n",
       "2      0.0      0.0      0.0  0.0  0.0  0.527139  0.893030  \n",
       "3      0.0      0.0      0.0  0.0  0.0  0.000000  0.296870  \n",
       "4      0.0      0.0      0.0  0.0  0.0  0.000000  0.489351  "
      ]
     },
     "execution_count": 58,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Predict on test data\n",
    "testing_data = pd.read_csv(\"normalized_testing_features_and_targets.csv\", sep = \",\")\n",
    "testing_data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Separate features and targets\n",
    "X_testing = testing_data.iloc[:,0:num_features].values #notice retaining only the underlying numpy arrays\n",
    "Y_testing = pd.DataFrame(testing_data[\"Hardness\"]).values\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Predice for each sample using all hyperparamconfig-trained estimators"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'tuple' object has no attribute 'predict'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-42-917e5b4d7a32>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      8\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      9\u001b[0m     \u001b[1;31m# Predict using that estimator\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 10\u001b[1;33m     \u001b[0mtest_predictions\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mregressor\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpredict\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mX_testing\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mastype\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfloat32\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     11\u001b[0m     \u001b[1;31m# Store the predictions\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     12\u001b[0m     \u001b[0mtest_predictions_by_all_estimators\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtest_predictions\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mAttributeError\u001b[0m: 'tuple' object has no attribute 'predict'"
     ]
    }
   ],
   "source": [
    "test_predictions_by_all_estimators = []\n",
    "\n",
    "for estimator_ctr in range(no_of_hyperparam_configs):\n",
    "    \n",
    "    regressor = NeuralNet(\n",
    "                        module=Regression_Module,\n",
    "                        )\n",
    "    regressor.initialize()  # This is important!\n",
    "    regressor.load_params(f_params=\"../Saved_Models/nn_model_\" + str(estimator_ctr) + \".pkl\")\n",
    "    \n",
    "    # Predict using that estimator\n",
    "    test_predictions = regressor.predict(X_testing.astype(np.float32))\n",
    "    # Store the predictions\n",
    "    test_predictions_by_all_estimators.append(test_predictions)\n",
    "    \n",
    "    \n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ### Plot Uncertainty among different models trained for each test sample"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mean_cost_of_random_agent = np.mean(scores)\n",
    "\n",
    "std_of_costs_of_random_agent = np.std(scores)\n",
    "\n",
    "# Define individual_agents_name_list, positions, bar heights and list_of_standard_deviations_of_costs_of_all_agents bar heights\n",
    "individual_agents_name_list = ['',]\n",
    "x_pos = np.arange(len(individual_agents_name_list))\n",
    "list_of_mean_costs_of_all_agents = [mean_cost_of_random_agent,]\n",
    "list_of_standard_deviations_of_costs_of_all_agents = [std_of_costs_of_random_agent,]\n",
    "\n",
    "# Build the plot\n",
    "fig, ax = plt.subplots()\n",
    "ax.bar(x_pos, list_of_mean_costs_of_all_agents,\n",
    "       yerr=list_of_standard_deviations_of_costs_of_all_agents,\n",
    "       align='center',\n",
    "       alpha=0.5,\n",
    "       ecolor='black',\n",
    "       capsize=10)\n",
    "ax.set_ylabel('Average Cost of Episode')\n",
    "ax.set_xticks(x_pos)\n",
    "ax.set_xticklabels(individual_agents_name_list)\n",
    "ax.set_title('Agent vs Average Cost of Episode')\n",
    "ax.yaxis.grid(True)\n",
    "\n",
    "# Save the figure and show\n",
    "plt.tight_layout()\n",
    "plt.savefig('../Graphs/bar_plot_with_error_bars.png')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Find the best hyperparameter configuration among all the trained estimators"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "estimator_index_for_least_avg_mse = index(min(average_mse_on_validation))\n",
    "print(\"The best hyperparameter configuration index: {}\".format(estimator_index_for_least_avg_mse))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Generate learning curve on the whole trianing data for the estimator trained with the best hyperparameters\n",
    "Can determine the best hyperparam model using the avg validation loss --> note that this best model has already been trained and you only need to use the history attribute to plot the training curve now.<br><br>\n",
    "https://www.kaggle.com/mlpotter/pytorch-and-skorch-for-deep-learning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'history' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-3-1e3b45f00a1a>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[1;31m# get train losses from all epochs, a list of floats\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 2\u001b[1;33m \u001b[0mtraining_loss\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mhistory\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m'train_loss'\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      3\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      4\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      5\u001b[0m \u001b[0mplt\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mplot\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mlen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtraining_loss\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtraining_loss\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m'r'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mlinewidth\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m2\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mNameError\u001b[0m: name 'history' is not defined"
     ]
    }
   ],
   "source": [
    "# get train losses from all epochs, a list of floats\n",
    "\n",
    "use \"history\" attribute of skorch for the best estimator for this\n",
    "training_loss = history[:, 'train_loss']\n",
    "\n",
    "\n",
    "plt.plot(len(training_loss), training_loss, 'r', linewidth=2)\n",
    "plt.rcParams.update({'font.size': 10})\n",
    "plt.tick_params(labelsize=10)\n",
    "plt.xlabel('Epoch No.', fontsize=10)\n",
    "plt.ylabel('MSE Loss', fontsize=10)\n",
    "plt.title('Training MSE Loss Plot', fontsize=12)\n",
    "\n",
    "# Add the legend for all the curves in the order of plot() call to get the corresponding color for each curve's legend\n",
    "plt.legend([\"NN with Chosen Hyperparameters\"], fontsize=7)\n",
    "plt.show()\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# IGNORE ANYTHING BELOW THIS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "common_param_grid = {\n",
    "    \n",
    "    \"lr\" : lr,\n",
    "    \"max_epochs\" : max_epochs,\n",
    "    \n",
    "    \n",
    "    \n",
    "    #list all the formal arguments of the torch module that u pass to skorch regressor here beginning with module__\n",
    "    \"module__num_units\" : no_of_nodes_per_layer,\n",
    "    \"module__dropout\" : dropout_probability_per_node,\n",
    "    #not passing the activation here though\n",
    "    \n",
    "    \n",
    "    \n",
    "    \"optimizer\" : optimizers,\n",
    "    \"optimizer__weight_decay\": weight_decay_for_regularization,\n",
    "    \"optimizer__momentum\" : momentum_vals,\n",
    "    \"optimizer__dampening\" : momentum_dampening,\n",
    "    \"optimizer__nesterov\" : nesterov, \n",
    "    \n",
    "    \n",
    "    \n",
    "    \"iterator_train__batch_size\": minibatch_size,\n",
    "    #\"callbacks__scheduler__epoch\": [10, 50, 100], #learning rate scheduler    \n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Create a Randomized Search Instance and pass the estimator, thehyperparameters' grid, the no of random samples to take from the grid, and the value of k for k-Fold cross-validation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "sgd_randomized_search = RandomizedSearchCV(estimator = skorch_regressor,\n",
    "                                          scoring = \"neg_mean_squared_error\", #negative since the scorer tries to maximize this\n",
    "                                          param_distributions = common_param_grid,\n",
    "                                          n_iter = 1, #350,  #no of random samples to take from the grid                                                    \n",
    "                                          cv = 2, #20,\n",
    "                                          return_train_score = True # produce metrics on the train set as well\n",
    "                                          )\n",
    "\n",
    "#skorch_regressor.get_params().keys()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Train the models."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  epoch    train_loss    valid_loss     dur\n",
      "-------  ------------  ------------  ------\n",
      "      1        \u001b[36m0.5185\u001b[0m        \u001b[32m0.4200\u001b[0m  0.2057\n",
      "      2        \u001b[36m0.4923\u001b[0m        \u001b[32m0.3896\u001b[0m  0.0080\n",
      "      3        \u001b[36m0.4594\u001b[0m        \u001b[32m0.3560\u001b[0m  0.0080\n",
      "      4        \u001b[36m0.4230\u001b[0m        \u001b[32m0.3215\u001b[0m  0.0080\n",
      "      5        \u001b[36m0.3854\u001b[0m        \u001b[32m0.2876\u001b[0m  0.0080\n",
      "      6        \u001b[36m0.3483\u001b[0m        \u001b[32m0.2554\u001b[0m  0.0079\n",
      "      7        \u001b[36m0.3128\u001b[0m        \u001b[32m0.2255\u001b[0m  0.0080\n",
      "      8        \u001b[36m0.2797\u001b[0m        \u001b[32m0.1982\u001b[0m  0.0109\n",
      "      9        \u001b[36m0.2492\u001b[0m        \u001b[32m0.1737\u001b[0m  0.0090\n",
      "     10        \u001b[36m0.2217\u001b[0m        \u001b[32m0.1521\u001b[0m  0.0100\n",
      "     11        \u001b[36m0.1970\u001b[0m        \u001b[32m0.1331\u001b[0m  0.0089\n",
      "     12        \u001b[36m0.1752\u001b[0m        \u001b[32m0.1167\u001b[0m  0.0100\n",
      "     13        \u001b[36m0.1560\u001b[0m        \u001b[32m0.1026\u001b[0m  0.0080\n",
      "     14        \u001b[36m0.1393\u001b[0m        \u001b[32m0.0905\u001b[0m  0.0090\n",
      "     15        \u001b[36m0.1249\u001b[0m        \u001b[32m0.0802\u001b[0m  0.0090\n",
      "     16        \u001b[36m0.1123\u001b[0m        \u001b[32m0.0716\u001b[0m  0.0090\n",
      "     17        \u001b[36m0.1016\u001b[0m        \u001b[32m0.0645\u001b[0m  0.0090\n",
      "     18        \u001b[36m0.0925\u001b[0m        \u001b[32m0.0586\u001b[0m  0.0080\n",
      "     19        \u001b[36m0.0848\u001b[0m        \u001b[32m0.0539\u001b[0m  0.0079\n",
      "     20        \u001b[36m0.0783\u001b[0m        \u001b[32m0.0500\u001b[0m  0.0080\n",
      "     21        \u001b[36m0.0729\u001b[0m        \u001b[32m0.0469\u001b[0m  0.0110\n",
      "     22        \u001b[36m0.0684\u001b[0m        \u001b[32m0.0444\u001b[0m  0.0090\n",
      "     23        \u001b[36m0.0646\u001b[0m        \u001b[32m0.0425\u001b[0m  0.0080\n",
      "     24        \u001b[36m0.0615\u001b[0m        \u001b[32m0.0410\u001b[0m  0.0100\n",
      "     25        \u001b[36m0.0589\u001b[0m        \u001b[32m0.0399\u001b[0m  0.0090\n",
      "     26        \u001b[36m0.0568\u001b[0m        \u001b[32m0.0390\u001b[0m  0.0100\n",
      "     27        \u001b[36m0.0551\u001b[0m        \u001b[32m0.0384\u001b[0m  0.0110\n",
      "     28        \u001b[36m0.0537\u001b[0m        \u001b[32m0.0379\u001b[0m  0.0109\n",
      "     29        \u001b[36m0.0525\u001b[0m        \u001b[32m0.0376\u001b[0m  0.0089\n",
      "     30        \u001b[36m0.0515\u001b[0m        \u001b[32m0.0373\u001b[0m  0.0090\n",
      "     31        \u001b[36m0.0507\u001b[0m        \u001b[32m0.0372\u001b[0m  0.0109\n",
      "     32        \u001b[36m0.0501\u001b[0m        \u001b[32m0.0371\u001b[0m  0.0120\n",
      "     33        \u001b[36m0.0496\u001b[0m        \u001b[32m0.0371\u001b[0m  0.0100\n",
      "     34        \u001b[36m0.0491\u001b[0m        \u001b[32m0.0371\u001b[0m  0.0110\n",
      "     35        \u001b[36m0.0488\u001b[0m        0.0371  0.0090\n",
      "     36        \u001b[36m0.0485\u001b[0m        0.0371  0.0090\n",
      "     37        \u001b[36m0.0483\u001b[0m        0.0372  0.0080\n",
      "     38        \u001b[36m0.0481\u001b[0m        0.0372  0.0089\n",
      "     39        \u001b[36m0.0479\u001b[0m        0.0373  0.0090\n",
      "     40        \u001b[36m0.0478\u001b[0m        0.0373  0.0090\n",
      "     41        \u001b[36m0.0477\u001b[0m        0.0374  0.0080\n",
      "     42        \u001b[36m0.0476\u001b[0m        0.0374  0.0110\n",
      "     43        \u001b[36m0.0475\u001b[0m        0.0375  0.0090\n",
      "     44        \u001b[36m0.0474\u001b[0m        0.0375  0.0090\n",
      "     45        \u001b[36m0.0474\u001b[0m        0.0376  0.0130\n",
      "     46        \u001b[36m0.0473\u001b[0m        0.0376  0.0100\n",
      "     47        \u001b[36m0.0473\u001b[0m        0.0376  0.0080\n",
      "     48        \u001b[36m0.0473\u001b[0m        0.0376  0.0100\n",
      "     49        \u001b[36m0.0472\u001b[0m        0.0377  0.0100\n",
      "     50        \u001b[36m0.0472\u001b[0m        0.0377  0.0090\n",
      "     51        \u001b[36m0.0472\u001b[0m        0.0377  0.0110\n",
      "     52        \u001b[36m0.0472\u001b[0m        0.0377  0.0110\n",
      "     53        \u001b[36m0.0472\u001b[0m        0.0378  0.0090\n",
      "     54        \u001b[36m0.0472\u001b[0m        0.0378  0.0080\n",
      "     55        \u001b[36m0.0472\u001b[0m        0.0378  0.0100\n",
      "     56        \u001b[36m0.0472\u001b[0m        0.0378  0.0110\n",
      "     57        \u001b[36m0.0472\u001b[0m        0.0378  0.0110\n",
      "     58        \u001b[36m0.0471\u001b[0m        0.0378  0.0100\n",
      "     59        \u001b[36m0.0471\u001b[0m        0.0378  0.0100\n",
      "     60        \u001b[36m0.0471\u001b[0m        0.0378  0.0100\n",
      "     61        0.0471        0.0378  0.0090\n",
      "     62        0.0471        0.0378  0.0090\n",
      "     63        0.0471        0.0379  0.0080\n",
      "     64        0.0472        0.0379  0.0090\n",
      "     65        0.0472        0.0379  0.0090\n",
      "     66        0.0472        0.0379  0.0090\n",
      "     67        0.0472        0.0379  0.0090\n",
      "     68        0.0472        0.0379  0.0110\n",
      "     69        0.0472        0.0379  0.0080\n",
      "     70        0.0472        0.0379  0.0110\n",
      "     71        0.0472        0.0379  0.0090\n",
      "     72        0.0472        0.0379  0.0120\n",
      "     73        0.0472        0.0379  0.0090\n",
      "     74        0.0472        0.0379  0.0090\n",
      "     75        0.0472        0.0379  0.0090\n",
      "     76        0.0472        0.0379  0.0110\n",
      "     77        0.0472        0.0379  0.0090\n",
      "     78        0.0472        0.0379  0.0110\n",
      "     79        0.0472        0.0379  0.0090\n",
      "     80        0.0472        0.0379  0.0110\n",
      "     81        0.0472        0.0379  0.0110\n",
      "     82        0.0472        0.0379  0.0090\n",
      "     83        0.0472        0.0379  0.0110\n",
      "     84        0.0472        0.0379  0.0110\n",
      "     85        0.0472        0.0379  0.0130\n",
      "     86        0.0472        0.0379  0.0120\n",
      "     87        0.0472        0.0379  0.0120\n",
      "     88        0.0472        0.0379  0.0090\n",
      "     89        0.0472        0.0379  0.0130\n",
      "     90        0.0473        0.0379  0.0110\n",
      "     91        0.0473        0.0379  0.0100\n",
      "     92        0.0473        0.0379  0.0100\n",
      "     93        0.0473        0.0379  0.0090\n",
      "     94        0.0473        0.0379  0.0100\n",
      "     95        0.0473        0.0379  0.0110\n",
      "     96        0.0473        0.0379  0.0110\n",
      "     97        0.0473        0.0379  0.0110\n",
      "     98        0.0473        0.0379  0.0120\n",
      "     99        0.0473        0.0379  0.0110\n",
      "    100        0.0473        0.0379  0.0130\n",
      "    101        0.0473        0.0379  0.0110\n",
      "    102        0.0473        0.0379  0.0080\n",
      "    103        0.0473        0.0379  0.0120\n",
      "    104        0.0473        0.0379  0.0100\n",
      "    105        0.0473        0.0379  0.0110\n",
      "    106        0.0473        0.0380  0.0080\n",
      "    107        0.0473        0.0380  0.0100\n",
      "    108        0.0473        0.0380  0.0080\n",
      "    109        0.0473        0.0380  0.0080\n",
      "    110        0.0473        0.0380  0.0090\n",
      "    111        0.0473        0.0380  0.0090\n",
      "    112        0.0473        0.0380  0.0080\n",
      "    113        0.0473        0.0380  0.0110\n",
      "    114        0.0473        0.0380  0.0080\n",
      "    115        0.0473        0.0380  0.0120\n",
      "    116        0.0473        0.0380  0.0090\n",
      "    117        0.0474        0.0380  0.0090\n",
      "    118        0.0474        0.0380  0.0090\n",
      "    119        0.0474        0.0380  0.0109\n",
      "    120        0.0474        0.0380  0.0090\n",
      "    121        0.0474        0.0380  0.0079\n",
      "    122        0.0474        0.0380  0.0080\n",
      "    123        0.0474        0.0380  0.0080\n",
      "    124        0.0474        0.0380  0.0109\n",
      "    125        0.0474        0.0380  0.0120\n",
      "  epoch    train_loss    valid_loss     dur\n",
      "-------  ------------  ------------  ------\n",
      "      1        \u001b[36m0.3209\u001b[0m        \u001b[32m0.4148\u001b[0m  0.0070\n",
      "      2        \u001b[36m0.3092\u001b[0m        \u001b[32m0.3973\u001b[0m  0.0110\n",
      "      3        \u001b[36m0.2944\u001b[0m        \u001b[32m0.3775\u001b[0m  0.0079\n",
      "      4        \u001b[36m0.2778\u001b[0m        \u001b[32m0.3567\u001b[0m  0.0110\n",
      "      5        \u001b[36m0.2604\u001b[0m        \u001b[32m0.3355\u001b[0m  0.0100\n",
      "      6        \u001b[36m0.2429\u001b[0m        \u001b[32m0.3148\u001b[0m  0.0100\n",
      "      7        \u001b[36m0.2258\u001b[0m        \u001b[32m0.2947\u001b[0m  0.0100\n",
      "      8        \u001b[36m0.2094\u001b[0m        \u001b[32m0.2757\u001b[0m  0.0079\n",
      "      9        \u001b[36m0.1940\u001b[0m        \u001b[32m0.2577\u001b[0m  0.0100\n",
      "     10        \u001b[36m0.1797\u001b[0m        \u001b[32m0.2411\u001b[0m  0.0100\n",
      "     11        \u001b[36m0.1665\u001b[0m        \u001b[32m0.2256\u001b[0m  0.0090\n",
      "     12        \u001b[36m0.1545\u001b[0m        \u001b[32m0.2114\u001b[0m  0.0080\n",
      "     13        \u001b[36m0.1435\u001b[0m        \u001b[32m0.1984\u001b[0m  0.0100\n",
      "     14        \u001b[36m0.1336\u001b[0m        \u001b[32m0.1865\u001b[0m  0.0110\n",
      "     15        \u001b[36m0.1247\u001b[0m        \u001b[32m0.1757\u001b[0m  0.0100\n",
      "     16        \u001b[36m0.1167\u001b[0m        \u001b[32m0.1659\u001b[0m  0.0110\n",
      "     17        \u001b[36m0.1095\u001b[0m        \u001b[32m0.1569\u001b[0m  0.0110\n",
      "     18        \u001b[36m0.1031\u001b[0m        \u001b[32m0.1489\u001b[0m  0.0100\n",
      "     19        \u001b[36m0.0974\u001b[0m        \u001b[32m0.1415\u001b[0m  0.0080\n",
      "     20        \u001b[36m0.0924\u001b[0m        \u001b[32m0.1349\u001b[0m  0.0100\n",
      "     21        \u001b[36m0.0879\u001b[0m        \u001b[32m0.1289\u001b[0m  0.0080\n",
      "     22        \u001b[36m0.0839\u001b[0m        \u001b[32m0.1235\u001b[0m  0.0090\n",
      "     23        \u001b[36m0.0803\u001b[0m        \u001b[32m0.1186\u001b[0m  0.0090\n",
      "     24        \u001b[36m0.0772\u001b[0m        \u001b[32m0.1142\u001b[0m  0.0080\n",
      "     25        \u001b[36m0.0744\u001b[0m        \u001b[32m0.1101\u001b[0m  0.0110\n",
      "     26        \u001b[36m0.0720\u001b[0m        \u001b[32m0.1065\u001b[0m  0.0080\n",
      "     27        \u001b[36m0.0698\u001b[0m        \u001b[32m0.1032\u001b[0m  0.0080\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "     28        \u001b[36m0.0679\u001b[0m        \u001b[32m0.1003\u001b[0m  0.0130\n",
      "     29        \u001b[36m0.0662\u001b[0m        \u001b[32m0.0976\u001b[0m  0.0090\n",
      "     30        \u001b[36m0.0647\u001b[0m        \u001b[32m0.0951\u001b[0m  0.0100\n",
      "     31        \u001b[36m0.0634\u001b[0m        \u001b[32m0.0929\u001b[0m  0.0090\n",
      "     32        \u001b[36m0.0622\u001b[0m        \u001b[32m0.0909\u001b[0m  0.0080\n",
      "     33        \u001b[36m0.0612\u001b[0m        \u001b[32m0.0891\u001b[0m  0.0090\n",
      "     34        \u001b[36m0.0603\u001b[0m        \u001b[32m0.0874\u001b[0m  0.0090\n",
      "     35        \u001b[36m0.0595\u001b[0m        \u001b[32m0.0859\u001b[0m  0.0080\n",
      "     36        \u001b[36m0.0588\u001b[0m        \u001b[32m0.0845\u001b[0m  0.0092\n",
      "     37        \u001b[36m0.0581\u001b[0m        \u001b[32m0.0832\u001b[0m  0.0080\n",
      "     38        \u001b[36m0.0576\u001b[0m        \u001b[32m0.0821\u001b[0m  0.0080\n",
      "     39        \u001b[36m0.0571\u001b[0m        \u001b[32m0.0810\u001b[0m  0.0080\n",
      "     40        \u001b[36m0.0566\u001b[0m        \u001b[32m0.0801\u001b[0m  0.0080\n",
      "     41        \u001b[36m0.0562\u001b[0m        \u001b[32m0.0792\u001b[0m  0.0090\n",
      "     42        \u001b[36m0.0559\u001b[0m        \u001b[32m0.0784\u001b[0m  0.0090\n",
      "     43        \u001b[36m0.0556\u001b[0m        \u001b[32m0.0777\u001b[0m  0.0090\n",
      "     44        \u001b[36m0.0553\u001b[0m        \u001b[32m0.0770\u001b[0m  0.0100\n",
      "     45        \u001b[36m0.0550\u001b[0m        \u001b[32m0.0764\u001b[0m  0.0079\n",
      "     46        \u001b[36m0.0548\u001b[0m        \u001b[32m0.0758\u001b[0m  0.0100\n",
      "     47        \u001b[36m0.0546\u001b[0m        \u001b[32m0.0753\u001b[0m  0.0090\n",
      "     48        \u001b[36m0.0545\u001b[0m        \u001b[32m0.0748\u001b[0m  0.0100\n",
      "     49        \u001b[36m0.0543\u001b[0m        \u001b[32m0.0744\u001b[0m  0.0110\n",
      "     50        \u001b[36m0.0542\u001b[0m        \u001b[32m0.0740\u001b[0m  0.0090\n",
      "     51        \u001b[36m0.0540\u001b[0m        \u001b[32m0.0736\u001b[0m  0.0090\n",
      "     52        \u001b[36m0.0539\u001b[0m        \u001b[32m0.0733\u001b[0m  0.0090\n",
      "     53        \u001b[36m0.0538\u001b[0m        \u001b[32m0.0729\u001b[0m  0.0080\n",
      "     54        \u001b[36m0.0537\u001b[0m        \u001b[32m0.0727\u001b[0m  0.0080\n",
      "     55        \u001b[36m0.0536\u001b[0m        \u001b[32m0.0724\u001b[0m  0.0110\n",
      "     56        \u001b[36m0.0536\u001b[0m        \u001b[32m0.0721\u001b[0m  0.0100\n",
      "     57        \u001b[36m0.0535\u001b[0m        \u001b[32m0.0719\u001b[0m  0.0110\n",
      "     58        \u001b[36m0.0534\u001b[0m        \u001b[32m0.0717\u001b[0m  0.0089\n",
      "     59        \u001b[36m0.0534\u001b[0m        \u001b[32m0.0715\u001b[0m  0.0080\n",
      "     60        \u001b[36m0.0533\u001b[0m        \u001b[32m0.0713\u001b[0m  0.0090\n",
      "     61        \u001b[36m0.0533\u001b[0m        \u001b[32m0.0712\u001b[0m  0.0100\n",
      "     62        \u001b[36m0.0532\u001b[0m        \u001b[32m0.0710\u001b[0m  0.0080\n",
      "     63        \u001b[36m0.0532\u001b[0m        \u001b[32m0.0709\u001b[0m  0.0080\n",
      "     64        \u001b[36m0.0532\u001b[0m        \u001b[32m0.0707\u001b[0m  0.0089\n",
      "     65        \u001b[36m0.0531\u001b[0m        \u001b[32m0.0706\u001b[0m  0.0090\n",
      "     66        \u001b[36m0.0531\u001b[0m        \u001b[32m0.0705\u001b[0m  0.0090\n",
      "     67        \u001b[36m0.0531\u001b[0m        \u001b[32m0.0704\u001b[0m  0.0090\n",
      "     68        \u001b[36m0.0531\u001b[0m        \u001b[32m0.0703\u001b[0m  0.0100\n",
      "     69        \u001b[36m0.0530\u001b[0m        \u001b[32m0.0702\u001b[0m  0.0100\n",
      "     70        \u001b[36m0.0530\u001b[0m        \u001b[32m0.0701\u001b[0m  0.0080\n",
      "     71        \u001b[36m0.0530\u001b[0m        \u001b[32m0.0700\u001b[0m  0.0090\n",
      "     72        \u001b[36m0.0530\u001b[0m        \u001b[32m0.0699\u001b[0m  0.0110\n",
      "     73        \u001b[36m0.0530\u001b[0m        \u001b[32m0.0699\u001b[0m  0.0090\n",
      "     74        \u001b[36m0.0530\u001b[0m        \u001b[32m0.0698\u001b[0m  0.0100\n",
      "     75        \u001b[36m0.0529\u001b[0m        \u001b[32m0.0698\u001b[0m  0.0080\n",
      "     76        \u001b[36m0.0529\u001b[0m        \u001b[32m0.0697\u001b[0m  0.0100\n",
      "     77        \u001b[36m0.0529\u001b[0m        \u001b[32m0.0697\u001b[0m  0.0090\n",
      "     78        \u001b[36m0.0529\u001b[0m        \u001b[32m0.0696\u001b[0m  0.0090\n",
      "     79        \u001b[36m0.0529\u001b[0m        \u001b[32m0.0696\u001b[0m  0.0080\n",
      "     80        \u001b[36m0.0529\u001b[0m        \u001b[32m0.0695\u001b[0m  0.0100\n",
      "     81        \u001b[36m0.0529\u001b[0m        \u001b[32m0.0695\u001b[0m  0.0080\n",
      "     82        \u001b[36m0.0529\u001b[0m        \u001b[32m0.0694\u001b[0m  0.0080\n",
      "     83        \u001b[36m0.0529\u001b[0m        \u001b[32m0.0694\u001b[0m  0.0090\n",
      "     84        \u001b[36m0.0529\u001b[0m        \u001b[32m0.0694\u001b[0m  0.0100\n",
      "     85        \u001b[36m0.0529\u001b[0m        \u001b[32m0.0694\u001b[0m  0.0100\n",
      "     86        \u001b[36m0.0529\u001b[0m        \u001b[32m0.0693\u001b[0m  0.0089\n",
      "     87        \u001b[36m0.0529\u001b[0m        \u001b[32m0.0693\u001b[0m  0.0100\n",
      "     88        \u001b[36m0.0528\u001b[0m        \u001b[32m0.0693\u001b[0m  0.0089\n",
      "     89        \u001b[36m0.0528\u001b[0m        \u001b[32m0.0693\u001b[0m  0.0110\n",
      "     90        \u001b[36m0.0528\u001b[0m        \u001b[32m0.0692\u001b[0m  0.0109\n",
      "     91        \u001b[36m0.0528\u001b[0m        \u001b[32m0.0692\u001b[0m  0.0100\n",
      "     92        \u001b[36m0.0528\u001b[0m        \u001b[32m0.0692\u001b[0m  0.0090\n",
      "     93        \u001b[36m0.0528\u001b[0m        \u001b[32m0.0692\u001b[0m  0.0080\n",
      "     94        \u001b[36m0.0528\u001b[0m        \u001b[32m0.0692\u001b[0m  0.0080\n",
      "     95        \u001b[36m0.0528\u001b[0m        \u001b[32m0.0692\u001b[0m  0.0100\n",
      "     96        \u001b[36m0.0528\u001b[0m        \u001b[32m0.0692\u001b[0m  0.0090\n",
      "     97        \u001b[36m0.0528\u001b[0m        \u001b[32m0.0691\u001b[0m  0.0090\n",
      "     98        \u001b[36m0.0528\u001b[0m        \u001b[32m0.0691\u001b[0m  0.0110\n",
      "     99        \u001b[36m0.0528\u001b[0m        \u001b[32m0.0691\u001b[0m  0.0110\n",
      "    100        \u001b[36m0.0528\u001b[0m        \u001b[32m0.0691\u001b[0m  0.0090\n",
      "    101        \u001b[36m0.0528\u001b[0m        \u001b[32m0.0691\u001b[0m  0.0110\n",
      "    102        \u001b[36m0.0528\u001b[0m        \u001b[32m0.0691\u001b[0m  0.0110\n",
      "    103        \u001b[36m0.0528\u001b[0m        \u001b[32m0.0691\u001b[0m  0.0080\n",
      "    104        \u001b[36m0.0528\u001b[0m        \u001b[32m0.0691\u001b[0m  0.0100\n",
      "    105        \u001b[36m0.0528\u001b[0m        \u001b[32m0.0691\u001b[0m  0.0109\n",
      "    106        \u001b[36m0.0528\u001b[0m        \u001b[32m0.0691\u001b[0m  0.0089\n",
      "    107        \u001b[36m0.0528\u001b[0m        \u001b[32m0.0691\u001b[0m  0.0090\n",
      "    108        \u001b[36m0.0528\u001b[0m        \u001b[32m0.0691\u001b[0m  0.0100\n",
      "    109        \u001b[36m0.0528\u001b[0m        \u001b[32m0.0691\u001b[0m  0.0090\n",
      "    110        \u001b[36m0.0528\u001b[0m        \u001b[32m0.0690\u001b[0m  0.0100\n",
      "    111        \u001b[36m0.0528\u001b[0m        \u001b[32m0.0690\u001b[0m  0.0110\n",
      "    112        \u001b[36m0.0528\u001b[0m        \u001b[32m0.0690\u001b[0m  0.0090\n",
      "    113        \u001b[36m0.0528\u001b[0m        \u001b[32m0.0690\u001b[0m  0.0080\n",
      "    114        \u001b[36m0.0528\u001b[0m        \u001b[32m0.0690\u001b[0m  0.0090\n",
      "    115        \u001b[36m0.0528\u001b[0m        \u001b[32m0.0690\u001b[0m  0.0110\n",
      "    116        \u001b[36m0.0528\u001b[0m        \u001b[32m0.0690\u001b[0m  0.0110\n",
      "    117        \u001b[36m0.0528\u001b[0m        \u001b[32m0.0690\u001b[0m  0.0110\n",
      "    118        \u001b[36m0.0528\u001b[0m        \u001b[32m0.0690\u001b[0m  0.0100\n",
      "    119        \u001b[36m0.0528\u001b[0m        \u001b[32m0.0690\u001b[0m  0.0099\n",
      "    120        \u001b[36m0.0528\u001b[0m        \u001b[32m0.0690\u001b[0m  0.0080\n",
      "    121        \u001b[36m0.0528\u001b[0m        \u001b[32m0.0690\u001b[0m  0.0090\n",
      "    122        \u001b[36m0.0528\u001b[0m        \u001b[32m0.0690\u001b[0m  0.0100\n",
      "    123        \u001b[36m0.0528\u001b[0m        \u001b[32m0.0690\u001b[0m  0.0080\n",
      "    124        \u001b[36m0.0528\u001b[0m        \u001b[32m0.0690\u001b[0m  0.0079\n",
      "    125        \u001b[36m0.0528\u001b[0m        \u001b[32m0.0690\u001b[0m  0.0110\n",
      "  epoch    train_loss    valid_loss     dur\n",
      "-------  ------------  ------------  ------\n",
      "      1        \u001b[36m0.5371\u001b[0m        \u001b[32m0.5231\u001b[0m  0.0160\n",
      "      2        \u001b[36m0.4750\u001b[0m        \u001b[32m0.4483\u001b[0m  0.0189\n",
      "      3        \u001b[36m0.4009\u001b[0m        \u001b[32m0.3745\u001b[0m  0.0170\n",
      "      4        \u001b[36m0.3313\u001b[0m        \u001b[32m0.3124\u001b[0m  0.0169\n",
      "      5        \u001b[36m0.2729\u001b[0m        \u001b[32m0.2627\u001b[0m  0.0150\n",
      "      6        \u001b[36m0.2263\u001b[0m        \u001b[32m0.2225\u001b[0m  0.0150\n",
      "      7        \u001b[36m0.1884\u001b[0m        \u001b[32m0.1902\u001b[0m  0.0180\n",
      "      8        \u001b[36m0.1582\u001b[0m        \u001b[32m0.1643\u001b[0m  0.0189\n",
      "      9        \u001b[36m0.1349\u001b[0m        \u001b[32m0.1442\u001b[0m  0.0149\n",
      "     10        \u001b[36m0.1158\u001b[0m        \u001b[32m0.1283\u001b[0m  0.0189\n",
      "     11        \u001b[36m0.1012\u001b[0m        \u001b[32m0.1156\u001b[0m  0.0170\n",
      "     12        \u001b[36m0.0898\u001b[0m        \u001b[32m0.1056\u001b[0m  0.0160\n",
      "     13        \u001b[36m0.0805\u001b[0m        \u001b[32m0.0976\u001b[0m  0.0180\n",
      "     14        \u001b[36m0.0733\u001b[0m        \u001b[32m0.0911\u001b[0m  0.0170\n",
      "     15        \u001b[36m0.0678\u001b[0m        \u001b[32m0.0861\u001b[0m  0.0160\n",
      "     16        \u001b[36m0.0634\u001b[0m        \u001b[32m0.0822\u001b[0m  0.0170\n",
      "     17        \u001b[36m0.0599\u001b[0m        \u001b[32m0.0791\u001b[0m  0.0149\n",
      "     18        \u001b[36m0.0573\u001b[0m        \u001b[32m0.0767\u001b[0m  0.0139\n",
      "     19        \u001b[36m0.0552\u001b[0m        \u001b[32m0.0747\u001b[0m  0.0170\n",
      "     20        \u001b[36m0.0537\u001b[0m        \u001b[32m0.0733\u001b[0m  0.0150\n",
      "     21        \u001b[36m0.0524\u001b[0m        \u001b[32m0.0720\u001b[0m  0.0189\n",
      "     22        \u001b[36m0.0514\u001b[0m        \u001b[32m0.0710\u001b[0m  0.0169\n",
      "     23        \u001b[36m0.0506\u001b[0m        \u001b[32m0.0702\u001b[0m  0.0170\n",
      "     24        \u001b[36m0.0500\u001b[0m        \u001b[32m0.0696\u001b[0m  0.0180\n",
      "     25        \u001b[36m0.0495\u001b[0m        \u001b[32m0.0691\u001b[0m  0.0170\n",
      "     26        \u001b[36m0.0491\u001b[0m        \u001b[32m0.0686\u001b[0m  0.0159\n",
      "     27        \u001b[36m0.0487\u001b[0m        \u001b[32m0.0683\u001b[0m  0.0159\n",
      "     28        \u001b[36m0.0485\u001b[0m        \u001b[32m0.0679\u001b[0m  0.0170\n",
      "     29        \u001b[36m0.0481\u001b[0m        \u001b[32m0.0675\u001b[0m  0.0140\n",
      "     30        \u001b[36m0.0478\u001b[0m        \u001b[32m0.0672\u001b[0m  0.0150\n",
      "     31        \u001b[36m0.0476\u001b[0m        \u001b[32m0.0670\u001b[0m  0.0189\n",
      "     32        \u001b[36m0.0475\u001b[0m        \u001b[32m0.0668\u001b[0m  0.0179\n",
      "     33        \u001b[36m0.0474\u001b[0m        \u001b[32m0.0667\u001b[0m  0.0179\n",
      "     34        \u001b[36m0.0473\u001b[0m        \u001b[32m0.0666\u001b[0m  0.0160\n",
      "     35        \u001b[36m0.0472\u001b[0m        \u001b[32m0.0665\u001b[0m  0.0149\n",
      "     36        \u001b[36m0.0472\u001b[0m        \u001b[32m0.0664\u001b[0m  0.0160\n",
      "     37        \u001b[36m0.0471\u001b[0m        \u001b[32m0.0664\u001b[0m  0.0150\n",
      "     38        \u001b[36m0.0471\u001b[0m        \u001b[32m0.0664\u001b[0m  0.0170\n",
      "     39        \u001b[36m0.0471\u001b[0m        0.0664  0.0170\n",
      "     40        0.0471        0.0664  0.0160\n",
      "     41        0.0471        0.0664  0.0140\n",
      "     42        0.0471        0.0665  0.0145\n",
      "     43        0.0472        0.0665  0.0170\n",
      "     44        0.0472        0.0665  0.0150\n",
      "     45        0.0472        0.0664  0.0150\n",
      "     46        0.0471        \u001b[32m0.0664\u001b[0m  0.0159\n",
      "     47        \u001b[36m0.0471\u001b[0m        \u001b[32m0.0663\u001b[0m  0.0180\n",
      "     48        \u001b[36m0.0471\u001b[0m        0.0663  0.0150\n",
      "     49        \u001b[36m0.0471\u001b[0m        0.0663  0.0149\n",
      "     50        0.0471        \u001b[32m0.0663\u001b[0m  0.0150\n",
      "     51        \u001b[36m0.0470\u001b[0m        \u001b[32m0.0663\u001b[0m  0.0140\n",
      "     52        \u001b[36m0.0470\u001b[0m        \u001b[32m0.0662\u001b[0m  0.0149\n",
      "     53        \u001b[36m0.0470\u001b[0m        \u001b[32m0.0662\u001b[0m  0.0160\n",
      "     54        \u001b[36m0.0470\u001b[0m        \u001b[32m0.0661\u001b[0m  0.0150\n",
      "     55        \u001b[36m0.0469\u001b[0m        \u001b[32m0.0661\u001b[0m  0.0150\n",
      "     56        0.0470        \u001b[32m0.0661\u001b[0m  0.0140\n",
      "     57        \u001b[36m0.0469\u001b[0m        \u001b[32m0.0661\u001b[0m  0.0160\n",
      "     58        \u001b[36m0.0469\u001b[0m        0.0661  0.0150\n",
      "     59        0.0469        0.0661  0.0150\n",
      "     60        0.0470        0.0662  0.0169\n",
      "     61        0.0470        0.0663  0.0150\n",
      "     62        0.0470        0.0663  0.0150\n",
      "     63        0.0470        0.0662  0.0160\n",
      "     64        0.0470        0.0662  0.0160\n",
      "     65        0.0469        \u001b[32m0.0661\u001b[0m  0.0150\n",
      "     66        \u001b[36m0.0469\u001b[0m        \u001b[32m0.0660\u001b[0m  0.0149\n",
      "     67        \u001b[36m0.0469\u001b[0m        0.0660  0.0149\n",
      "     68        0.0469        0.0661  0.0170\n",
      "     69        0.0469        0.0661  0.0150\n",
      "     70        0.0469        0.0660  0.0150\n",
      "     71        0.0469        0.0661  0.0150\n",
      "     72        0.0469        0.0662  0.0150\n",
      "     73        0.0470        0.0662  0.0170\n",
      "     74        0.0470        0.0662  0.0150\n",
      "     75        0.0470        0.0662  0.0140\n",
      "     76        0.0470        0.0662  0.0150\n",
      "     77        0.0469        0.0662  0.0150\n",
      "     78        0.0470        0.0662  0.0150\n",
      "     79        0.0469        0.0661  0.0150\n",
      "     80        0.0469        0.0661  0.0150\n",
      "     81        \u001b[36m0.0469\u001b[0m        \u001b[32m0.0660\u001b[0m  0.0150\n",
      "     82        \u001b[36m0.0469\u001b[0m        \u001b[32m0.0660\u001b[0m  0.0150\n",
      "     83        \u001b[36m0.0468\u001b[0m        \u001b[32m0.0659\u001b[0m  0.0170\n",
      "     84        \u001b[36m0.0468\u001b[0m        0.0659  0.0200\n",
      "     85        0.0468        \u001b[32m0.0659\u001b[0m  0.0150\n",
      "     86        \u001b[36m0.0468\u001b[0m        \u001b[32m0.0659\u001b[0m  0.0150\n",
      "     87        \u001b[36m0.0468\u001b[0m        0.0659  0.0170\n",
      "     88        0.0468        0.0660  0.0152\n",
      "     89        0.0469        0.0661  0.0229\n",
      "     90        0.0469        0.0661  0.0170\n",
      "     91        0.0469        0.0661  0.0160\n",
      "     92        0.0469        0.0661  0.0168\n",
      "     93        0.0469        0.0661  0.0149\n",
      "     94        0.0469        0.0661  0.0150\n",
      "     95        0.0469        0.0661  0.0170\n",
      "     96        0.0469        0.0661  0.0149\n",
      "     97        0.0469        0.0660  0.0159\n",
      "     98        0.0469        0.0660  0.0170\n",
      "     99        0.0468        0.0660  0.0150\n",
      "    100        0.0469        0.0660  0.0140\n",
      "    101        0.0469        0.0661  0.0160\n",
      "    102        0.0469        0.0661  0.0189\n",
      "    103        0.0469        0.0661  0.0170\n",
      "    104        0.0469        0.0661  0.0150\n",
      "    105        0.0469        0.0661  0.0149\n",
      "    106        0.0469        0.0661  0.0150\n",
      "    107        0.0469        0.0661  0.0170\n",
      "    108        0.0469        0.0660  0.0170\n",
      "    109        0.0469        0.0660  0.0150\n",
      "    110        0.0469        0.0660  0.0169\n",
      "    111        0.0468        0.0660  0.0150\n",
      "    112        0.0469        0.0660  0.0170\n",
      "    113        0.0469        0.0660  0.0170\n",
      "    114        0.0469        0.0661  0.0170\n",
      "    115        0.0469        0.0661  0.0170\n",
      "    116        0.0469        0.0661  0.0150\n",
      "    117        0.0469        0.0661  0.0170\n",
      "    118        0.0469        0.0662  0.0170\n",
      "    119        0.0469        0.0662  0.0170\n",
      "    120        0.0470        0.0662  0.0150\n",
      "    121        0.0469        0.0661  0.0169\n",
      "    122        0.0469        0.0661  0.0140\n",
      "    123        0.0469        0.0661  0.0150\n",
      "    124        0.0469        0.0662  0.0170\n",
      "    125        0.0470        0.0662  0.0140\n"
     ]
    }
   ],
   "source": [
    "assert len(X_training) == len(Y_training)\n",
    "randomized_search_result = sgd_randomized_search.fit(X_training.values, Y_training.values)\n",
    "\n",
    "#dataframe.values gives the underlying numpy array of the data frame, doing this conversion since pandas data frame \n",
    "#is not supported."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Save and view the best parameters and hyperparameters obtained from randomized search."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>mean_fit_time</th>\n",
       "      <th>std_fit_time</th>\n",
       "      <th>mean_score_time</th>\n",
       "      <th>std_score_time</th>\n",
       "      <th>param_optimizer__weight_decay</th>\n",
       "      <th>param_optimizer__nesterov</th>\n",
       "      <th>param_optimizer__momentum</th>\n",
       "      <th>param_optimizer__dampening</th>\n",
       "      <th>param_optimizer</th>\n",
       "      <th>param_module__num_units</th>\n",
       "      <th>...</th>\n",
       "      <th>params</th>\n",
       "      <th>split0_test_score</th>\n",
       "      <th>split1_test_score</th>\n",
       "      <th>mean_test_score</th>\n",
       "      <th>std_test_score</th>\n",
       "      <th>rank_test_score</th>\n",
       "      <th>split0_train_score</th>\n",
       "      <th>split1_train_score</th>\n",
       "      <th>mean_train_score</th>\n",
       "      <th>std_train_score</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1.582486</td>\n",
       "      <td>0.128185</td>\n",
       "      <td>0.002476</td>\n",
       "      <td>0.000516</td>\n",
       "      <td>0.1</td>\n",
       "      <td>True</td>\n",
       "      <td>0.75</td>\n",
       "      <td>0</td>\n",
       "      <td>&lt;class 'torch.optim.sgd.SGD'&gt;</td>\n",
       "      <td>14</td>\n",
       "      <td>...</td>\n",
       "      <td>{'optimizer__weight_decay': 0.1, 'optimizer__n...</td>\n",
       "      <td>-0.054545</td>\n",
       "      <td>-0.049608</td>\n",
       "      <td>-0.052098</td>\n",
       "      <td>0.002469</td>\n",
       "      <td>1</td>\n",
       "      <td>-0.045399</td>\n",
       "      <td>-0.056154</td>\n",
       "      <td>-0.050776</td>\n",
       "      <td>0.005377</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>1 rows  24 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "   mean_fit_time  std_fit_time  mean_score_time  std_score_time  \\\n",
       "0       1.582486      0.128185         0.002476        0.000516   \n",
       "\n",
       "  param_optimizer__weight_decay param_optimizer__nesterov  \\\n",
       "0                           0.1                      True   \n",
       "\n",
       "  param_optimizer__momentum param_optimizer__dampening  \\\n",
       "0                      0.75                          0   \n",
       "\n",
       "                 param_optimizer param_module__num_units  ...  \\\n",
       "0  <class 'torch.optim.sgd.SGD'>                      14  ...   \n",
       "\n",
       "                                              params split0_test_score  \\\n",
       "0  {'optimizer__weight_decay': 0.1, 'optimizer__n...         -0.054545   \n",
       "\n",
       "  split1_test_score mean_test_score std_test_score  rank_test_score  \\\n",
       "0         -0.049608       -0.052098       0.002469                1   \n",
       "\n",
       "   split0_train_score  split1_train_score  mean_train_score  std_train_score  \n",
       "0           -0.045399           -0.056154         -0.050776         0.005377  \n",
       "\n",
       "[1 rows x 24 columns]"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Import the parameter combinations and the corresponding results into a pandas dataframe\n",
    "pd.DataFrame(randomized_search_result.cv_results_)\n",
    "\n",
    "#Each row of this dataframe gives one combination of the hyperparameters used and the corresponding performance of the\n",
    "#estimator used."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Interpretation of cv_results_\n",
    "Column \"mean_test_score\" is the average of columns split_0_test_score, split_1_test_score, split_2_test_score...split_k_test_score for \"this\" hyperparameter configuration.<br><br>\n",
    "\"randomized_search_result.best_score_\" will return the max value of the column mean_test_score.<br><br>\n",
    "Column \"rank_test_score\" ranks all hyperparameter combinations by the values of mean_test_score.<br><br>\n",
    "Column \"std_test_score\" is the standard deviation of split_0_test_score, split_1_test_score,...., split_k_test_score. It shows how consistently \"this\" set of hyperparameters is performing on the hold-out data of each k-fold validation."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Save the results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\users\\being_aerys\\appdata\\local\\programs\\python\\python36\\lib\\site-packages\\torch\\serialization.py:360: UserWarning: Couldn't retrieve source code for container of type Regression_Module. It won't be checked for correctness upon loading.\n",
      "  \"type \" + obj.__name__ + \". It won't be checked \"\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'optimizer__weight_decay': 0.1,\n",
       " 'optimizer__nesterov': True,\n",
       " 'optimizer__momentum': 0.75,\n",
       " 'optimizer__dampening': 0.0,\n",
       " 'optimizer': torch.optim.sgd.SGD,\n",
       " 'module__num_units': 14,\n",
       " 'module__dropout': 0.0,\n",
       " 'max_epochs': 125,\n",
       " 'lr': 0.005,\n",
       " 'iterator_train__batch_size': 64}"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#saving the best estimator i.e., the best neural network parameters\n",
    "# randomized_search_result.best_estimator_ gives the neural net with best parameters\n",
    "joblib.dump(randomized_search_result.best_estimator_, 'best_params_of_NN_by_randomized_search.pkl', compress = 1)\n",
    "\n",
    "#saving the hyperparameters that were used to get this best estimator\n",
    "# randomized_search_result.best_params_ gives the best hyperparameters that were used to get this best estimator\n",
    "joblib.dump(randomized_search_result.best_params_, 'best_hyperparams_for_NN_by_randomized_search.pkl', compress = 1)\n",
    "\n",
    "#saving all 350 models' hyperparameter configurations and the neural network parameters\n",
    "\n",
    "\n",
    "\n",
    "# Displaying the best hyperparameters configuration\n",
    "randomized_search_result.best_params_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def report(results, num_of_top_results_to_report = 5):\n",
    "    #this utility method was found online.\n",
    "    \n",
    "    for idx in range(0, num_of_top_results_to_report):\n",
    "        candidates = np.flatnonzero(results['rank_test_score'] == idx)\n",
    "        for candidate in candidates:\n",
    "            print(\"Model with rank: {0}\".format(idx))\n",
    "            print(\"Mean validation score: {0:.3f} (std: {1:.3f})\".format(\n",
    "                  results['mean_test_score'][candidate],\n",
    "                  results['std_test_score'][candidate]))\n",
    "            print(\"Parameters: {0}\".format(results['params'][candidate]))\n",
    "            print(\"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model with rank: 1\n",
      "Mean validation score: -0.052 (std: 0.002)\n",
      "Parameters: {'optimizer__weight_decay': 0.1, 'optimizer__nesterov': True, 'optimizer__momentum': 0.75, 'optimizer__dampening': 0.0, 'optimizer': <class 'torch.optim.sgd.SGD'>, 'module__num_units': 14, 'module__dropout': 0.0, 'max_epochs': 125, 'lr': 0.005, 'iterator_train__batch_size': 64}\n",
      "\n"
     ]
    }
   ],
   "source": [
    "report(sgd_randomized_search.cv_results_,num_of_top_results_to_report = 3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Visualize training and validation loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get training and validation loss\n",
    "epochs = [i for i in range(len(sgd_randomized_search.best_estimator_.history))]\n",
    "training_loss = sgd_randomized_search.best_estimator_.history[:,'train_loss']\n",
    "validation_loss = sgd_randomized_search.best_estimator_.history[:,'valid_loss']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(epochs,training_loss,'g-');\n",
    "plt.plot(epochs,validation_loss,'r-');\n",
    "plt.title('Training and Validation Loss Curves');\n",
    "plt.xlabel('Epochs');\n",
    "plt.ylabel('Mean Squared Error');\n",
    "plt.legend(['Train','Validation']);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Test on Test Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Predict on test data\n",
    "testing_data = pd.read_csv(\"normalized_testing_features_and_targets.csv\", sep = \",\")\n",
    "testing_data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Separate features and targets\n",
    "X_testing = testing_data.iloc[:,0:num_features].values #notice retaining only the underlying numpy arrays\n",
    "Y_testing = pd.DataFrame(testing_data[\"Hardness\"]).values\n",
    "\n",
    "Y_predictions_on_test_data = sgd_randomized_search.best_estimator_.predict(X_testing.astype(np.float32))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Y_predictions_on_test_data = sgd_randomized_search.best_estimator_.predict(X_testing.astype(np.float32))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Y_testing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Y_predictions_on_test_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get Root Mean Squared Error\n",
    "MSE(Y_testing,Y_predictions_on_test_data)**(1/2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Kernel Density Estimation Plot\n",
    "\n",
    "# sns.kdeplot(Y_predictions_on_test_data.squeeze(), label='predictions of the model', shade=True)\n",
    "# sns.kdeplot(Y_testing.squeeze(), label='true values', shade=True)\n",
    "# plt.xlabel('Hardness');"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Dist Plot\n",
    "\n",
    "# sns.distplot(Y_testing.squeeze()-Y_predictions_on_test_data.squeeze(),label='error', bins = 10);\n",
    "# plt.xlabel('Error');"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# R^2 plot\n",
    "\n",
    "print(r2_score(Y_testing,Y_predictions_on_test_data))\n",
    "\n",
    "plt.plot(Y_predictions_on_test_data,Y_testing,'g*')\n",
    "plt.xlabel('predicted')\n",
    "plt.ylabel('actual')\n",
    "plt.title('$R^{2}$ Plot');"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### View the actual values vs predicted values in real scale"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#The maximum and minimum targets of the training data were [983.91] and [116.] from feature engineering file\n",
    "\n",
    "max_hardness = 983.91\n",
    "min_hardness = 116.\n",
    "\n",
    "def convert_to_original_scale(scaled_hardness):\n",
    "    original_scale_hardness = (scaled_hardness * (max_hardness - min_hardness)) + min_hardness\n",
    "    return original_scale_hardness\n",
    "\n",
    "for test_sample in range(len(Y_predictions_on_test_data)):\n",
    "    print(\"True Hardness: {}\".format(convert_to_original_scale(Y_testing[test_sample])))\n",
    "    print(\"Predicted Hardness: {}\".format(convert_to_original_scale(Y_predictions_on_test_data[test_sample])))\n",
    "    print(\"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
